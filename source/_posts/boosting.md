---
title: boosting
catalog: true
date: 2019-12-05 12:22:51
subtitle: Boosting模型
header-img:
tags: 机器学习
---
DT(决策树)

ID3：基于信息增益

C4.5：基于信息增益比

CART：基于尼基指数

> 本文主要简单的比较常用的boosting算法的一些区别，从Adaboost到LightGBM。包括Adaboost，GBDT，XGBoost，LightGBM四个模型的简单介绍。

### AdaBoost

1. 初始化训练数据的权值分布，wi = 1/N
2. 训练数据集，得到基本分类器。G(x)：x -> {-1, 1}
3. 计算分类误差率。（相加误分类权重e）
4. 计算分类器系数am=(1/2)log(1-e/e)
5. 更新权值分布，继续训练。
6. 最终分类器等于所有弱分类器加权之和，权值为am。(分类误差率越小的基本分类器在最终分类器中的作用越大)。

- 分类错误的样本的权重会增大而分类正确的样本的权重则会减小。这样在训练h{m+1}时会侧重对错误样本的训练，以达到模型性能的提升，但是AdaBoost模型每个基分类器的损失函数优化目标是相同的且独立的，都是最优化当前样本（样本权重）的指数损失。

### GBDT

- GBDT (Gradient Boosting Decision Tree) 梯度提升迭代决策树。GBDT 也是 Boosting 算法的一种，但是和 AdaBoost 算法不同（AdaBoost 算法上一篇文章已经介绍）；
- 区别如下：AdaBoost 算法是利用前一轮的弱学习器的误差来更新样本权重值，然后一轮一轮的迭代；GBDT 也是迭代，但是 GBDT 要求弱学习器必须是 CART 模型，而且 GBDT 在模型训练的时候，是要求模型预测的样本损失尽可能的小。

1. 采用确定回归树T1(x)。通过确定最优划分点，最小化平方误差损失函数确定最优划分点。
2. 构建残差表，求解T2(x)来拟合残差。
3. 依次迭代。所有分类器之和极为强分类器。即为所求提升数模型。

### XGBoost

1. XGBoost提升模型也是采用残差，分裂节点选取的损失函数如下：

![123awed](\img\article\123awed.jpg)

2. 损失函数有：平方损失函数，逻辑回归损失函数。

3. XGBoost是一个加法模型，我们t次迭代拟合的数值就是上一次的残差结果。每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差。

   ![v2-a9b82954ae62e9e6da256c69ba22d38b_r](\img\article\v2-a9b82954ae62e9e6da256c69ba22d38b_r.jpg)

   ![v2-f0cd240fcc70e7615dae7c2a29856bfc_r](\img\article\v2-f0cd240fcc70e7615dae7c2a29856bfc_r.jpg)

4.  将全部k棵树的复杂度进行求和，添加到目标函数中作为正则化项，用于防止模型过度拟合。 

   ![v2-142ca609c9ff3dc2df877a00c30756ca_hd](\img\article\v2-142ca609c9ff3dc2df877a00c30756ca_hd.jpg)

   ![v2-c7ab2fcfd3196dbc0bce05d17b11d220_hd](\img\article\v2-c7ab2fcfd3196dbc0bce05d17b11d220_hd.jpg)

   ![v2-d0cf0063c23679e711146f861d36fc17_r](\img\article\v2-d0cf0063c23679e711146f861d36fc17_r.jpg)

5.  一种办法是贪心算法，遍历一个节点内的所有特征，按照公式计算出按照每一个特征分割的信息增益，找到信息增益最大的点进行树的分割。增加的新叶子惩罚项对应了树的剪枝，当gain小于某个阈值的时候，我们可以剪掉这个分割。但是这种办法不适用于数据量大的时候，因此，我们需要运用近似算法。 

6.  XGBoost在寻找splitpoint的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，按照**特征值的密度分布**，构造直方图计算特征值分布的面积，然后划分分布形成若干个bucket(桶)，每个bucket的面积相同，将**bucket边界上的特征值**作为splitpoint的候选，**遍历所有的候选分裂点**来找到最佳分裂点。

-  传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 
-  传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了**二阶泰勒展开，同时用到了一阶和二阶导数**。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 
-  xgboost在**代价函数里加入了正则项，用于控制模型的复杂度**。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 
- **Shrinkage（缩减），相当于学习速率（xgboost中的eta）**。每次迭代，增加新的模型，在前面成上一个小于1的系数，降低优化的速度，每次走一小步逐步逼近最优模型比每次走一大步逼近更加容易避免过拟合现象；
- 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样（即每次的输入特征不是全部特征），不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
- 忽略缺失值：在寻找splitpoint的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个工程技巧来减少了为稀疏离散特征寻找splitpoint的时间开销
- 指定缺失值的分隔方向：可以为缺失值或者指定的值指定分支的默认方向，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，分到那个子节点带来的增益大，默认的方向就是哪个子节点，这能大大提升算法的效率。
- 并行化处理：在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点

### LightGBM




