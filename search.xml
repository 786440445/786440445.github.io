<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>test</title>
      <link href="2021/10/24/test/"/>
      <url>2021/10/24/test/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>chain_model</title>
      <link href="2021/05/26/chain-model/"/>
      <url>2021/05/26/chain-model/</url>
      
        <content type="html"><![CDATA[<h1 id="chain-model训练过程"><a href="#chain-model训练过程" class="headerlink" title="chain model训练过程"></a>chain model训练过程</h1><blockquote><p>单音素训练</p><ul><li>train_mono.sh  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">steps/train_mono.sh --cmd "$train_cmd" --nj 10 data/train data/lang exp/mono</span><br><span class="line"><span class="meta">#</span><span class="bash"> params：</span></span><br><span class="line">1. train_data</span><br><span class="line">2. lang</span><br><span class="line">3. exp模型目录</span><br></pre></td></tr></table></figure></li></ul><p>构图</p><ul><li>mkgraph.sh  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">utils/mkgraph.sh data/lang_test exp/mono exp/mono graph</span><br><span class="line"><span class="meta">#</span><span class="bash"> params：</span></span><br><span class="line">1. lang_test</span><br><span class="line">2. exp模型目录</span><br><span class="line">3. exp构图目录</span><br></pre></td></tr></table></figure></li></ul><p>解码</p><ul><li>decode.sh  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">steps/decode.sh --cmd "$decode_cmd" --config conf/decode.config --nj 10 exp/mono/graph data/test exp/mono/decode_test</span><br></pre></td></tr></table></figure></li></ul><p>对齐，生成ali文件</p><ul><li>align_si.sh  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">steps/align_si.sh --cmd "$train_cmd" --nj 10 data/train data/lang exp/mono exp/mono_ali</span><br><span class="line"><span class="meta">#</span><span class="bash"> params：</span></span><br><span class="line">1. train_data</span><br><span class="line">2. lang</span><br><span class="line">3. exp模型目录</span><br><span class="line">4. exp对齐结果</span><br></pre></td></tr></table></figure></li></ul><p>三音素训练</p><ul><li>train_deltas.sh  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">steps&#x2F;train_deltas.sh --cmd &quot;$train_cmd&quot; 2500 20000 data&#x2F;train data&#x2F;lang exp&#x2F;tri1_ali exp&#x2F;tri2 || exit 1;</span><br><span class="line"># params：</span><br><span class="line">1. GMM个数</span><br><span class="line">2. 决策树状态建模结点个数</span><br><span class="line">3. train</span><br><span class="line">4. lang</span><br><span class="line">5. exp对齐结果</span><br><span class="line">6. exp模型</span><br></pre></td></tr></table></figure></li><li>mkgraph.sh</li><li>decode.sh</li><li>align_si.sh</li></ul><p>三音素训练</p><ul><li>train_deltas.sh</li><li>mkgraph.sh</li><li>decode.sh</li><li>align_si.sh</li></ul><p>do the alignment with fMLLR.</p><ul><li>train_lda_mllt</li><li>mkgraph.sh</li><li>decode.sh</li><li>align_fmllr.sh</li></ul><p>Building a larger SAT system.</p><ul><li>train_sat.sh</li><li>mkgraph.sh</li><li>decode_fmllr.sh</li><li>align_fmllr.sh</li><li>local/chain/run_tdnn.sh</li></ul></blockquote><h1 id="run-tdnn-sh"><a href="#run-tdnn-sh" class="headerlink" title="run_tdnn.sh"></a>run_tdnn.sh</h1><ul><li><p>local/nnet3/run_ivector_common.sh</p></li><li><p>align_fmllr_lats.sh</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">steps/align_fmllr_lats.sh --nj $nj --cmd "$train_cmd" data/$train_set data/lang exp/tri5a exp/tri5a_sp_lats</span><br></pre></td></tr></table></figure></li><li><p>gen_topo.py</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">steps/nnet3/chain/gen_topo.py $nonsilphonelist $silphonelist &gt;$lang/topo</span><br><span class="line"><span class="meta">#</span><span class="bash"> params：</span></span><br><span class="line">1. nonsilphonelist: $(cat $lang/phones/silence.csl) </span><br><span class="line">2. silphonelist: $(cat $lang/phones/nonsilence.csl)</span><br></pre></td></tr></table></figure></li><li>build_tree.sh  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">steps/nnet3/chain/build_tree.sh --frame-subsampling-factor 3 --context-opts "--context-width=2 --central-position=1" \</span><br><span class="line">--cmd "$train_cmd" 5000 data/$train_set $lang $ali_dir $treedir</span><br><span class="line"><span class="meta">#</span><span class="bash"> params：</span></span><br><span class="line">1. frame-subsampling-factor 下采样帧率</span><br><span class="line">2. context-opts：</span><br><span class="line">3. data</span><br><span class="line">4. lang</span><br><span class="line">5. ali_dir</span><br><span class="line">6. treedir</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>creating neural net configs using the xconfig parser”;</p></blockquote><pre><code>the first splicing is moved before the lda layer, so no splicing hererelu-batchnorm-layer name=tdnn1 dim=625relu-batchnorm-layer name=tdnn2 input=Append(-1,0,1) dim=625relu-batchnorm-layer name=tdnn3 input=Append(-1,0,1) dim=625relu-batchnorm-layer name=tdnn4 input=Append(-3,0,3) dim=625relu-batchnorm-layer name=tdnn5 input=Append(-3,0,3) dim=625relu-batchnorm-layer name=tdnn6 input=Append(-3,0,3) dim=625</code></pre><blockquote><p>train.py</p></blockquote><pre><code>steps/nnet3/chain/train.py</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>interview_0430</title>
      <link href="2021/05/01/interview-0430/"/>
      <url>2021/05/01/interview-0430/</url>
      
        <content type="html"><![CDATA[<h1 id="4-30-面试记录"><a href="#4-30-面试记录" class="headerlink" title="4.30 面试记录"></a>4.30 面试记录</h1><h2 id="ASR"><a href="#ASR" class="headerlink" title="ASR:"></a>ASR:</h2><ul><li>CE和CTC loss区别</li></ul><p>CE是基于帧粒度对两个序列的概率分布的相似程度进行建模，粒度比较细。</p><script type="math/tex; mode=display">loss = -\frac{\Sigma_{i=0}{p(i)log(q(i))}}{N}</script><p>CTC loss<br>引入blank符号，对进行序列的映射，从整体序列的维度进行考虑，但是CTC具有条件独立性，即前后帧条件独立，相互不影响。</p><p>RNN-A loss<br>类似CTC+RNN。one-one。在翻译的过程中，输入一帧，输出一帧，前后帧有时许关系，前帧可以影响后帧。</p><p>RNN-T loss<br>类似CTC+RNN。one-many。在翻译的过程中，输入一帧，输入多帧，直到遇到blank，选择下一个x，前帧可以影响后帧。</p><ul><li>维特比算法</li></ul><p>使用动态规划的思想，将大问题分解为字问题，即在第t时刻是最优的路径，则t-1时刻，也必然是最优路径。</p><p>在前一步的没个节点，选择出经过每个节点的D个状态的路径中的最优路径，作为下一时刻的前置子问题方案。下一个问题时，继续选择经过当前D个节点的D中的最优路径。</p><p>时间复杂度为O（L*D^2）,L为序列长度，D为可选择节点个数。</p><ul><li><p>token passing<br>在WFST解码过程中，使用token passing算法，分别对状态节点进行记忆和回溯，类似维特比算法。</p></li><li><p>1x1卷积和FC区别<br>1x1卷积就等于Fully connection</p></li></ul><ul><li>除了fbank特征，mfcc还有什么特征。</li></ul><p>Linear Predict C：线性预测系数(LPC)</p><p>i-vector</p><p>x-vector。</p><h2 id="TTS"><a href="#TTS" class="headerlink" title="TTS"></a>TTS</h2><ul><li>MelGan的损失函数</li></ul><ol><li>hinge-loss</li><li>MSE-loss</li><li></li></ol><ul><li>什么是回归，自回归，分类与回归的区别</li></ul><h3 id="5-26-面试记录"><a href="#5-26-面试记录" class="headerlink" title="5.26 面试记录"></a>5.26 面试记录</h3><ol><li>self attention在实时ASR过程中的训练及预测方法</li><li>CTC+language model怎么训练的</li><li>kaldi chain model流程</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>gmm-hmm,生成模型和判别模型</title>
      <link href="2021/05/01/gmm-hmm/"/>
      <url>2021/05/01/gmm-hmm/</url>
      
        <content type="html"><![CDATA[<h2 id="生成模型和判别模型"><a href="#生成模型和判别模型" class="headerlink" title="生成模型和判别模型"></a>生成模型和判别模型</h2><ol><li>生成模型：基于观测值x，求其隐藏状态值y的过程。其中x是由o以某种概率密度分布进行决定。主体是y，因此p(y|x)无法根据条件概率直接建模。需要根据贝叶斯公式进行转化。<script type="math/tex; mode=display">p(y|x)=\frac{p(x,y)}{p(x)}=\frac{p(x|y)p(y)}{p(x)}</script></li></ol><blockquote><p>生成模型包括有：GMM，HMM，Biays网络</p></blockquote><ul><li>其中P(x)是先验知识，可以直接计算出来。因此该式转换为P(x|y)来计算。</li></ul><ol><li>判别模型：基于观测值x，求y的条件概率p(y|x)，主体是x，y由x进行决定。可以直接对观测值，建模，推测出Y的结果。</li></ol><blockquote><p>判别模型:<br>CRF, LR, LogitsRegression,SVM,DNN</p></blockquote><ol><li><p>GMM-HMM中GMM为生成模型。根据语音帧序列的features构成的高斯分布来对状态进行建模。P(x|y)即为GMM的概率密度分布，通过求的GMM的各个分量的系数，均值，方差，即可完成GMM的模型构建。</p></li><li><p>DNN-HMM中DNN为判别模型，DNN输入是每一帧的40features，输出即表示该帧对应的logits，大小为决策树合并聚类的三状态的个数，经过softmax，可以求的最大概率对应的状态类别。</p></li></ol><h2 id="kaldi中GMM和HMM迭代过程"><a href="#kaldi中GMM和HMM迭代过程" class="headerlink" title="kaldi中GMM和HMM迭代过程"></a>kaldi中GMM和HMM迭代过程</h2><ul><li>单音速训练过程：</li></ul><ol><li>按照输入features（T，D），对应音速三状态，进行帧数均分。（类似Kmeans初始化）。</li><li>一个HMM状态可以多个帧。使用多个帧的features对该HMM状态中的观测概率分布进行建模，通用使用GMM。</li></ol><ul><li><p>GMM计算过程：(高斯混合模型)，混合系数 $\pi$ , 均值，协方差矩阵为参数。E-M算法更新。 </p></li><li><p>计算分模型k对观测数据yi的响应度r。E：step</p><script type="math/tex; mode=display">      \gamma_{jk}=\frac{a_k\varPhi(y_i|\theta)}{{\Sigma}a_k\varPhi(y_i|\theta)}    </script></li><li><p>根据相应度，迭代新一论参数。M：step</p><script type="math/tex; mode=display">      \mu_k=\frac{\Sigma\gamma_{jk}y_j}{\Sigma_{j=1}\gamma_{jk}}     </script><script type="math/tex; mode=display">  \sigma^2=\frac{\Sigma\gamma_{jk}(y_j-\mu_{k})^2}{\Sigma_{j=1}\gamma_{ij}}</script><script type="math/tex; mode=display">  a_{k} = \frac{\Sigma\gamma_{jk}}{N}</script></li></ul><ol><li>对HMM进行学习建模，数转移状态转移cout，得到发射矩阵，转移矩阵。即为HMM参数。</li><li>重对齐: Vertibi算法进行解码，预测输入freatures实际的状态序列。</li><li>由一个状态对应的多个帧继续对GMM状态进行更新。循环依次迭代。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> ASR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kaldi-libspeech</title>
      <link href="2021/05/01/kaldi-libspeech/"/>
      <url>2021/05/01/kaldi-libspeech/</url>
      
        <content type="html"><![CDATA[<h2 id="kaldi"><a href="#kaldi" class="headerlink" title="kaldi"></a>kaldi</h2><ul><li>模型初始化：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 单音速模型训练</span></span><br><span class="line">if [ $stage -le 8 ]; then</span><br><span class="line"><span class="meta">  #</span><span class="bash"> train a monophone system</span></span><br><span class="line">  steps/train_mono.sh --boost-silence 1.25 --nj 20 --cmd "$train_cmd" \</span><br><span class="line">                      data/train_2kshort data/lang_nosp exp/mono</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash"> boost-silence 1.25: 表示</span></span><br></pre></td></tr></table></figure></li></ul><ol><li>train_mono.sh内部过程</li></ol><ul><li>初始化单音素模型。调用gmm-init-mono，生成0.mdl、tree。</li><li>调用compile-train-graph生成text中每句抄本对应的fst，存放在fsts.JOB.gz</li><li>第一次对齐数据。调用align-equal-stats-ali生成对齐状态序列，通过管道传递给gmm-acc-stats-ali，得到更新参数时用到的统计量</li><li>第一次更新模型参数。调用gmm-est更新模型参数。</li><li>进入训练模型的主循环：在指定的对齐轮数，使用gmm-align-compiled对齐特征数据，得到新的对齐状态序列；每一轮都调用gmm-acc-stats-ali计算更新模型参数所用到的统计量，然后调用gmm-est更新模型参数，并且在每一轮中增加GMM的分量个数。</li></ul><h1 id=""><a href="#" class="headerlink" title="#"></a>#</h1><ol><li>单音素训练：train_mono.sh</li><li>解码测试：decode.sh</li><li>单音素对齐：align_si.sh</li><li>三音素训练<ul><li>tri1：</li></ul><ol><li>train_deltas.sh</li><li>decode.sh</li><li>align_si.sh</li></ol><ul><li>tri2：</li></ul><ol><li>train_deltas.sh</li><li>decode.sh</li><li>align_si.sh</li></ol></li><li>tri3a：区分性训练（LDA+MLLT）</li><li>tri4a：说话人自适应（FMLLR）</li><li>tri5a：SAT</li><li>nnet3/run_tdnn.sh</li><li>chain/run_tdnn.sh</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> ASR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>melgan</title>
      <link href="2021/05/01/melgan/"/>
      <url>2021/05/01/melgan/</url>
      
        <content type="html"><![CDATA[<h1 id="MelGan"><a href="#MelGan" class="headerlink" title="MelGan"></a>MelGan</h1><p>通过输入mel谱特征，生成wav数据，使用真实wav数据进行判别训练。<br>loss1：hingloss衡量生成wav与实际wav的差异。<br>loss2：衡量真实</p><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><ul><li>多层Conv1d: 对Mel频谱时域信息进行建模。使用weight norm能够让learning rate自适应/自稳定。</li><li>2 * (Unsampling Layer + Residal Block):</li></ul><ol><li>Unsampleing Layer：</li><li>Residual Block：<ol><li></li></ol></li></ol><ul><li>由对齐的mel谱进行上采样到音频，采用反一维卷积完成，帧数*上采样倍数=采样点个数</li></ul><h2 id="Descrimtor"><a href="#Descrimtor" class="headerlink" title="Descrimtor"></a>Descrimtor</h2>]]></content>
      
      
      
        <tags>
            
            <tag> TTS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tacotron2</title>
      <link href="2021/02/10/tacotron2/"/>
      <url>2021/02/10/tacotron2/</url>
      
        <content type="html"><![CDATA[<h2 id="Tacotron1："><a href="#Tacotron1：" class="headerlink" title="Tacotron1："></a>Tacotron1：</h2><h3 id="input-embedding"><a href="#input-embedding" class="headerlink" title="input_embedding"></a>input_embedding</h3><h3 id="prenet"><a href="#prenet" class="headerlink" title="prenet"></a>prenet</h3><ul><li>prenet是通过 使用两层全连接映射 实现 [N,T,256] -&gt; [N,T,256] -&gt; [N,T,128]作用是对输入进行初步特征提取和dropout以提升泛化能力</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prenet</span><span class="params">(inputs, is_training, layer_sizes, scope=None)</span>:</span></span><br><span class="line">  x = inputs</span><br><span class="line">  drop_rate = <span class="number">0.5</span> <span class="keyword">if</span> is_training <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(scope <span class="keyword">or</span> <span class="string">'prenet'</span>):</span><br><span class="line">    <span class="keyword">for</span> i, size <span class="keyword">in</span> enumerate(layer_sizes):</span><br><span class="line">      dense = tf.layers.dense(x, units=size, activation=tf.nn.relu, name=<span class="string">'dense_%d'</span> % (i+<span class="number">1</span>))</span><br><span class="line">      x = tf.layers.dropout(dense, rate=drop_rate, training=is_training, name=<span class="string">'dropout_%d'</span> % (i+<span class="number">1</span>))</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="encoder-cbhg"><a href="#encoder-cbhg" class="headerlink" title="encoder_cbhg"></a>encoder_cbhg</h3><ul><li>encoder_cbhg是输入端的cbhg结构: 包含三部分 </li></ul><ol><li>Convolution Bank</li></ol><ul><li>这一部分对于特征提取的思路是用height=1, width=word_dim的filter 过滤得到每一个input_embedding的特征值，然后相邻两个word一起过滤得到局部窗口为2的上下文特征值，即2-gram，一直提取出 16-gram。128个filter用于关注不同的特征值。</li><li>大体上和textCNN的思路差不多，都是n-gram的思想，不同的是这里面进行卷积操作的时候使用的padding=‘same’ 为了保证对齐，textCNN里面我们用的时候使用 padding=’valid’ 然后用reduce_max进行对齐。</li><li>encoder_cbhg使用的 kernel_size 为 [1,2,3,…,16] filter_size = 128<br>每个conv1d 结果按照 axis = -1 拼接起来后的shape 为 [batch_size, Time_stmp,128*16]</li></ul><ol><li>Highway Network</li></ol><ul><li>在residual connection 后 接了4层 highway net。highway net 就是加速的深层DNN。result shape = [batch_size,Time_stmp,128]</li></ul><ol><li>BiGRU</li></ol><ul><li>输入是 highway net 的输出 shape = [batch_size,Time_stmp,128] 经过双向rnn 后 由于 num_units = 128 , 双向拼接后为 256 故而 result shape = [batch_size,Time_stmp,256]</li></ul><h1 id="Tacotron2"><a href="#Tacotron2" class="headerlink" title="Tacotron2"></a>Tacotron2</h1><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2>]]></content>
      
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word2vec</title>
      <link href="2020/04/22/word2vec/"/>
      <url>2020/04/22/word2vec/</url>
      
        <content type="html"><![CDATA[<h1 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h1><h3 id="一-wordvec简介"><a href="#一-wordvec简介" class="headerlink" title="一. wordvec简介"></a>一. wordvec简介</h3><ol><li>wordvec是一种训练词向量分布式表示（distrubute representation）的方式。</li><li>分为cbow和skip-gram两种训练方式。cbow通过上下文词汇作为输入，中间词汇作为输出。skip-gram通过中间词汇作为输入，上下文词汇作为输出，进行训练。<ul><li>如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』。适合大规模数据集中。</li><li>而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』。适合少量数据集中。</li></ul></li><li>通常的文本数据中，词库少则数万，多则百万，在训练中直接训练多分类逻辑回归并不现实。word2vec中提供了两种针对大规模多分类问题的优化手段， negative sampling（负采样）和hierarchical softmax（层次softmax）。</li></ol><h3 id="二-负采样"><a href="#二-负采样" class="headerlink" title="二. 负采样"></a>二. 负采样</h3><ol><li>在优化中，negative sampling 只更新少量负面类，从而减轻了计算量。hierarchical softmax 将词库表示成前缀树，从树根到叶子的路径可以表示为一系列二分类器，一次多分类计算的复杂度从|V|降低到了树的高度。</li></ol><p><img src="\img\article\20190319100122167.png" alt="20190319100122167"></p><ol><li><p>负采样，negative sampling。是指在skip-gram过程中。将中间词汇和上下文正确匹配的词汇形成正例。而把匹配失败的样本称为负例子。当训练样本过大时候，负样本数量是远远大于正样本数量的。因此如果使用p（w1|w2）常规的方法来计算损失函数，时间复杂度会很大。</p></li><li><p>采用负采样技术，在负样本中随机选择部分进行损失函数的计算。</p></li><li><p>word2vec源代码中使用tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes)。range_max=num_classes。</p></li><li>默认情况下，他会用log_uniform_candidate_sampler去采样。那么log_uniform_candidate_sampler是怎么采样的呢？他的实现在这里：1、会在[0, range_max)中采样出一个整数k。2、P(k) = (log(k + 2) - log(k + 1)) / log(range_max + 1)</li><li>TF的word2vec实现里，词频越大，词的类别编号也就越小。因此，在TF的word2vec里，负采样的过程其实就是优先采词频高的词作为负样本。</li></ol><h3 id="三-层次softmax"><a href="#三-层次softmax" class="headerlink" title="三. 层次softmax"></a>三. 层次softmax</h3><p>​    在层次softmax模型中，叶子结点的词没有直接输出的向量，而非叶子节点都有响应的输在在模型的训练过程中，通过Huffman编码，构造了一颗庞大的Huffman树，同时会给非叶子结点赋予向量。我们要计算的是目标词w的概率，这个概率的具体含义，是指从root结点开始随机走，走到目标词w的概率。因此在途中路过非叶子结点（包括root）时，需要分别知道往左走和往右走的概率。例如到达非叶子节点n的时候往左边走和往右边走的概率分别是：</p><p><img src="\img\article\20190319100307838.png" alt="20190319100307838"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅析文本相似度</title>
      <link href="2020/04/09/textSimilarily/"/>
      <url>2020/04/09/textSimilarily/</url>
      
        <content type="html"><![CDATA[<h2 id="文本相似度"><a href="#文本相似度" class="headerlink" title="文本相似度"></a>文本相似度</h2><p>​    在自然语言处理(Natural Language Processing, NLP)中，经常会涉及到如何度量两个文本的相似度问题。在诸如对话系统(Dialog system)和信息检索(Information retrieval)等的问题中，如何度量句子或者短语之间的相似度尤为重要。</p><h3 id="1-基于关键词匹配的传统方法"><a href="#1-基于关键词匹配的传统方法" class="headerlink" title="1.基于关键词匹配的传统方法"></a>1.基于关键词匹配的传统方法</h3><h4 id="1-1-n-gram相似度"><a href="#1-1-n-gram相似度" class="headerlink" title="1.1 n-gram相似度"></a>1.1 n-gram相似度</h4><ul><li>基于N-Gram模型定义的句子（字符串）相似度是一种模糊匹配方式，通过两个长得很像的句子间的“差异”来衡量相似度。</li><li>N-Gram相似度的计算是指按长度N切分原句得到词段，也就是原句中所有长度为N的子字符串。对于两个句子S和T，则可以从共有子串的数量上去定义两个句子的相似度。</li></ul><script type="math/tex; mode=display">Similarity=|G_{N}(S)|+|G_{N}(T)|−2∗|G_{N}(S)∩G_{N}(T)|</script><ul><li>其中，GN(S)GN(S)和GN(T)GN(T)分别表示字符串S和T中N-Gram的集合，N一般取2或3。字符串距离越近，它们就越相似，当两个字符串完全相等时，距离为0。</li></ul><h4 id="1-2-Jaccard相似度"><a href="#1-2-Jaccard相似度" class="headerlink" title="1.2 Jaccard相似度"></a>1.2 Jaccard相似度</h4><ul><li>Jaccard相似度的计算相对简单，原理也容易理解。就是计算两个句子之间词集合的交集和并集的比值。该值越大，表示两个句子越相似，在涉及大规模并行运算的时候，该方法在效率上有一定的优势，公式如下：<script type="math/tex; mode=display">J(A,B)=\frac{|A∩B|}{|A∪B|}</script></li></ul><h3 id="2-基于向量空间"><a href="#2-基于向量空间" class="headerlink" title="2.基于向量空间"></a>2.基于向量空间</h3><p>将文本映射到向量空间，利用余弦相似度等方法:<strong>Distributed representation</strong></p><h4 id="2-1-word2vec"><a href="#2-1-word2vec" class="headerlink" title="2.1 word2vec:"></a>2.1 word2vec:</h4><ul><li>编码到向量空间，求其余弦距离。</li></ul><h4 id="2-2-TF-IDF："><a href="#2-2-TF-IDF：" class="headerlink" title="2.2 TF-IDF："></a>2.2 TF-IDF：</h4><ul><li>通过计算出文档中每个词的TF-IDF值，然后结合相似度计算方法（一般采用余弦相似度）就可以计算两个文档的相似度。采用TF-IDF的前提是“文章的词语重要性与词语在文章中出现的位置不相关”。</li></ul><h4 id="2-3-计算距离"><a href="#2-3-计算距离" class="headerlink" title="2.3 计算距离"></a>2.3 计算距离</h4><ol><li><p><strong>欧式距离</strong></p><script type="math/tex; mode=display">d=\sqrt{\sum_{i=1}^{n}(xi−yi)2}</script></li><li><p><strong>曼哈顿距离</strong></p><script type="math/tex; mode=display">d=\sum_{i=1}^{n}|xi−yi|</script></li><li><p><strong>余弦相似度</strong></p><script type="math/tex; mode=display">d=\frac{\sum_{i=1}^{n}xi*yi}{\sqrt{\sum_{i=1}^{n}xi^2\sum_{i=1}^{n}yi^2}}</script></li></ol><h3 id="3-基于深度学习"><a href="#3-基于深度学习" class="headerlink" title="3.基于深度学习"></a>3.基于深度学习</h3><p>​    随着深度学习在图像和语音识别中取得不错的进展，近些年深度学习也开始应用于自然语言处理的不同应用中。语义相似性匹配问题已经逐渐从人工设计特征转向分布式表达和神经网络结构相结合的方式</p><ul><li>利用深度网络学习语义匹配模型：</li><li>DSSM，ConvNet，SiameseLSTM</li><li>DIIN，DRCN，BIMPM等。</li><li>详细看相关论文</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双向多视角匹配BiMPM</title>
      <link href="2020/04/09/textMatch-BiMPM/"/>
      <url>2020/04/09/textMatch-BiMPM/</url>
      
        <content type="html"><![CDATA[<h2 id="BiMPM：双向多视角匹配"><a href="#BiMPM：双向多视角匹配" class="headerlink" title="BiMPM：双向多视角匹配"></a>BiMPM：双向多视角匹配</h2><p>​    文本蕴含或者自然语言推理任务，就是判断后一句话（假设句）能否从前一句话（前提句）中推断出来。</p><p>​    在BiMPM论文提出前，大量的学者在对两句话进行匹配时，常常仅考虑到前一句话对后一句话，或后一句话对前一句话的单向语义匹配，忽略了双向语义匹配的重要性；</p><p>​    并且一般只考虑单一粒度的语义匹配（逐字或逐句）。基于以上不足，该论文作者提出了bilateral multi-perspective matching （BiMPM）model，即双向多视角匹配模型。</p><h3 id="2-Context-Representation-Layer"><a href="#2-Context-Representation-Layer" class="headerlink" title="2.Context Representation Layer"></a>2.Context Representation Layer</h3><p>​    将上下文信息融入到句子P和句子Q的每一个time step中。首先用一个双向LSTM编码句子p中每一个time step的上下文embedding。</p><p>​    使用同一个双向LSTM，编码q中的每一个time step的上下文embedding。即将该双向LSTM进行权值共享。</p><h3 id="3-Matching-Layer"><a href="#3-Matching-Layer" class="headerlink" title="3.Matching Layer"></a>3.Matching Layer</h3><p>​    这一层是本模型的核心，也是亮点。本层的目的是用一句话中每一个time step的上下文向量去匹配另一句话中所有time steps的上下文向量。如图1所示，本模型从两个方向（P-&gt;Q和Q-&gt;P）去匹配句子P和句子Q的上下文向量。下面仅从一个方向P-&gt;Q，详细讲一下多视角匹配算法，另一方向Q-&gt;P与其相同。</p><p>多视角匹配算法包含两步：</p><p>（1）定义了多视角余弦匹配函数 fm去比较两个向量，即 <img src="https://www.zhihu.com/equation?tex=m%3Df_%7Bm%7D%28v_%7B1%7D%2Cv_%7B2%7D%3BW%29" alt="[公式]"> </p><p>其中v1和v2是d维的向量。W（lxd）是一个可训练的权重参数，l是视角的数目，也就是共有几个视角（可以理解成CNN做卷积时的多个filters）。返回值m是l维的向量<img src="https://www.zhihu.com/equation?tex=m%3D%5Bm_%7B1%7D%2C...m_%7Bk%7D%2C...m_%7Bl%7D%5D" alt="[公式]">。</p><p>其中mk是第k个视角的向量余弦匹配值，即 <img src="https://www.zhihu.com/equation?tex=m_%7Bk%7D%3Dcosine%28W_%7Bk%7D%5Ccirc+v_%7B1%7D%2CW_%7Bk%7D%5Ccirc+v_%7B2%7D%29" alt="[公式]"> </p><p>其中wk是第k行W的值</p><p>（2）基于 <img src="https://www.zhihu.com/equation?tex=f_%7Bm%7D" alt="[公式]"> 函数，本模型给出了四种匹配策略，分别是full-matching、maxpooling-matching、attentive-matching和max-attentive-matchong，如图2所示。</p><p><img src="\img\article\v2-6f5d775be674bfe4a73ac0f7c10ccae2_r.jpg" alt="v2-6f5d775be674bfe4a73ac0f7c10ccae2_r"></p><h4 id="1-full-matching"><a href="#1-full-matching" class="headerlink" title="1) full-matching"></a>1) full-matching</h4><p>​    在这中匹配策略中，我们将句子P中每一个time step的上下文向量（包含向前和向后上下文向量）分别与句子Q中最后一个time step的上下文向量（向前和向后上下文向量）计算余弦匹配值</p><p><img src="\img\article\v2-7d83edaa4ad519d991e6d15a3484f227_720w.jpg" alt="v2-7d83edaa4ad519d991e6d15a3484f227_720w"></p><h4 id="2-maxpooling-matching"><a href="#2-maxpooling-matching" class="headerlink" title="2) maxpooling-matching"></a>2) maxpooling-matching</h4><p>​    我们将句子P中每一个time step的上下文向量（包含向前和向后上下文向量）分别与句子Q中每一个time step的上下文向量（向前和向后上下文向量）计算余弦匹配值，但最后在与句子Q的每一个time step中选取最大的余弦匹配值</p><p><img src="\img\article\v2-abb4b0e0f6ea3d382c11a251d30ede44_r.jpg" alt="v2-abb4b0e0f6ea3d382c11a251d30ede44_r"></p><h4 id="3-attentive-matching"><a href="#3-attentive-matching" class="headerlink" title="3) attentive-matching"></a>3) attentive-matching</h4><p>​    我们先对句子P和句子Q中每一个time step的上下文向量（包含向前和向后上下文向量）计算余弦相似度（这里值得注意的一点是，余弦匹配值与余弦相似度是不一样的，余弦匹配值在计算时对两个向量赋予了权重值，而余弦相似度则是直接对两个向量进行计算），得到相似度矩阵。</p><p><img src="\img\article\v2-7fa2c277fd1a5a3f52b674ddd43df362_r.jpg" alt="v2-7fa2c277fd1a5a3f52b674ddd43df362_r"></p><p>​    我们将相似度矩阵，作为句子Q中每一个time step的权值，然后通过对句子Q的所有上下文向量加权求和，计算出整个句子Q的注意力向量。</p><p><img src="\img\article\v2-2213b5e3fdecd1d78a0cae64518b2cf5_720w.jpg" alt="v2-2213b5e3fdecd1d78a0cae64518b2cf5_720w"></p><p>最后，将句子P中每一个time step的上下文向量（包含向前和向后上下文向量）分别与句子Q的注意力向量计算余弦匹配值，即</p><p><img src="\img\article\v2-9e2ef86e2f00c986d10802ffe94fb4c1_720w.jpg" alt="v2-9e2ef86e2f00c986d10802ffe94fb4c1_720w"></p><h4 id="4-max-attentive-matching"><a href="#4-max-attentive-matching" class="headerlink" title="4) max-attentive-matching"></a>4) max-attentive-matching</h4><p>​    这种匹配策略与attentive-matching的匹配策略相似，不同的是，该匹配策略没有对句子Q的所有上下文向量加权求和来得到句子Q的注意力向量，而是选择句子Q所有上下文向量中余弦相似度最大的向量作为句子Q的注意力向量。</p><h3 id="4-aggregation-Layer"><a href="#4-aggregation-Layer" class="headerlink" title="4. aggregation Layer"></a>4. aggregation Layer</h3><p>​    这一层的目的是将两个序列的匹配向量聚合成一个固定长度的匹配向量。本模型利用另一个双向LSTM，将其分别应用于两个序列的匹配向量。然后，通过将双向LSTM的最后一个time step的向量串联起来（图2中四个绿色的向量），聚合成固定长度的匹配向量。</p><h3 id="5-Prediction-Layer"><a href="#5-Prediction-Layer" class="headerlink" title="5. Prediction Layer"></a>5. Prediction Layer</h3><p>​    这一层的目的是为了得到最终的预测结果。本模型将聚合得到的匹配向量，连接两层全连接层，并且在最后输出做softmax激活，最后得到文本蕴含的结果。</p><h3 id="6-参数"><a href="#6-参数" class="headerlink" title="6. 参数"></a>6. 参数</h3><p>​    该论文中，word embedding为300维，character embedding为20维，得到的character-composed embedding为50维；所有的双向LSTM的隐层节点数为100，dropout rate为0.1，学习率为0.001，采用adam优化器。论文中，对五种（1, 5, 10, 15, 20）不同大小的视角进行实验分析，发现视角个数为20时，模型效果最好</p>]]></content>
      
      
      
        <tags>
            
            <tag> 文本匹配 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>密集循环注意力网络DRCN</title>
      <link href="2020/04/09/textMatch-DRCN/"/>
      <url>2020/04/09/textMatch-DRCN/</url>
      
        <content type="html"><![CDATA[<h2 id="DRCN：密集循环注意力网络"><a href="#DRCN：密集循环注意力网络" class="headerlink" title="DRCN：密集循环注意力网络"></a>DRCN：密集循环注意力网络</h2><h3 id="1-Word-Representation-Layer"><a href="#1-Word-Representation-Layer" class="headerlink" title="1.Word Representation Layer"></a>1.Word Representation Layer</h3><p>​    将输入的两个句子表示为P和Q，长度分别为I和J。每个词的word representation feature由三部分构成：word embedding, character representation, the exact matched flag。</p><ol><li>其中word embedding包括固定好的预训练模型（GloVe或者Word2Vec）和可以训练的WordEmbedding。可训练的wordembedding会导致过拟合。固定的wordembedding缺少特殊场景的数据会很健壮。融合选取。</li><li>Character Embedding是随着模型训练的。</li><li>the exact matched flag则表示代表词是否在其他句子中出现。类似DIIN中的EM标志。</li></ol><h3 id="2-Attentively-Connected-RNN"><a href="#2-Attentively-Connected-RNN" class="headerlink" title="2.Attentively Connected RNN"></a>2.Attentively Connected RNN</h3><h4 id="2-1Densely-connected-Recurrent-Networks"><a href="#2-1Densely-connected-Recurrent-Networks" class="headerlink" title="2.1Densely-connected Recurrent Networks"></a>2.1Densely-connected Recurrent Networks</h4><p>​    传统的多层RNN结构在层数加深的过程中会遭遇梯度消失或梯度爆炸等问题。为了解决此问题，研究者们提出了残差网络ResNet，即在网络前向传播中增加跳跃传播的过程，将信息更有效地传播至更深的层。</p><p>​    但是ResNet的做法是将输入与输出相加作为下一层的输入。这样输入的信息会在传递的过程中产生一些影响。</p><p>​    借用DenseNet将输入和输出串联起来。通道数维度上进行连接。很好的进行传递。</p><h4 id="2-2Densely-connected-Co-attentive-Networks"><a href="#2-2Densely-connected-Co-attentive-Networks" class="headerlink" title="2.2Densely-connected Co-attentive Networks"></a>2.2Densely-connected Co-attentive Networks</h4><p>​    Attention机制可以在两个句子间建立对应关系，在多层RNN的每层中计算P和Q的attention信息，通过串联拼接得到每层RNN的输出。</p><p><img src="\img\article\v2-a3baf1e6e428303b7414d0eb0c927c74_hd.jpg" alt="v2-a3baf1e6e428303b7414d0eb0c927c74_hd"></p><p><img src="\img\article\v2-642056e4a761b6487c768e33ffe3cf1b_hd.jpg" alt="v2-642056e4a761b6487c768e33ffe3cf1b_hd"></p><h4 id="2-3Bottleneck-Component"><a href="#2-3Bottleneck-Component" class="headerlink" title="2.3Bottleneck Component"></a>2.3Bottleneck Component</h4><p>​    由于在多层RNN中进行了拼接操作，随着层数的加深，需要训练的参数会大量增加，因此引入自编码模块进行特征维数的压缩，实验证明，此操作对测试集起到了正则作用，提高了测试准确率。</p><h3 id="3-Interaction-and-Prediction-Layer"><a href="#3-Interaction-and-Prediction-Layer" class="headerlink" title="3.Interaction and Prediction Layer"></a>3.Interaction and Prediction Layer</h3><p>​    由于P、Q的长度不同，本文通过Max Pooling对每个句子分别得到其向量表示p、q，计算得到含有语义匹配信息的向量v，具体如下：</p><p><img src="\img\article\v2-fe65e454c2db3aeb00a2b911f2f4b729_hd.jpg" alt="v2-fe65e454c2db3aeb00a2b911f2f4b729_hd"></p><p>最后将v输入全连接层，并根据具体任务通过softmax进行分类。</p><p>本文整体架构是一个端到端的过程，损失函数定义为交叉熵损失与自编码器的重建损失之和，并选用RMSProp算法进行参数优化。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 文本匹配 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>密集交互推断网络DIIN</title>
      <link href="2020/04/09/textMatch-DIIN/"/>
      <url>2020/04/09/textMatch-DIIN/</url>
      
        <content type="html"><![CDATA[<h2 id="DIIN：密集交互推断网络"><a href="#DIIN：密集交互推断网络" class="headerlink" title="DIIN：密集交互推断网络"></a>DIIN：密集交互推断网络</h2><h3 id="1-Embedding-Layer"><a href="#1-Embedding-Layer" class="headerlink" title="1. Embedding Layer:"></a>1. Embedding Layer:</h3><p>将词向量，字符特征向量和句法特征向量 进行串联。</p><p>词向量可以是通过预训练模型word2vec拿到。</p><p>字符特征向量是将字符向量 经过一维卷积，然后最大池化得到。</p><p>句法特征向量由词性Pos特征向量和二进制精确匹配EM特征向量组成。如果在另一个句子中有与该词具有相同词干或相同词元的词，那么EM值会被激活。</p><h3 id="2-Eocoding-Layer"><a href="#2-Eocoding-Layer" class="headerlink" title="2. Eocoding Layer:"></a>2. Eocoding Layer:</h3><p>这一层主要是对输入v进行highway network + self attention + fuse gate过程</p><ol><li>attention的计算为W[a;a;axa]</li><li>对attnetion进行softmax按照权重乘以v结果为attention</li><li>将v与attention进行concat串联为v_hat。并计算z,r,f。z,r,f分别为v_hat的线性变换。</li><li>其中z使用tanh激活函数。r,f使用sigmoid激活函数。</li><li>res = r*v+f*z</li></ol><h3 id="3-Interaction-Layer"><a href="#3-Interaction-Layer" class="headerlink" title="3. Interaction Layer"></a>3. Interaction Layer</h3><p>对p和h进行点乘，结果为I</p><h3 id="4-Feature-Extraction-layer"><a href="#4-Feature-Extraction-layer" class="headerlink" title="4. Feature Extraction layer"></a>4. Feature Extraction layer</h3><p>将相关性结果I通过DenseNet进行特征提取</p><ul><li>相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入</li></ul><ol><li>DenseNet的网络结构主要由DenseBlock和Transition组成</li><li>在DenseBlock中，各个层的特征图大小一致，可以在channel维度上连接。DenseBlock中的非线性组合函数 <img src="https://www.zhihu.com/equation?tex=H%28%5Ccdot%29" alt="[公式]"> 采用的是<strong>BN+ReLU+3x3 Conv</strong>的结构</li><li>对于Transition层，它主要是连接两个相邻的DenseBlock，并且降低特征图大小。Transition层包括一个1x1的卷积和2x2的AvgPooling，结构为<strong>BN+ReLU+1x1 Conv+2x2 AvgPooling</strong></li></ol><ul><li>由于密集连接方式，DenseNet提升了梯度的反向传播，使得网络更容易训练。由于每层可以直达最后的误差信号，实现了隐式的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.5185">“deep supervision”</a>；</li><li>参数更小且计算更高效，这有点违反直觉，由于DenseNet是通过concat特征来实现短路连接，实现了特征重用，并且采用较小的growth rate，每个层所独有的特征图是比较小的；</li><li>由于特征复用，最后的分类器使用了低级特征。</li></ul><h3 id="5-Output-Layer"><a href="#5-Output-Layer" class="headerlink" title="5. Output Layer"></a>5. Output Layer</h3><p>dense+softmax到分类类别个数上。对于匹配模型，就是0，1。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 文本匹配 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FastText</title>
      <link href="2020/04/02/fastText/"/>
      <url>2020/04/02/fastText/</url>
      
        <content type="html"><![CDATA[<h1 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h1><ul><li>fastText是Facebook于2016年开源的一个词向量计算和文本分类工具，在学术上并没有太大创新。但是它的优点也非常明显，在文本分类任务中，fastText（浅层网络）往往能取得和深度网络相媲美的精度，却在训练时间上比深度网络快许多数量级。在标准的多核CPU上， 能够训练10亿词级别语料库的词向量在10分钟之内，能够分类有着30万多类别的50多万句子在1分钟之内。</li></ul><p>- <a href="https://arxiv.org/pdf/1607.01759v2.pdf" target="_blank" rel="noopener">paper click this</a></p><h2 id="一-faxtText简介"><a href="#一-faxtText简介" class="headerlink" title="一. faxtText简介"></a>一. faxtText简介</h2><ul><li>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：</li></ul><p>1、fastText在保持高精度的情况下加快了训练速度和测试速度</p><p>2、fastText不需要预训练好的词向量，fastText会自己训练词向量</p><p>3、fastText两个重要的优化：Hierarchical Softmax、N-gram。</p><p>4、fasttext分类方法是：计算每个输入的词向量，并对所有词向量求平均，然后进行softmax分类。</p><h2 id="二-N-gram特征"><a href="#二-N-gram特征" class="headerlink" title="二. N-gram特征"></a>二. N-gram特征</h2><ol><li><p>n-gram是基于语言模型的算法，基本思想是将文本内容按照子节顺序进行大小为N的窗口滑动操作，最终形成窗口为N的字节片段序列。而且需要额外注意一点是n-gram可以根据粒度不同有不同的含义，有字粒度的n-gram和词粒度的n-gram，下面分别给出了字粒度和词粒度的例子：</p><p>  <img src="\img\article\20190319100517714.png" alt="20190319100517714"></p><p><img src="\img\article\20190319100526877.png" alt="20190319100526877"></p></li><li><p>对于文本句子的n-gram来说，如上面所说可以是字粒度或者是词粒度，同时n-gram也可以在字符级别工作，例如对单个单词matter来说，假设采用3-gram特征，那么matter可以表示成图中五个3-gram特征，这五个特征都有各自的词向量，五个特征的词向量和即为matter这个词的向其中“&lt;”和“&gt;”是作为边界符号被添加，来将一个单词的ngrams与单词本身区分开来：</p><p><img src="\img\article\20190319100559177.png" alt="20190319100559177"></p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hmm_and_crf</title>
      <link href="2020/04/02/hmm-and-crf/"/>
      <url>2020/04/02/hmm-and-crf/</url>
      
        <content type="html"><![CDATA[<h1 id="HMM分词"><a href="#HMM分词" class="headerlink" title="HMM分词"></a>HMM分词</h1><blockquote><p>解决三个问题：模型λ=（A，B，π）。A为状态转移概率矩阵，B是观测概率矩阵。π是初始状态概率向量。<br>π和A决定状态序列。B决定观测序列。O为观测序列，I为状态序列。</p></blockquote><ol><li>概率计算问题：已知（A，B，π），和O观测序列。求：O观测序列出现概率P(O|λ)。 </li><li>学习问题：已知O观测序列，估计模型的参数λ，使P(O|λ)最大。</li><li>预测问题：也就是解码问题。已知λ=（A，B，π）和观测序列O，求给定观测序列条件概率P（I|O）最大的状态序列I。</li></ol><ul><li>篱笆网络（Lattice）求最短路径问题采用维特比算法进行解码。维特比算法是采用动态规划求最短路径的方案。实现的时间复杂度为O(d*N^2)。d为序列长度，N为状态数量。</li><li>根据一条路径A{1,2,3,4}-B{1,2,3,4}-C{1,2,3,4}-D{1,2,3,4}通过维特比算法求其最短路径方案。A{1-4}-B{1-4}有16种方案。保留其A{1-4}-B1最优的一种。保留A{1-4}-B2最优的一种，保留A{1-4}-B3最优的一种，保留A{1-4}-B4最优的一种。根据这四种继续往C计算16种，再保留分别经过C1，C2，C3，C4的最优的一种。直到D。这就是维特比算法。每次16中路径中保留了最优的四种方案。时间复杂度为O（序列长度乘以状态个数的平方）。</li></ul><h1 id="CRF-Condition-Random-Field"><a href="#CRF-Condition-Random-Field" class="headerlink" title="CRF(Condition Random Field)"></a>CRF(Condition Random Field)</h1><p>定义：条件随机场是给定随机变量X条件下，随机变量Y的马尔科夫随机场。满足 <img src="https://www.zhihu.com/equation?tex=P%28Y_%7Bv%7D%7CX%2CY_%7Bw%7D%2Cw+%5Cneq+v%29+%3D+P%28Y_%7Bv%7D%7CX%2CY_%7Bw%7D%2Cw+%5Csim+v%29+" alt="[公式]"> 的马尔科夫随机场叫做条件随机场（CRF）</p><h1 id="概率图模型："><a href="#概率图模型：" class="headerlink" title="概率图模型："></a>概率图模型：</h1><h3 id="有向图模型："><a href="#有向图模型：" class="headerlink" title="有向图模型："></a>有向图模型：</h3><p><img src="\img\article\v2-5b3f6b4a2d905297b7f73a89e92ee618_r.jpg" alt="v2-5b3f6b4a2d905297b7f73a89e92ee618_r"></p><p>联合概率P（X）=p(x1)p(x2|x1)p(x3|x2)p(x4|x2)p(x5|x3,x4)</p><h3 id="无向图模型："><a href="#无向图模型：" class="headerlink" title="无向图模型："></a>无向图模型：</h3><p><img src="\img\article\wuxiangtu.png" alt="wuxiangtu"></p><p>可以用因子分解将 <img src="https://www.zhihu.com/equation?tex=P%3D%28Y%29" alt="[公式]"> 写为若干个联合概率的乘积。咋分解呢，将一个图分为若干个“小团”，注意每个团必须是“最大团”。结果为所有最大团上势函数的乘积。</p><p><img src="\img\article\wuxiangtugailv.png" alt=""></p><p>Z(x)为规范化因子，即<img src="\img\article\wuxiangtuguifanhuayinzi.png" alt="wuxiangtuguifanhuayinzi"></p><p>所以对于上图的无向图而言，</p><p><img src="\img\article\wuxiangtugailv1.png" alt="wuxiangtugailv1"></p><p>其中<img src="https://www.zhihu.com/equation?tex=+%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29" alt="[公式]"> 是一个最大团 <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]"> 上随机变量们的联合概率，一般取指数函数的：</p><p><img src="\img\article\zuidatuan.png" alt="zuidatuan"></p><p>管这个东西叫做势函数。其中有点CRF的影子。</p><p>那么概率无向图的联合概率分布可以在因子分解下表示为：</p><p><img src="\img\article\yinshifenjie.png" alt="yinshifenjie"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TextClassify</title>
      <link href="2019/12/05/TextClassify/"/>
      <url>2019/12/05/TextClassify/</url>
      
        <content type="html"><![CDATA[<h1 id="TextRank算法"><a href="#TextRank算法" class="headerlink" title="TextRank算法"></a>TextRank算法</h1><h2 id="pagerank"><a href="#pagerank" class="headerlink" title="pagerank"></a>pagerank</h2><ul><li>TextRank来源于PageRank算法，PageRank算法如下所示：</li></ul><p><img src="\img\article\pagerank01.png" style="zoom: 67%;" /></p><ol><li>d为固定系数0.85，S(Vi)表示Vi的权重，In[Vi]为Vi的入度的节点集合，Out(Vj)为Vj出度的节点的集合。绝对值表示其个数。</li><li>迭代n次后，网页中节点的权重参数不再变化，最后每一个节点的S(Vi)即为该节点的重要性。</li></ol><h2 id="TextRank"><a href="#TextRank" class="headerlink" title="TextRank"></a>TextRank</h2><ul><li><p>若在pagerank中将每个图中的节点换成句子，或者是关键词，则可以演进为TextRank算法。（计算公式如下所示d=0.85</p><p><img src="\img\article\image-20191120144110314.png" alt="image-20191120144110314"></p></li><li><p>对于摘要提取过程，图的信息主要来自于边的权重，边的权重是句子i和句子j的相似度。所以在迭代的过程中需要乘上边的权重。 </p></li><li><p>两个句子的相似程度计算如下：</p><p><img src="\img\article\textrank-01.png" style="zoom: 67%;" /></p><ol><li>分子是在两个句子中都出现的单词的数量。|Si|是句子i的单词数。 </li><li>这里采用jieba分词对每两个句子之前进行计算相似度。</li></ol></li><li><p>整体过程就是一个迭代过程，一次遍历，就是遍历所有节点。</p></li></ul><h2 id="关于textrank算法在弹幕提取中的应用："><a href="#关于textrank算法在弹幕提取中的应用：" class="headerlink" title="关于textrank算法在弹幕提取中的应用："></a>关于textrank算法在弹幕提取中的应用：</h2><ol><li>首先根据每一个直播间的前8000条弹幕（按频数选取，从高到底选取8000条，不用全部，全部都有几千万乃至上亿条，数据量过大不需要）。</li><li>初始设置特殊弹幕的权重为1，其他所有弹幕权重为0.5。</li><li>计算每两个句子之间的相似度，这里按照上式的公式来计算。</li><li>按照textrank算法迭代，直到图收敛。<ul><li>一次遍历计算所有句子的得分：根据(1-d)+d*所有入度的句子里的</li></ul></li><li>最后选出top50的弹幕作为自己提取的关键性弹幕。</li></ol><h1 id="TF-IDF算法"><a href="#TF-IDF算法" class="headerlink" title="TF-IDF算法"></a>TF-IDF算法</h1><ol><li><p>Text Frequency：统计出现次数最多的词：关键句在该文档中出现的频数</p></li><li><p>Inverse Document Frequency：IDF逆文档频率，log(文档的总数量/关键句在文档中出现次数+1)</p></li></ol><h2 id="关于tf-idf在弹幕提取中的应用"><a href="#关于tf-idf在弹幕提取中的应用" class="headerlink" title="关于tf-idf在弹幕提取中的应用"></a>关于tf-idf在弹幕提取中的应用</h2><ol><li>判断当前文本在当前直播间所有弹幕中出现的频数。这里需要根据文本相似度进行计算。不能直接根据弹幕的频数计算，因为，从数据库中读取的数据都是合并后的topN数据。</li><li>计算逆文档频率。log(sum/count+1)。sum表示文档总数，count表示当前弹幕在其他文档里出现的次数（加1是为了防止一次都没有出现）</li><li>TF*IDF就表示最后的弹幕的权重。按照权重topN来进行提取选择。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TfIdf</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__int__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_tf</span><span class="params">(self, lst)</span>:</span></span><br><span class="line">        if_dict = Counter(lst)</span><br><span class="line">        <span class="keyword">return</span> if_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_idf</span><span class="params">(self, lst, document_lst)</span>:</span></span><br><span class="line">        num = len(document_lst)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        idf_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> lst:</span><br><span class="line">            <span class="keyword">for</span> doc <span class="keyword">in</span> document_lst:</span><br><span class="line">                <span class="keyword">if</span> doc.get(item):</span><br><span class="line">                    count += <span class="number">1</span></span><br><span class="line">            idf_dict[item] = math.log(num/(count+<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> idf_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_tf_idf_scores</span><span class="params">(self, tf_scores, idf_scores, top_num)</span>:</span></span><br><span class="line">        result = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> k, (content, tf) <span class="keyword">in</span> enumerate(tf_scores.items()):</span><br><span class="line">            idf = idf_scores.get(content)</span><br><span class="line">            scores = tf * idf</span><br><span class="line">            result[content] = scores</span><br><span class="line">        result = sorted(result.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        result = result[:top_num]</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h1 id="LDA算法"><a href="#LDA算法" class="headerlink" title="LDA算法"></a>LDA算法</h1><h2 id="（Latent-Dirichlet-Allocation）-潜在狄利克雷分布"><a href="#（Latent-Dirichlet-Allocation）-潜在狄利克雷分布" class="headerlink" title="（Latent Dirichlet Allocation）:潜在狄利克雷分布"></a>（Latent Dirichlet Allocation）:潜在狄利克雷分布</h2><ul><li><p>它是一种无监督的贝叶斯模型。文档生成模型</p></li><li><p>认为一偏文档是有多个主题的，每个主题又对应着不同的词（这里可以理解为关键句）。一篇文档的构造过程，首先是一定的概率选择某个主题。 然后再在这个主题下以一定的概率选出某一个词，这样就生成了这篇文档的第一个词。不断重复这个过程，就生成了整篇文章 。（文档——主题——词）</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>boosting</title>
      <link href="2019/12/05/boosting/"/>
      <url>2019/12/05/boosting/</url>
      
        <content type="html"><![CDATA[<p>DT(决策树)</p><p>ID3：基于信息增益</p><p>C4.5：基于信息增益比</p><p>CART：基于尼基指数</p><blockquote><p>本文主要简单的比较常用的boosting算法的一些区别，从Adaboost到LightGBM。包括Adaboost，GBDT，XGBoost，LightGBM四个模型的简单介绍。</p></blockquote><h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><ol><li>初始化训练数据的权值分布，wi = 1/N</li><li>训练数据集，得到基本分类器。G(x)：x -&gt; {-1, 1}</li><li>计算分类误差率。（相加误分类权重e）</li><li>计算分类器系数am=(1/2)log(1-e/e)</li><li>更新权值分布，继续训练。</li><li>最终分类器等于所有弱分类器加权之和，权值为am。(分类误差率越小的基本分类器在最终分类器中的作用越大)。</li></ol><ul><li>分类错误的样本的权重会增大而分类正确的样本的权重则会减小。这样在训练h{m+1}时会侧重对错误样本的训练，以达到模型性能的提升，但是AdaBoost模型每个基分类器的损失函数优化目标是相同的且独立的，都是最优化当前样本（样本权重）的指数损失。</li></ul><h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><ul><li>GBDT (Gradient Boosting Decision Tree) 梯度提升迭代决策树。GBDT 也是 Boosting 算法的一种，但是和 AdaBoost 算法不同（AdaBoost 算法上一篇文章已经介绍）；</li><li>区别如下：AdaBoost 算法是利用前一轮的弱学习器的误差来更新样本权重值，然后一轮一轮的迭代；GBDT 也是迭代，但是 GBDT 要求弱学习器必须是 CART 模型，而且 GBDT 在模型训练的时候，是要求模型预测的样本损失尽可能的小。</li></ul><ol><li>采用确定回归树T1(x)。通过确定最优划分点，最小化平方误差损失函数确定最优划分点。</li><li>构建残差表，求解T2(x)来拟合残差。</li><li>依次迭代。所有分类器之和极为强分类器。即为所求提升数模型。</li></ol><h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><ol><li>XGBoost提升模型也是采用残差，分裂节点选取的损失函数如下：</li></ol><p><img src="\img\article\123awed.jpg" alt="123awed"></p><ol><li><p>损失函数有：平方损失函数，逻辑回归损失函数。</p></li><li><p>XGBoost是一个加法模型，我们t次迭代拟合的数值就是上一次的残差结果。每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差。</p><p><img src="\img\article\v2-a9b82954ae62e9e6da256c69ba22d38b_r.jpg" alt="v2-a9b82954ae62e9e6da256c69ba22d38b_r"></p><p><img src="\img\article\v2-f0cd240fcc70e7615dae7c2a29856bfc_r.jpg" alt="v2-f0cd240fcc70e7615dae7c2a29856bfc_r"></p></li><li><p>将全部k棵树的复杂度进行求和，添加到目标函数中作为正则化项，用于防止模型过度拟合。 </p><p><img src="\img\article\v2-142ca609c9ff3dc2df877a00c30756ca_hd.jpg" alt="v2-142ca609c9ff3dc2df877a00c30756ca_hd"></p><p><img src="\img\article\v2-c7ab2fcfd3196dbc0bce05d17b11d220_hd.jpg" alt="v2-c7ab2fcfd3196dbc0bce05d17b11d220_hd"></p><p><img src="\img\article\v2-d0cf0063c23679e711146f861d36fc17_r.jpg" alt="v2-d0cf0063c23679e711146f861d36fc17_r"></p></li><li><p>一种办法是贪心算法，遍历一个节点内的所有特征，按照公式计算出按照每一个特征分割的信息增益，找到信息增益最大的点进行树的分割。增加的新叶子惩罚项对应了树的剪枝，当gain小于某个阈值的时候，我们可以剪掉这个分割。但是这种办法不适用于数据量大的时候，因此，我们需要运用近似算法。 </p></li><li><p>XGBoost在寻找splitpoint的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，按照<strong>特征值的密度分布</strong>，构造直方图计算特征值分布的面积，然后划分分布形成若干个bucket(桶)，每个bucket的面积相同，将<strong>bucket边界上的特征值</strong>作为splitpoint的候选，<strong>遍历所有的候选分裂点</strong>来找到最佳分裂点。</p></li></ol><ul><li>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 </li><li>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了<strong>二阶泰勒展开，同时用到了一阶和二阶导数</strong>。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 </li><li>xgboost在<strong>代价函数里加入了正则项，用于控制模型的复杂度</strong>。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 </li><li><strong>Shrinkage（缩减），相当于学习速率（xgboost中的eta）</strong>。每次迭代，增加新的模型，在前面成上一个小于1的系数，降低优化的速度，每次走一小步逐步逼近最优模型比每次走一大步逼近更加容易避免过拟合现象；</li><li>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样（即每次的输入特征不是全部特征），不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</li><li>忽略缺失值：在寻找splitpoint的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个工程技巧来减少了为稀疏离散特征寻找splitpoint的时间开销</li><li>指定缺失值的分隔方向：可以为缺失值或者指定的值指定分支的默认方向，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，分到那个子节点带来的增益大，默认的方向就是哪个子节点，这能大大提升算法的效率。</li><li>并行化处理：在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点</li></ul><h3 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h3>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo-Theme-Snail</title>
      <link href="2019/11/01/Hexo-Theme-Snail/"/>
      <url>2019/11/01/Hexo-Theme-Snail/</url>
      
        <content type="html"><![CDATA[<h1 id="hexo-theme-snail"><a href="#hexo-theme-snail" class="headerlink" title="hexo-theme-snail"></a>hexo-theme-snail</h1><p><a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">View Hexo-Theme-Snail Sources On Github &#10174; </a></p><p><a href="https://www.dusign.net" target="_blank" rel="noopener">View Live Super Snail Blog &#10174;</a></p><p><img src="snail.png" alt="hexo-theme-snail"></p><p>Hexo-theme-snail is a succinct hexo theme. It has two colors, light and star, that can be set according to your own preferences in the settings, and also has the functions of sharing and commenting. More features are under development.</p><h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><ul><li>light color theme and star theme</li><li>diversified comment system</li><li>notice tips</li><li>share to other platforms (under development)</li><li>picture sharing (under development)</li></ul><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Install-Hexo"><a href="#Install-Hexo" class="headerlink" title="Install Hexo"></a>Install Hexo</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-cli -g</span><br></pre></td></tr></table></figure><h3 id="Setup-your-blog"><a href="#Setup-your-blog" class="headerlink" title="Setup your blog"></a>Setup your blog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br></pre></td></tr></table></figure><h3 id="Installation-Theme"><a href="#Installation-Theme" class="headerlink" title="Installation Theme"></a>Installation Theme</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> blog</span><br><span class="line">$ rm -rf <span class="built_in">source</span></span><br><span class="line">$ rm _config.yml package.json README.md LICENSE</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/dusign/hexo-theme-snail.git</span><br><span class="line">$ mv ./hexo-theme-snail/snail ./themes</span><br><span class="line">$ mv ./hexo-theme-snail/* ./</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure><h3 id="Set-Theme"><a href="#Set-Theme" class="headerlink" title="Set Theme"></a>Set Theme</h3><p>Modify the value of <code>theme:</code> in <code>_config.yml</code><br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">snail</span></span><br></pre></td></tr></table></figure></p><h3 id="Start-the-Server"><a href="#Start-the-Server" class="headerlink" title="Start the Server"></a>Start the Server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><h3 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h3><p>Replace the following information with your own.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Site</span></span><br><span class="line"><span class="attr">title:</span> </span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">At</span> <span class="string">the</span> <span class="string">bottom</span> <span class="string">of</span> <span class="string">the</span> <span class="string">well,</span> <span class="string">it</span> <span class="string">is</span> <span class="string">destined</span> <span class="string">to</span> <span class="string">see</span> <span class="string">only</span> <span class="string">the</span> <span class="string">sky</span> <span class="string">at</span> <span class="string">the</span> <span class="string">wellhead.</span> </span><br><span class="line">          <span class="string">However,</span> <span class="string">the</span> <span class="string">starting</span> <span class="string">point</span> <span class="string">only</span> <span class="string">affects</span> <span class="string">the</span> <span class="string">process</span> <span class="string">of</span> <span class="string">reaching</span> <span class="string">your</span> <span class="string">peak</span> <span class="string">and</span> <span class="string">does</span> <span class="string">not</span> <span class="string">determine</span> <span class="string">the</span> <span class="string">height</span> <span class="string">you</span> <span class="string">reach.</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">Dusign</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">en</span></span><br><span class="line"><span class="attr">timezone:</span></span><br></pre></td></tr></table></figure></p><h3 id="Site-Settings"><a href="#Site-Settings" class="headerlink" title="Site Settings"></a>Site Settings</h3><p>Put customized pictures in <code>img</code> directory.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Site settings</span></span><br><span class="line"><span class="attr">SEOTitle:</span> <span class="string">Hexo-theme-snail</span></span><br><span class="line"><span class="attr">email:</span> <span class="string">hexo-theme-snail@mail.com</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">"A hexo theme"</span></span><br><span class="line"><span class="attr">keyword:</span> <span class="string">"dusign, hexo-theme-snail"</span></span><br><span class="line"><span class="attr">header-img:</span> <span class="string">img/header_img/home-bg-1-dark.jpg</span></span><br><span class="line"><span class="attr">signature:</span> <span class="literal">true</span> <span class="comment">#show signature</span></span><br><span class="line"><span class="attr">signature-img:</span> <span class="string">img/signature/Just-do-it-white.png</span></span><br></pre></td></tr></table></figure></p><h3 id="SNS-Settings"><a href="#SNS-Settings" class="headerlink" title="SNS Settings"></a>SNS Settings</h3><p>If you don’t want to display it, you can delete it directly.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SNS settings</span></span><br><span class="line"><span class="attr">github_username:</span>    <span class="string">dusign</span></span><br><span class="line"><span class="attr">twitter_username:</span>   <span class="string">dusignr</span></span><br><span class="line"><span class="attr">facebook_username:</span>  <span class="string">Gang</span> <span class="string">Du</span></span><br><span class="line"><span class="attr">zhihu_username:</span> <span class="string">dusignr</span></span><br></pre></td></tr></table></figure></p><h3 id="Sidebar-Settings"><a href="#Sidebar-Settings" class="headerlink" title="Sidebar Settings"></a>Sidebar Settings</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sidebar Settings</span></span><br><span class="line"><span class="attr">sidebar:</span> <span class="literal">true</span>                      <span class="comment"># whether or not using Sidebar.</span></span><br><span class="line"><span class="attr">sidebar-about-description:</span> <span class="string">"Welcome to visit, I'm Dusign!"</span></span><br><span class="line"><span class="attr">sidebar-avatar:</span> <span class="string">img/ironman-draw.png</span>      <span class="comment"># use absolute URL, seeing it's used in both `/` and `/about/`</span></span><br><span class="line"><span class="attr">widgets:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">featured-tags</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">short-about</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">recent-posts</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">friends-blog</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">archive</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">category</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># widget behavior</span></span><br><span class="line"><span class="comment">## Archive</span></span><br><span class="line"><span class="attr">archive_type:</span> <span class="string">'monthly'</span></span><br><span class="line"><span class="attr">show_count:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## Featured Tags</span></span><br><span class="line"><span class="attr">featured-tags:</span> <span class="literal">true</span>                     <span class="comment"># whether or not using Feature-Tags</span></span><br><span class="line"><span class="attr">featured-condition-size:</span> <span class="number">1</span>              <span class="comment"># A tag will be featured if the size of it is more than this condition value</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## Friends</span></span><br><span class="line"><span class="attr">friends:</span> <span class="string">[</span></span><br><span class="line">    <span class="string">&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Dusign's Blog"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"https://blog.csdn.net/d_Nail"</span></span><br><span class="line">    <span class="string">&#125;,&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Dusign's Web"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"#"</span></span><br><span class="line">    <span class="string">&#125;,&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Dusign's Github"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"https://github.com/dusign"</span></span><br><span class="line">    <span class="string">&#125;,&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Other"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"#"</span></span><br><span class="line">    <span class="string">&#125;</span></span><br><span class="line"><span class="string">]</span></span><br></pre></td></tr></table></figure><h3 id="Theme"><a href="#Theme" class="headerlink" title="Theme"></a>Theme</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">snail</span></span><br></pre></td></tr></table></figure><h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span></span><br><span class="line">      <span class="attr">github:</span> <span class="string">github.repository.address</span></span><br><span class="line">      <span class="attr">coding:</span> <span class="string">coding.repository.address</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><h3 id="Comment"><a href="#Comment" class="headerlink" title="Comment"></a>Comment</h3><p>See httpymls://github.com/imsun/gitment for detailed configuration method.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Comment</span></span><br><span class="line"><span class="comment">## This comment system is gitment</span></span><br><span class="line"><span class="comment">## gitment url: https://github.com/imsun/gitment</span></span><br><span class="line"><span class="attr">comment:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">owner:</span></span><br><span class="line">  <span class="attr">repo:</span></span><br><span class="line">  <span class="attr">client_id:</span></span><br><span class="line">  <span class="attr">client_secret:</span></span><br></pre></td></tr></table></figure></p><h3 id="Tip"><a href="#Tip" class="headerlink" title="Tip"></a>Tip</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tip</span></span><br><span class="line"><span class="attr">tip:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">content:</span> <span class="string">欢迎访问</span> <span class="string">&lt;a</span> <span class="string">href="https://www.dusign.net"</span> <span class="string">target="dusign"&gt;dusign&lt;/a&gt;</span> <span class="string">的博客，博客系统一键分享的功能还在完善中，请大家耐心等待。</span></span><br><span class="line">          <span class="string">若有问题或者有好的建议欢迎留言，笔者看到之后会及时回复。</span></span><br><span class="line">          <span class="string">评论点赞需要github账号登录，如果没有账号的话请点击</span> </span><br><span class="line">          <span class="string">&lt;a</span> <span class="string">href="https://github.com"</span> <span class="string">target="view_window"</span> <span class="string">&gt;</span> <span class="string">github</span> <span class="string">&lt;/a&gt;</span> <span class="string">注册，</span> <span class="string">谢谢</span> <span class="string">!</span></span><br></pre></td></tr></table></figure><h3 id="Color-Sheme"><a href="#Color-Sheme" class="headerlink" title="Color Sheme"></a>Color Sheme</h3><p>Set the <code>enable</code> value of the desired color sheme to <code>true</code>. If the value of <code>bg_effects.star.enable</code> is <code>true</code>, please modify the value of <code>highlight_theme</code> in <code>./themes/snail/_config.yml</code> to <code>night</code>.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Color Sheme</span></span><br><span class="line"><span class="comment">## If there is no effect after modification, please empty the cache and try again.</span></span><br><span class="line"><span class="comment">## ⚠️ The following special effects will take up a lot of cpu resorces, please open it carefully.</span></span><br><span class="line"><span class="attr">bg_effects:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">line:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">color:</span> <span class="number">129</span><span class="string">,200,61</span></span><br><span class="line">    <span class="attr">pointColor:</span> <span class="number">129</span><span class="string">,200,61</span></span><br><span class="line">    <span class="attr">opacity:</span> <span class="number">0.7</span></span><br><span class="line">    <span class="attr">zIndex:</span> <span class="number">-9</span></span><br><span class="line">    <span class="attr">count:</span> <span class="number">99</span></span><br><span class="line">  <span class="attr">mouse_click:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">content:</span> <span class="string">'"🌱","just do it","🌾","🍀","don'</span><span class="string">'t give up","🍂","🌻","try it again","🍃","never say die","🌵","🌿","🌴"'</span></span><br><span class="line">    <span class="attr">color:</span> <span class="string">'"rgb(121,93,179)"</span></span><br><span class="line"><span class="string">          ,"rgb(76,180,231)"</span></span><br><span class="line"><span class="string">          ,"rgb(184,90,154)"</span></span><br><span class="line"><span class="string">          ,"rgb(157,211,250)"</span></span><br><span class="line"><span class="string">          ,"rgb(255,0,0)"</span></span><br><span class="line"><span class="string">          ,"rgb(242,153,29)"</span></span><br><span class="line"><span class="string">          ,"rgb(23,204,16)"</span></span><br><span class="line"><span class="string">          ,"rgb(222,0,0)"</span></span><br><span class="line"><span class="string">          ,"rgb(22,36,92)"</span></span><br><span class="line"><span class="string">          ,"rgb(127,24,116)"</span></span><br><span class="line"><span class="string">          ,"rgb(119,195,79)"</span></span><br><span class="line"><span class="string">          ,"rgb(4,77,34)"</span></span><br><span class="line"><span class="string">          ,"rgb(122,2,60)"'</span></span><br><span class="line">  <span class="attr">star:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><h2 id="Releases"><a href="#Releases" class="headerlink" title="Releases"></a>Releases</h2><p>V1.0</p><ul><li>fix the bugs</li><li>add comment system</li><li>add notice tips</li><li>add star sheme</li></ul><h2 id="License"><a href="#License" class="headerlink" title="License"></a>License</h2><p>Apache License 2.0 Copyright(c) 2018-2020 <a href="https://github.com/dusign" target="_blank" rel="noopener">Dusign</a>   </p><p><a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">hexo-theme-snail</a> is derived from <a href="https://github.com/Huxpro/huxpro.github.io" target="_blank" rel="noopener">Huxpro</a> Apache License 2.0. Copyright (c) 2015-2020 Huxpro</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo-theme-snail </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep-FSMN</title>
      <link href="2019/07/10/d-fsmn-model/"/>
      <url>2019/07/10/d-fsmn-model/</url>
      
        <content type="html"><![CDATA[<h1 id="D-FSMN"><a href="#D-FSMN" class="headerlink" title="D-FSMN"></a>D-FSMN</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/img/article/d-fsmn_1.png" alt=""></p><h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><ol><li>D-FSMN是建立在C-FSMN的基础上，进行深层次的构建网络的。</li><li>我们通过在cFSMN的记忆模块（红色框框表示）之间添加跳转连接（skip connection），从而使得低层记忆模块的输出会被直接累加到高层记忆模块里。这样在训练过程中，高层记忆模块的梯度会直接赋值给低层的记忆模块，从而可以克服由于网络的深度造成的梯度消失问题，使得可以稳定的训练深层的网络。</li></ol><h2 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h2><p><img src="/img/article/d-fsmn_2.png" alt=""></p><ol><li>其中表示第层记忆模块第t个时刻的输出。和分别表示历史和未来时刻的编码步幅因子，例如 则表示对历史信息进行编码时每隔一个时刻取一个值作为输入。这样在相同的阶数的情况下可以看到更远的历史，从而可以更加有效的对长时相关性进行建模。对于实时的语音识别系统我们可以通过灵活的设置未来阶数来控制模型的时延.</li><li>我们提出的DFSMN优势在于，通过跳转连接可以训练很深的网络。对于原来的cFSMN，由于每个隐层已经通过矩阵的低秩分解拆分成了两层的结构，这样对于一个包含4层cFSMN层以及两个DNN层的网络，总共包含的层数将达到13层，从而采用更多的cFSMN层，会使得层数更多而使得训练出现梯度消失问题，导致训练的不稳定性。</li><li>在极端情况下，当我们将每个记忆模块的未来阶数都设置为0，则我们可以实现无时延的一个声学模型。对于一些任务，我们可以忍受一定的时延，我们可以设置小一些的未来阶数。</li><li>最近有研究提出一种低帧率（LowFrame Rate，LFR）建模方案：通过将相邻时刻的语音帧进行绑定作为输入，去预测这些语音帧的目标输出得到的一个平均输出目标。可以结合D-FSMN使用，使其达到更好的效率。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C-FSMN(简洁的FSMN)</title>
      <link href="2019/07/10/c-fsmn-model/"/>
      <url>2019/07/10/c-fsmn-model/</url>
      
        <content type="html"><![CDATA[<h1 id="C-FSMN"><a href="#C-FSMN" class="headerlink" title="C-FSMN"></a>C-FSMN</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/img/article/c-fsmn_1.png" alt=""></p><h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><ol><li>FSMN相比于FNN，需要将记忆模块的输出作为下一个隐层的额外输入，这样就会引入额外的模型参数。隐层包含的节点越多，则引入的参数越多。</li><li>我们通过结合矩阵低秩分解（Low-rank matrix factorization）的思路，提出了一种改进的FSMN结构，称之为简洁的FSMN（Compact FSMN，cFSMN）</li><li>对于C-FSMN，通过在网络的隐层后添加一个低维度的线性投影层，并且将记忆模块添加在这些线性投影层上。即h(l, t)到p(l, t)经过一次线性变换。</li><li>进一步的，cFSMN对记忆模块的编码公式进行了一些改变，通过将当前时刻的输出显式的添加到记忆模块的表达中，从而只需要将记忆模块的表达作为下一层的输入。这样可以有效的减少模型的参数量，加快网络的训练。</li><li>下一层的结果h(l+1,t) = U’*P’(t)</li></ol><p>具体的，单向和双向的cFSMN记忆模块的公式表达分别如下：<br><img src="/img/article/c-fsmn_2.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FSMN前馈型序列记忆网络</title>
      <link href="2019/07/10/fsmn-model/"/>
      <url>2019/07/10/fsmn-model/</url>
      
        <content type="html"><![CDATA[<h1 id="FSMN-前馈型序列记忆网络"><a href="#FSMN-前馈型序列记忆网络" class="headerlink" title="FSMN(前馈型序列记忆网络)"></a>FSMN(前馈型序列记忆网络)</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/img/article/fsmn_1.png" alt=""><br><img src="/img/article/fsmn_2.png" alt=""></p><h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><ol><li>在隐藏层的旁边，FSMN挂了一个记忆模块Memory Block，记忆模块的作用与LSTM门结构类似，可以用来记住t时刻输入信息的相邻时刻序列的信息。</li><li>根据记忆模块编码方式的区别，FSMN又可以分为sFSMN和vFSMN，前者代表以标量系数编码，后者代表以向量系数编码。</li></ol><h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><ul><li>如上图，以记住前N个时刻信息为例子，计算公式如下<script type="math/tex; mode=display">\overrightarrow{\tilde{h}}_{t}^{l}=\sum_{i=0}^{N} a_{i}^{l} \cdot \overrightarrow{h_{t-i}^{l}}, \text {in... sFSMN}</script><script type="math/tex; mode=display">\overrightarrow{\tilde{h}}_{t}^{l}=\sum_{i=0}^{N} \overrightarrow{a_{i}^{l}} \odot \overrightarrow{h_{t-i}^{l}}, \text {in...} v F S M N</script></li><li>一式表示标量乘积，二式表示Hadamard积</li><li>有了这个隐藏层旁挂着的记忆模块，就要将此记忆模块作为输入传递到下一个隐藏层<script type="math/tex; mode=display">h_{t}^{\overrightarrow{l+1}}=f\left(W^{l} \overrightarrow{h_{t}^{l}}+\tilde{W}^{l} \overrightarrow{\tilde{h}_{t}^{l}}+\overrightarrow{b^{l}}\right)</script></li><li>以上就是简单的回看式FSMN，也就是说当下的记忆模块只关注了它之前的信息，如果还要关注未来的信息，实现上下文联通，也就是所谓的双向的FSMN，直接在上式中添加后看的阶数即可，如下：<br><img src="/img/article/fsmn_3.png" alt=""><br>其中N1和N2分别代表前看和后看的阶数。</li></ul><h2 id="简而言之"><a href="#简而言之" class="headerlink" title="简而言之"></a>简而言之</h2><p>给定一个包含T个单词的序列X，我们可以构造一个T阶的方阵M：<br><img src="/img/article/fsmn_4.png" alt=""><br><img src="/img/article/fsmn_5.png" alt=""><br>鉴于上式，我们就有了很美的以下这个公式：</p><script type="math/tex; mode=display">\tilde{H}=H M</script><p>更为推广的，对于给定的K个序列：</p><script type="math/tex; mode=display">L=\left\{X_{1}, X_{2}, \ldots, X_{K}\right\}</script><p>一个更美的公式诞生了：</p><script type="math/tex; mode=display">\tilde{H}=\left[H_{1}, H_{2}, \ldots, H_{K}\right]\left[\begin{array}{cccc}{M_{1}} & {} & {} & {} \\ {} & {M_{2}} & {} & {} \\ {} & {} & {\ddots} & {} \\ {} & {} & {} & {M_{K}}\end{array}\right]=\overline{H} \overline{M}</script>]]></content>
      
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CTC原理解析</title>
      <link href="2019/07/09/speech-ctc-md/"/>
      <url>2019/07/09/speech-ctc-md/</url>
      
        <content type="html"><![CDATA[<h1 id="CTC简介-Connectionist-Temporal-Classfication"><a href="#CTC简介-Connectionist-Temporal-Classfication" class="headerlink" title="CTC简介(Connectionist Temporal Classfication)"></a>CTC简介(Connectionist Temporal Classfication)</h1><p>连接时序分类，适合不知道输入输出是否对其的情况使用的算法，所以CTC适合语音识别和手写字符识别的任务。</p><h2 id="CTC原理"><a href="#CTC原理" class="headerlink" title="CTC原理"></a>CTC原理</h2><p>我们定义如下，输入用符号序列X=[x1,x2,…xT], Y=[y1,y2,…yU],为了方便训练这些数据我闷希望能够找到输入X和输出Y之间的映射关系。</p><h2 id="损失函数定义"><a href="#损失函数定义" class="headerlink" title="损失函数定义"></a>损失函数定义</h2><p>对于给定的输入X，我们训练模型希望最大化Y的后验概率P(Y|X)，P(Y|X)应该是可导的，这样就能利用梯度下降训练模型了。</p><h2 id="对齐存在的问题"><a href="#对齐存在的问题" class="headerlink" title="对齐存在的问题"></a>对齐存在的问题</h2><p><img src="/img/article/ctc_duiqi.png" alt=""></p><ul><li>通常这种对齐方式是不合理的，比如在语音识别任务中，有些音频可能是无声的，这时候应该是没有字符输出的</li><li>对于一些本应含有重复字符的输出，这种对齐方式没法得到准确的输出。例如输出对齐的结果为[h, h, e, l, l, l, o],通过去重操作后得到的不是“hello”而是“helo”</li></ul><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ul><li>为了解决上述问题，CTC算法引入一个新的占位符用于输出对齐结果。这个占位符称为空白占位符，通常使用符号<script type="math/tex">\epsilon</script>,这个符号在对齐结果中输出，但是在最后的去重操作将会所有的<script type="math/tex">\epsilon</script>删除得到最终的输出。利用这个占位符，可以将输入与输出有了非常合理的对应关系。<br><img src="/img/article/ctc_duiqi2.png" alt=""></li><li>在这个映射方式中，如果在标定文本中有重复的字符，对齐过程中会在两个重复的字符当中插入<script type="math/tex">\epsilon</script>占位符。利用这个规则，上面的“hello”就不会变成“helo”了。<br><img src="/img/article/ctc_duiqi3.png" alt=""></li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ol><li>CTC对齐输入输出是多对一的。多条路径最后的输出都是hello，要讲所有的路径相加才是输出的条件概率。</li><li>对于一对输入输出(X,Y)来说，CTC的目标是将下式概率最大化。<script type="math/tex; mode=display">p(Y | X)=\sum_{A \in \mathcal{A}_{X, Y}} \prod_{t=1}^{T} p_{t}\left(a_{t} | X\right)</script>乘法表示一条路径的所有字符概率相乘，加法表示多条路径。</li><li>对于一个输出，路径会非常多，这样直接计算概率是不现实的，CTC算法采用动态规划的思想来求解输出的条件概率。<br><img src="/img/article/ctc_method.png" alt=""></li></ol><h1 id="动态规划求解最大后验概率方法"><a href="#动态规划求解最大后验概率方法" class="headerlink" title="动态规划求解最大后验概率方法"></a>动态规划求解最大后验概率方法</h1><p><img src="/img/article/ctc_dynamic_p.png" alt=""></p><ul><li><script type="math/tex">\alpha</script>表示对齐结果合并后节点的概率，</li><li>如图所示可以采用动态规划求解该类问题。</li></ul><h2 id="Case1"><a href="#Case1" class="headerlink" title="Case1:"></a>Case1:</h2><ol><li>如果<script type="math/tex">\alpha_{s, t}=\epsilon</script>，则<script type="math/tex">\alpha_{s, t}</script>只能由前一个字符<script type="math/tex">\alpha_{s-1, t-1}</script>或者本身<script type="math/tex">\alpha_{s, t-1}</script>得到</li><li>如果<script type="math/tex">\alpha_{s, t}</script>不等于<script type="math/tex">\epsilon</script>,且<script type="math/tex">\alpha_{s, t}</script>为连续字符的第二个(上面第二个o)，则<script type="math/tex">\alpha_{s, t}</script>只能由一个空白符<script type="math/tex">\alpha_{s-1, t-1}</script>或者其本身<script type="math/tex">\alpha_{s, t-1}</script>得到，而不能由前一个字符得到。</li></ol><ul><li>上述两种情况中，<script type="math/tex">\alpha_{s, t}</script>可由下式算出，其中<script type="math/tex">p_{t}\left(z_{s} | X\right)</script>表示在时刻t输出字符<script type="math/tex">z_{s}</script>的概率。<script type="math/tex; mode=display">\alpha_{s, t}=(\alpha(s, t-1)+\alpha(s-1, t-1)) \cdot p_{t}\left(z_{s} | X\right)</script></li></ul><h2 id="Case2"><a href="#Case2" class="headerlink" title="Case2:"></a>Case2:</h2><ol><li>如果<script type="math/tex">\alpha_{s, t}</script>不等于<script type="math/tex">\epsilon</script>，则可由<script type="math/tex">\alpha_{s-2, t-1}, \alpha_{s-1, t-1}, \alpha_{s, t-1}</script>得来。<script type="math/tex; mode=display">\alpha_{s, t}=(\alpha(s, t-1)+\alpha(s-1, t-1)+\alpha(s-2, t-1)) \cdot p_{t}\left(z_{s} | X\right)</script></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上图中输出两个终止点，最后输出的条件概率为两个终止点输出的概率之和，使用这种方法就能高效的计算损失函数。<br>模型的目标就是最小化负对数似然函数</p><script type="math/tex; mode=display">\sum_{(X, Y) \in \mathcal{D}}-\log p(Y | X)</script><h1 id="CTC解码函数"><a href="#CTC解码函数" class="headerlink" title="CTC解码函数"></a>CTC解码函数</h1><h2 id="ctc-greedy-decoder"><a href="#ctc-greedy-decoder" class="headerlink" title="ctc_greedy_decoder"></a>ctc_greedy_decoder</h2><ul><li>采用贪婪算法进行解码</li></ul><ol><li>即直接算出每个时刻取概率最大的字符作为输出。</li><li>通常这种算法很有效，但是忽略了一个输出可能对应对个对齐结果。例如[a,a,null]和[a,a,a]各自概率均小于[b,b,b]，该算法得到的结果为Y=[b],但是结果为Y=[a]更为合理。</li></ol><h2 id="ctc-beam-search-decoder"><a href="#ctc-beam-search-decoder" class="headerlink" title="ctc_beam_search_decoder"></a>ctc_beam_search_decoder</h2><ul><li>采用集束搜索算法</li></ul><ol><li>该算法有个参数为width，假设宽度为3，在RNN输出中，该算法每个时间t输出时。不同于贪婪算法只找最高的，而是找最高的三个概率作为下一次的输入，依次迭代。(当width=1时，则就变成贪婪算法)。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>get_mfcc_features</title>
      <link href="2019/07/03/get-mfcc-features/"/>
      <url>2019/07/03/get-mfcc-features/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>常见的深度学习优化方法</title>
      <link href="2019/06/06/deep-learning-optimizer/"/>
      <url>2019/06/06/deep-learning-optimizer/</url>
      
        <content type="html"><![CDATA[<h1 id="一阶的梯度法简介"><a href="#一阶的梯度法简介" class="headerlink" title="一阶的梯度法简介"></a>一阶的梯度法简介</h1><ol><li>一阶的梯度法，包括BGD，SGD，Momentum，Nesterov，AdaGrad，RMSProp，Adam</li><li>其中SGD，Momentum，Nesterov需要手动设置学习速率</li><li>AdaGrad，RMSProp，Adam能够自动调节学习速率</li></ol><h1 id="仔细分析"><a href="#仔细分析" class="headerlink" title="仔细分析"></a>仔细分析</h1><h2 id="BGDbatch梯度下降"><a href="#BGDbatch梯度下降" class="headerlink" title="BGDbatch梯度下降"></a>BGDbatch梯度下降</h2><p>即batch gradient descent. 在训练中,每一步迭代都使用训练集的所有内容. 也就是说,利用现有参数对训练集中的每一个输入生成一个估计输出yi^,然后跟实际输出yi比较,统计所有误差,求平均以后得到平均误差,以此来作为更新参数的依据.<br>具体实现:<br>需要学习速率ϵ,初始参数θ<br>每步迭代过程</p><ol><li>提取训练集中的所有内容{x1, x2, …, xn}，以及相关的输出yi</li><li>计算损失函数梯度并更新参数<script type="math/tex; mode=display">\begin{array}{l}{\hat{g} \leftarrow+\frac{1}{n} \nabla_{\theta} \sum_{i} L\left(f\left(x_{i} ; \theta\right), y_{i}\right)} \\{\theta \leftarrow \theta-\epsilon \hat{g}}\end{array}</script>优点:<br>由于每一步都利用了训练集中的所有数据,因此当损失函数达到最小值以后,能够保证此时计算出的梯度为0,换句话说,就是能够收敛.因此,使用BGD时不需要逐渐减小学习速率<script type="math/tex">\epsilon</script><br>缺点:<br>由于每一步都要使用所有数据,因此随着数据集的增大,运行速度会越来越慢.</li></ol><h2 id="SGD随机梯度下降"><a href="#SGD随机梯度下降" class="headerlink" title="SGD随机梯度下降"></a>SGD随机梯度下降</h2><p>SGD全名 stochastic gradient descent， 即随机梯度下降。不过这里的SGD其实跟MBGD(minibatch gradient descent)是一个意思,即随机抽取一批样本,以此为根据来更新参数.</p><p>具体实现:<br>需要:学习速率ϵ, 初始参数θ<br>每步迭代过程: </p><ol><li>从训练集中的随机抽取一批容量为m的样本{x1,…,xm}，以及相关的输出yi</li><li>计算梯度和误差并更新参数: <script type="math/tex; mode=display">\begin{array}{l}{\hat{g} \leftarrow+\frac{1}{m} \nabla_{\theta} \sum_{i} L\left(f\left(x_{i} ; \theta\right), y_{i}\right)} \\{\theta \leftarrow \theta-\epsilon \hat{g}}\end{array}</script>优点:<br>训练速度快,对于很大的数据集,也能够以较快的速度收敛.</li></ol><p>缺点:<br>由于是抽取,因此不可避免的,得到的梯度肯定有误差.因此学习速率需要逐渐减小.否则模型无法收敛<br>因为误差,所以每一次迭代的梯度受抽样的影响比较大,也就是说梯度含有比较大的噪声,不能很好的反映真实梯度.</p><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>上面的SGD有个问题,就是每次迭代计算的梯度含有比较大的噪音. 而Momentum方法可以比较好的缓解这个问题,尤其是在面对小而连续的梯度但是含有很多噪声的时候,可以很好的加速学习.Momentum借用了物理中的动量概念,即前几次的梯度也会参与运算.为了表示动量,引入了一个新的变量v(velocity).v是之前的梯度的累加,但是每回合都有一定的衰减.<br>具体实现:<br>需要:学习速率ϵ,初始参数θ,初始速率v,动量衰减参数α<br>每步迭代过程: </p><ol><li>从训练集中的随机抽取一批容量为m的样本{x1,…,xm},以及相关的输出yi</li><li>计算梯度和误差,并更新速度v和参数θ:<script type="math/tex; mode=display">\begin{array}{l}{\hat{g} \leftarrow+\frac{1}{m} \nabla_{\theta} \sum_{i} L\left(f\left(x_{i} ; \theta\right), y_{i}\right)} \\ {v \leftarrow \alpha v-\epsilon \hat{g}} \\ {\theta \leftarrow \theta+v}\end{array}</script></li></ol><p>其中参数α表示每回合速率v的衰减程度.同时也可以推断得到,如果每次迭代得到的梯度都是g,那么最后得到的v的稳定值为 <script type="math/tex">\frac{\epsilon\|g\|}{1-\alpha}</script><br>也就是说,Momentum最好情况下能够将学习速率加速<script type="math/tex">\frac{1}{1-\alpha}</script>倍.一般α的取值有0.5,0.9,0.99这几种.当然,也可以让α的值随着时间而变化,一开始小点,后来再加大.不过这样一来,又会引进新的参数.<br>特点:<br>前后梯度方向一致时,能够加速学习<br>前后梯度方向不一致时,能够抑制震荡</p>]]></content>
      
      
      
        <tags>
            
            <tag> deep_learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow再次实现简单的CNN</title>
      <link href="2019/06/06/tensorflow-simple-cnn/"/>
      <url>2019/06/06/tensorflow-simple-cnn/</url>
      
        <content type="html"><![CDATA[<h1 id="简单的CNN"><a href="#简单的CNN" class="headerlink" title="简单的CNN"></a>简单的CNN</h1><ol><li>padding=’VALID’指卷积边界无填充，’SAME’是指卷积边界有填充，填充到原大小</li><li>设置keep_prob=0.5的进行训练，即减少部分网络神经防止过拟合，然后测试准确率使用全部网络结构</li><li>reduction_indices指reduce_mean的维度方向</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'../Chapter4/MNIST_data/'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_varibale</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initail = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initail)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, w, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"><span class="comment"># 四维向量输入</span></span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 第一个卷积层：卷积核大小，输入通道数为1，输出通道数为32，输出宽度为[28, 28]</span></span><br><span class="line">w_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_varibale([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)</span><br><span class="line"><span class="comment"># 池化层输出宽度为[14, 14]</span></span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二个卷句层：卷积核大小，输入通道为32，输出通道为64,输出宽度为[14, 14]</span></span><br><span class="line">w_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_varibale([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)</span><br><span class="line"><span class="comment"># 池化层输出宽度为[7, 7]</span></span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全链接层1,全链接输出1024个结点</span></span><br><span class="line">w_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_varibale([<span class="number">1024</span>])</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fcl_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"><span class="comment"># 全链接层2,输出10个结点</span></span><br><span class="line">w_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_varibale([<span class="number">10</span>])</span><br><span class="line">y_conv = tf.nn.softmax(tf.matmul(h_fcl_drop, w_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算交叉熵</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2000</span>):</span><br><span class="line">        batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            train_accuracy = accuracy.eval(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">            print(<span class="string">"step: %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;x: mnist.train.images, y_: mnist.train.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow实现自编码器</title>
      <link href="2019/06/06/tensorflow-autoencoder/"/>
      <url>2019/06/06/tensorflow-autoencoder/</url>
      
        <content type="html"><![CDATA[<h1 id="自编码器"><a href="#自编码器" class="headerlink" title="自编码器"></a>自编码器</h1><ol><li>最具代表性质的去燥编码器</li><li>xavier初始化使根据某一层网络的输入，输出结点数量自动调整最合适的分布，使均值为0，方差为2/(n_in+n_out)</li><li>standard_scale是对数据进行归一化处理，即均值为0，方差为1</li><li>softplus是ReLU激励函数的平滑版，函数为<script type="math/tex">\zeta(x)=\log \left(1+e^{x}\right)</script> </li></ol><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> prep</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理，使权重初始化不大不小，即均方为0，方差为2/(n_in+n_out)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xavier_init</span><span class="params">(fan_in, fan_out, constant=<span class="number">1</span>)</span>:</span></span><br><span class="line">    low = -constant * np.sqrt(<span class="number">6.0</span>/(fan_in + fan_out))</span><br><span class="line">    high = constant * np.sqrt(<span class="number">6.0</span>/(fan_in + fan_out))</span><br><span class="line">    <span class="keyword">return</span> tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现一个自编码器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdditiveGaussianNoiseAutoencoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_input, n_hidden, transfer_function=tf.nn.softplus,</span></span></span><br><span class="line"><span class="function"><span class="params">                 optimizer=tf.train.AdamOptimizer<span class="params">()</span>, scale=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        self.n_input = n_input</span><br><span class="line">        self.n_hidden = n_hidden</span><br><span class="line">        <span class="comment"># 隐藏层激活函数</span></span><br><span class="line">        self.transfer = transfer_function</span><br><span class="line">        self.scale = tf.placeholder(tf.float32)</span><br><span class="line">        self.training_scale = scale</span><br><span class="line">        network_weight = self._initialize_weights()</span><br><span class="line">        self.weights = network_weight</span><br><span class="line">        <span class="comment"># 输入x，结点数为n_input</span></span><br><span class="line">        self.x = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_input])</span><br><span class="line">        <span class="comment"># 隐藏层，存在一个噪音百分比scale</span></span><br><span class="line">        self.hidden = self.transfer(tf.add(tf.matmul(self.x + scale * tf.random_normal((n_input,)),</span><br><span class="line">                                                     self.weights[<span class="string">'w1'</span>]), self.weights[<span class="string">'b1'</span>]))</span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights[<span class="string">'w2'</span>]), self.weights[<span class="string">'b2'</span>])</span><br><span class="line">        <span class="comment"># 平方误差</span></span><br><span class="line">        self.cost = <span class="number">0.5</span> * tf.reduce_mean(tf.pow(tf.subtract(self.reconstruction, self.x), <span class="number">2.0</span>))</span><br><span class="line">        self.optimizer = optimizer.minimize(self.cost)</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line">        self.sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        all_weights = dict()</span><br><span class="line">        all_weights[<span class="string">'w1'</span>] = tf.Variable(xavier_init(self.n_input, self.n_hidden))</span><br><span class="line">        all_weights[<span class="string">'b1'</span>] = tf.Variable(tf.zeros([self.n_hidden], dtype=tf.float32))</span><br><span class="line">        all_weights[<span class="string">'w2'</span>] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype=tf.float32))</span><br><span class="line">        all_weights[<span class="string">'b2'</span>] = tf.Variable(tf.zeros([self.n_input], dtype=tf.float32))</span><br><span class="line">        <span class="keyword">return</span> all_weights</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用一个batch数据训练，并返回cost</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">partial_fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;)</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算总的误差cost</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_total_cost</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.cost, feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出隐藏层数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transfer</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.hidden, feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出输出层结果</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(self, hidden=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> hidden <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            hidden = np.random.normal(size=self.weights[<span class="string">'b1'</span>])</span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.reconstruction, feed_dict=&#123;self.hidden: hidden&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层结果</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.reconstruction, feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 权重</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getWeights</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.weights[<span class="string">'w1'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 偏移量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getBiases</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.weights[<span class="string">'b1'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化处理数据，均值为0方差为1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standard_scale</span><span class="params">(X_train, X_test)</span>:</span></span><br><span class="line">    preprocessor = prep.StandardScaler().fit(X_train)</span><br><span class="line">    X_train = preprocessor.transform(X_train)</span><br><span class="line">    X_test = preprocessor.transform(X_test)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生层一个batch数据块</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_block_from_data</span><span class="params">(data, batch_size)</span>:</span></span><br><span class="line">    start_index = np.random.randint(<span class="number">0</span>, len(data) - batch_size)</span><br><span class="line">    <span class="keyword">return</span> data[start_index: (start_index + batch_size)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'./MNIST_data'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)</span><br><span class="line">n_samples = int(mnist.train.num_examples)</span><br><span class="line">training_epochs = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">auto_encoder = AdditiveGaussianNoiseAutoencoder(n_input=<span class="number">784</span>,</span><br><span class="line">                                                n_hidden=<span class="number">200</span>,</span><br><span class="line">                                                transfer_function=tf.nn.softplus,</span><br><span class="line">                                                optimizer=tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">                                                scale=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">    avg_cost = <span class="number">0</span></span><br><span class="line">    total_batch = int(n_samples/batch_size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">        batch_xs = get_random_block_from_data(X_train, batch_size)</span><br><span class="line">        cost = auto_encoder.partial_fit(batch_xs)</span><br><span class="line">        avg_cost += cost/n_samples * batch_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">print(<span class="string">"Total cost: "</span> + str(auto_encoder.calc_total_cost(X_test)))</span><br></pre></td></tr></table></figure><h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p><img src="/img/article/auto_encoder.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow实现深度循环LSTM</title>
      <link href="2019/05/22/tensorflow-deep-lstm/"/>
      <url>2019/05/22/tensorflow-deep-lstm/</url>
      
        <content type="html"><![CDATA[<h1 id="LSTM网络结构"><a href="#LSTM网络结构" class="headerlink" title="LSTM网络结构"></a>LSTM网络结构</h1><h2 id="单个lstm结构"><a href="#单个lstm结构" class="headerlink" title="单个lstm结构"></a>单个lstm结构</h2><p><img src="/img/article/lstm_cell.png" alt=""></p><ol><li>Cell，就是我们的小本子，有个叫做state的参数东西来记事儿的</li><li>Input Gate，Output Gate，在参数输入输出的时候起点作用，算一算东西</li><li>Forget Gate：不是要记东西吗，咋还要Forget呢。这个没找到为啥就要加入这样一个东西，因为原始的LSTM在这个位置就是一个值1，是连接到下一时间的那个参数，估计是以前的事情记太牢了，最近的就不住就不好了，所以要选择性遗忘一些东西。（没找到解释设置这个东西的动机，还望指正）</li></ol><h2 id="BasicLSTMCell"><a href="#BasicLSTMCell" class="headerlink" title="BasicLSTMCell"></a>BasicLSTMCell</h2><p>将forget_bias（默认值：1）添加到忘记门的偏差(biases)中以便在训练开始时减少以往的比例(scale)。该神经元不允许单元裁剪(cell clipping),投影层，也不使用peep-hole连接，它是一个基本的LSTM神经元。想要更高级的模型可以使用：tf.nn.rnn_cell.LSTMCell。</p><h2 id="embedding-lookup"><a href="#embedding-lookup" class="headerlink" title="embedding_lookup"></a>embedding_lookup</h2><h2 id="整个过程原理"><a href="#整个过程原理" class="headerlink" title="整个过程原理"></a>整个过程原理</h2><ol><li>输入数据为[batch_size, num_steps],每个句子的单词个数为num_steps，由于句子长度就是时间长度，因此用num_steps代表句子长度。</li><li>在NLP问题中，我们用词向量表示一个单词，这里VOCAB_SIZE表示整个向量长度，语料库中单词的个数是vocab_size</li><li>LSTM结构中是一个神经网络，这个网络的隐藏单元个数我们设为hidden_size，那么这个LSTM单元里就有4*hidden_size个隐藏单元，也就是隐含变量的维度。</li><li>lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)创建一个简单的LSTM</li><li>cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * NUM_LAYERS) 创建多层LSTM</li><li></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.models.tutorials.rnn.ptb <span class="keyword">import</span> reader</span><br><span class="line"></span><br><span class="line">DATA_PATH = <span class="string">'./simple-examples/data'</span></span><br><span class="line"><span class="comment"># 隐藏层层数</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">200</span></span><br><span class="line"><span class="comment"># LSTM结构层数</span></span><br><span class="line">NUM_LAYERS = <span class="number">2</span></span><br><span class="line"><span class="comment"># 词典规模</span></span><br><span class="line">VOCAB_SIZE = <span class="number">10000</span></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">LEARNING_RATE = <span class="number">1.0</span></span><br><span class="line"><span class="comment"># batch大小</span></span><br><span class="line">TRAIN_BATCH_SIZE = <span class="number">20</span></span><br><span class="line"><span class="comment"># 训练数据截断长度</span></span><br><span class="line">TRAIN_NUM_STEP = <span class="number">35</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试时不需要截断，所以可以将测试数据看成一个超长序列</span></span><br><span class="line"><span class="comment"># 测试数据batch大小</span></span><br><span class="line">EVAL_BATCH_SIZE = <span class="number">1</span></span><br><span class="line"><span class="comment"># 测试数据截断长度</span></span><br><span class="line">EVAL_NUM_STEP = <span class="number">1</span></span><br><span class="line"><span class="comment"># 使用训练数据轮数</span></span><br><span class="line">NUM_EPOCH = <span class="number">2</span></span><br><span class="line"><span class="comment"># 结点不被dropout的概率</span></span><br><span class="line">KEEP_PROB = <span class="number">0.5</span></span><br><span class="line"><span class="comment"># 控制梯度膨胀的参数</span></span><br><span class="line">MAX_GRAD_NORM = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PTBModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, is_training, batch_size, num_steps)</span>:</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_steps = num_steps</span><br><span class="line">        <span class="comment"># 输入数据占位符,shape=[batch_size, 25]</span></span><br><span class="line">        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])</span><br><span class="line">        <span class="comment"># 目标输出占位符,shape=[batch_size, 25]</span></span><br><span class="line">        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])</span><br><span class="line">        <span class="comment"># 一个简单的LSTMcell，输入shape=[HIDDEN_SIZE, 1]</span></span><br><span class="line">        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)</span><br><span class="line">        <span class="comment"># 训练中设置dropout</span></span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=KEEP_PROB)</span><br><span class="line">        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * NUM_LAYERS)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化最初状态</span></span><br><span class="line">        self.initial_state = cell.zero_state(batch_size, tf.float32)</span><br><span class="line">        <span class="comment"># 将单词ID转换为单词向量,总共有VOCAB_SIZE个单词，每个单词的维度为HIDDEN_SIZE</span></span><br><span class="line">        embedding = tf.get_variable(<span class="string">"embedding"</span>, [VOCAB_SIZE, HIDDEN_SIZE])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将原本batch_size * num_steps个单词ID转化为单词向量</span></span><br><span class="line">        <span class="comment"># 转化后的输入层维度为batch_size * num_steps * HIDDEN_SIZE</span></span><br><span class="line">        inputs = tf.nn.embedding_lookup(embedding, self.input_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只有在训练时使用dropout</span></span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">            inputs = tf.nn.dropout(inputs, KEEP_PROB)</span><br><span class="line">        outputs = []</span><br><span class="line"></span><br><span class="line">        state = self.input_data</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"RNN"</span>):</span><br><span class="line">            <span class="keyword">for</span> time_step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">                <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">                    tf.get_variable_scope().reuse_variables()</span><br><span class="line">                (cell_output, state) = cell(inputs[:, time_step, :], state)</span><br><span class="line">                outputs.append(cell_output)</span><br><span class="line">        <span class="comment"># 把输出队列展开成[batch, hidden_size * num_steps]</span></span><br><span class="line">        <span class="comment"># 再reshape成[batch * numsteps, hidden_size]</span></span><br><span class="line">        output = tf.reshape(tf.concat(outputs, <span class="number">1</span>), [<span class="number">-1</span>, HIDDEN_SIZE])</span><br><span class="line"></span><br><span class="line">        weight = tf.get_variable(<span class="string">"weight"</span>, [HIDDEN_SIZE, VOCAB_SIZE])</span><br><span class="line">        bias = tf.get_variable(<span class="string">"bias"</span>, [VOCAB_SIZE])</span><br><span class="line">        logits = tf.matmul(output, weight) + bias</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line">        loss = tf.contrib.seq2seq.sequence_loss_by_example(</span><br><span class="line">            [logits],</span><br><span class="line">            [tf.reshape(self.targets, [<span class="number">-1</span>])],   <span class="comment"># 期待的答案，将[batch_size, num_steps]压缩为一维数组</span></span><br><span class="line">            [tf.ones([batch_size * num_steps], dtype=tf.float32)])</span><br><span class="line"></span><br><span class="line">        self.cost = tf.reduce_mean(loss)/batch_size</span><br><span class="line">        self.final_state = state</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        trainable_variables = tf.trainable_variables()</span><br><span class="line">        <span class="comment"># 控制梯度大小，避免梯度膨胀问题</span></span><br><span class="line">        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)</span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)</span><br><span class="line">        <span class="comment"># 定义训练步骤</span></span><br><span class="line">        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(sess, model, data, train_op, output_log)</span>:</span></span><br><span class="line">    <span class="comment"># 计算perplexity的辅助变量</span></span><br><span class="line">    total_costs = <span class="number">0.0</span></span><br><span class="line">    iters = <span class="number">0</span></span><br><span class="line">    state = sess.run(model.initial_state)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(model.num_steps):</span><br><span class="line">        x, y = reader.ptb_producer(data, model.batch_size, step)</span><br><span class="line">        cost, state, _ = sess.run(</span><br><span class="line">            [model.cost, model.final_state, train_op],</span><br><span class="line">            &#123;model.input_data: x, model.targets: y, model.initial_state: state&#125;)</span><br><span class="line"></span><br><span class="line">        total_costs += cost</span><br><span class="line">        iters += model.num_steps</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_log <span class="keyword">and</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"After %d steps, perplexity is %.3f"</span> % (step, np.exp(total_costs/iters)))</span><br><span class="line">    <span class="keyword">return</span> np.exp(total_costs/iters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">    train_data, valid_data, test_data, _ = reader.ptb_raw_data(DATA_PATH)</span><br><span class="line">    initializer = tf.random_uniform_initializer(<span class="number">-0.05</span>, <span class="number">0.05</span>)</span><br><span class="line">    <span class="comment"># 定义训练用的网络</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"language_model"</span>, reuse=<span class="literal">None</span>, initializer=initializer):</span><br><span class="line">        train_model = PTBModel(<span class="literal">True</span>, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义测试用的网络</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"language_model"</span>, reuse=<span class="literal">True</span>, initializer=initializer):</span><br><span class="line">        eval_model = PTBModel(<span class="literal">False</span>, EVAL_BATCH_SIZE, EVAL_NUM_STEP)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        tf.initialize_all_variables().run()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_EPOCH):</span><br><span class="line">            print(<span class="string">"In iteration: %d"</span> % (i+<span class="number">1</span>))</span><br><span class="line">            run_epoch(sess, train_model, train_data, train_model.train_op, <span class="literal">True</span>)</span><br><span class="line">            valid_perplexity = run_epoch(sess, eval_model, valid_data, tf.no_op(), <span class="literal">False</span>)</span><br><span class="line">            print(<span class="string">"Epoch : %d Validation Perplexity: %.3f"</span> % (i+<span class="number">1</span>, valid_perplexity))</span><br><span class="line"></span><br><span class="line">        test_perplexity = run_epoch(sess, eval_model, test_data, tf.no_op(), <span class="literal">False</span>)</span><br><span class="line">        print(<span class="string">"Testing Perplexity: %.3f"</span> % test_perplexity)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inception-v3之迁移学习</title>
      <link href="2019/05/20/transfer-learn/"/>
      <url>2019/05/20/transfer-learn/</url>
      
        <content type="html"><![CDATA[<h1 id="Inception-v3模型"><a href="#Inception-v3模型" class="headerlink" title="Inception-v3模型"></a>Inception-v3模型</h1><ol><li>将ImageNet上训练好的Inception-v3模型转移到另一个图像分类数据集上</li><li>所谓迁移学习就是将一个问题上训练好的模型通过简单的调整使其适用于一个新的问题。</li><li>本案例在Inception-v3保留所有卷积层参数，替换其最后一层全链接层</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> gfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inception-v3瓶颈结点个数</span></span><br><span class="line">BOTTLENECK_TENSOR_SIZE = <span class="number">2048</span></span><br><span class="line">BOTTLENECK_TENSOR_NAME = <span class="string">'pool_3/_reshape:0'</span></span><br><span class="line"></span><br><span class="line">JEPG_DATA_TENSOR_NAME = <span class="string">'DecodeJpeg/contents:0'</span></span><br><span class="line">MODEL_DIR = <span class="string">'./inception_dec_2015/'</span></span><br><span class="line">MODEL_FILE = <span class="string">'tensorflow_inception_graph.pb'</span></span><br><span class="line">CACHE_DIR = <span class="string">'./cache/bottleneck'</span></span><br><span class="line">INPUT_DATA = <span class="string">'./flower_photos'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证的数据百分比</span></span><br><span class="line">VALIDATION_PERCENTAGE = <span class="number">10</span></span><br><span class="line">TEST_PERCENTAGE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">0.01</span></span><br><span class="line">STEPS = <span class="number">4000</span></span><br><span class="line">BATCH = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从数据文件夹中读取所有图片列表，并按照训练，验证，测试数据分开</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_image_lists</span><span class="params">(testing_percentage, validation_percentage)</span>:</span></span><br><span class="line">    result = &#123;&#125;</span><br><span class="line">    <span class="comment"># os.walk 得到目录下的所有子目录和文件</span></span><br><span class="line">    sub_dirs = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> os.walk(INPUT_DATA)]</span><br><span class="line">    is_root_dir = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> sub_dir <span class="keyword">in</span> sub_dirs:</span><br><span class="line">        <span class="keyword">if</span> is_root_dir:</span><br><span class="line">            is_root_dir = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        extensions = [<span class="string">'jpg'</span>, <span class="string">'jpeg'</span>, <span class="string">'JPG'</span>, <span class="string">'JPEG'</span>]</span><br><span class="line">        file_list = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回path最后的文件名</span></span><br><span class="line">        dir_name = os.path.basename(sub_dir)</span><br><span class="line">        <span class="keyword">for</span> extension <span class="keyword">in</span> extensions:</span><br><span class="line">            file_glob = os.path.join(INPUT_DATA, dir_name, <span class="string">'*.'</span> + extension)</span><br><span class="line">            <span class="comment"># 匹配所有的符合条件的文件，并将其以list的形式返回。</span></span><br><span class="line">            file_list.extend(glob.glob(file_glob))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> file_list:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过目录名获取类别的名称</span></span><br><span class="line">        label_name = dir_name.lower()</span><br><span class="line"></span><br><span class="line">        training_images = []</span><br><span class="line">        testing_images = []</span><br><span class="line">        validation_images = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随机构造一个训练集80%，测试集10%，验证集10%</span></span><br><span class="line">        <span class="keyword">for</span> file_name <span class="keyword">in</span> file_list:</span><br><span class="line">            base_name = os.path.basename(file_name)</span><br><span class="line">            chance = np.random.randint(<span class="number">100</span>)</span><br><span class="line">            <span class="keyword">if</span> chance &lt; validation_percentage:</span><br><span class="line">                validation_images.append(base_name)</span><br><span class="line">            <span class="keyword">elif</span> chance &lt; (testing_percentage + validation_percentage):</span><br><span class="line">                testing_images.append(base_name)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                training_images.append(base_name)</span><br><span class="line"></span><br><span class="line">        result[label_name] = &#123;</span><br><span class="line">            <span class="string">'dir'</span>: dir_name,</span><br><span class="line">            <span class="string">'training'</span>: training_images,</span><br><span class="line">            <span class="string">'testing'</span>: testing_images,</span><br><span class="line">            <span class="string">'validation'</span>: validation_images</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_image_path</span><span class="params">(image_lists, image_dir, label_name, index, category)</span>:</span></span><br><span class="line">    label_lists = image_lists[label_name]</span><br><span class="line"></span><br><span class="line">    category_list = label_lists[category]</span><br><span class="line"></span><br><span class="line">    mod_index = index % len(category_list)</span><br><span class="line"></span><br><span class="line">    base_name = category_list[mod_index]</span><br><span class="line"></span><br><span class="line">    sub_dir = label_lists[<span class="string">'dir'</span>]</span><br><span class="line"></span><br><span class="line">    full_path = os.path.join(image_dir, sub_dir, base_name)</span><br><span class="line">    <span class="keyword">return</span> full_path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bottleneck_path</span><span class="params">(image_lists, label_name, index, category)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> get_image_path(image_lists, CACHE_DIR, label_name, index, category) + <span class="string">'.txt'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将当前图片作为输入计算瓶颈张量的值，这个瓶颈张量的值就是这个图的新的特征向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_botteneck_on_image</span><span class="params">(sess, image_data, image_data_tensor, bottleneck_tensor)</span>:</span></span><br><span class="line">    bottleneck_values = sess.run(bottleneck_tensor, &#123;image_data_tensor: image_data&#125;)</span><br><span class="line">    bottleneck_values = np.squeeze(bottleneck_values)</span><br><span class="line">    <span class="keyword">return</span> bottleneck_values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将所有的图片处理成数值向量形式存入</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_or_create_bottleneck</span><span class="params">(sess, image_lists, label_name, index,</span></span></span><br><span class="line"><span class="function"><span class="params">                             category, jpeg_data_tensor, bottleneck_tensor)</span>:</span></span><br><span class="line">    <span class="comment"># 获取一张图片对应的特征向量文件的路径</span></span><br><span class="line">    label_lists = image_lists[label_name]</span><br><span class="line">    sub_dir = label_lists[<span class="string">'dir'</span>]</span><br><span class="line">    sub_dir_path = os.path.join(CACHE_DIR, sub_dir)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(sub_dir_path):</span><br><span class="line">        os.makedirs(sub_dir_path)</span><br><span class="line">    bottleneck_path = get_bottleneck_path(image_lists, label_name, index, category)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(bottleneck_path):</span><br><span class="line">        image_path = get_image_path(image_lists, INPUT_DATA, label_name, index, category)</span><br><span class="line">        image_data = gfile.FastGFile(image_path, <span class="string">'rb'</span>).read()</span><br><span class="line">        bottleneck_values = run_botteneck_on_image(sess, image_data, jpeg_data_tensor, bottleneck_tensor)</span><br><span class="line">        bottleneck_string = <span class="string">','</span>.join(str(x) <span class="keyword">for</span> x <span class="keyword">in</span> bottleneck_values)</span><br><span class="line">        <span class="keyword">with</span> open(bottleneck_path, <span class="string">'w'</span>) <span class="keyword">as</span> bottleneck_file:</span><br><span class="line">            bottleneck_file.write(bottleneck_string)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">with</span> open(bottleneck_path, <span class="string">'r'</span>) <span class="keyword">as</span> bottleneck_file:</span><br><span class="line">            bottleneck_string = bottleneck_file.read()</span><br><span class="line">        bottleneck_values = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> bottleneck_string.split(<span class="string">','</span>)]</span><br><span class="line">    <span class="comment"># 返回特征向量</span></span><br><span class="line">    <span class="keyword">return</span> bottleneck_values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机获取任意种类，指定category类型的数据量为how_many的数据</span></span><br><span class="line"><span class="comment"># 返回数据bottlenecks = [HowMany, BOTTLENECK_TENSOR_SIZE]</span></span><br><span class="line"><span class="comment"># ground_truths = [HowMany, n_classes]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_cached_bottlenecks</span><span class="params">(sess, n_classes, image_lists, how_many, category,</span></span></span><br><span class="line"><span class="function"><span class="params">                                  jpeg_data_tensor, bottleneck_tensor)</span>:</span></span><br><span class="line">    bottlenecks = []</span><br><span class="line">    ground_truths = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(how_many):</span><br><span class="line">        label_index = random.randrange(n_classes)</span><br><span class="line">        label_name = list(image_lists.keys())[label_index]</span><br><span class="line">        image_index = random.randrange(<span class="number">65535</span>)</span><br><span class="line">        bottleneck = get_or_create_bottleneck(sess, image_lists, label_name, image_index,</span><br><span class="line">                                              category, jpeg_data_tensor, bottleneck_tensor)</span><br><span class="line"></span><br><span class="line">        ground_truth = np.zeros(n_classes, dtype=np.float32)</span><br><span class="line">        ground_truth[label_index] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        bottlenecks.append(bottleneck)</span><br><span class="line">        ground_truths.append(ground_truth)</span><br><span class="line">    <span class="keyword">return</span> bottlenecks, ground_truths</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取全部测试数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_bottlenecks</span><span class="params">(sess, image_lists, n_classes, jpeg_data_tensor, bottleneck_tensor)</span>:</span></span><br><span class="line">    bottlenecks = []</span><br><span class="line">    ground_truths = []</span><br><span class="line">    label_name_list = list(image_lists.keys())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> label_index, label_name <span class="keyword">in</span> enumerate(label_name_list):</span><br><span class="line">        category = <span class="string">'testing'</span></span><br><span class="line">        <span class="keyword">for</span> index, unused_base_name <span class="keyword">in</span> enumerate(image_lists[label_name][category]):</span><br><span class="line">            bottleneck = get_or_create_bottleneck(sess, image_lists, label_name, index, category,</span><br><span class="line">                                                  jpeg_data_tensor, bottleneck_tensor)</span><br><span class="line">            ground_truth = np.zeros(n_classes, dtype=np.float32)</span><br><span class="line">            ground_truth[label_index] = <span class="number">1.0</span></span><br><span class="line">            bottlenecks.append(bottleneck)</span><br><span class="line">            ground_truths.append(ground_truth)</span><br><span class="line">    <span class="keyword">return</span> bottlenecks, ground_truths</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">    <span class="comment"># 读取所有图片</span></span><br><span class="line">    image_lists = create_image_lists(TEST_PERCENTAGE, VALIDATION_PERCENTAGE)</span><br><span class="line">    <span class="comment"># 返回种类数量</span></span><br><span class="line">    n_classes = len(image_lists.keys())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取已经训练好的Inception-v3模型、</span></span><br><span class="line">    <span class="keyword">with</span> gfile.FastGFile(os.path.join(MODEL_DIR, MODEL_FILE), <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载读取的Inception-v3模型，返回数据输入所对应的张量以及计算瓶颈对应的张量</span></span><br><span class="line">    bottleneck_tensor, jpeg_data_tensor = tf.import_graph_def(</span><br><span class="line">        graph_def,</span><br><span class="line">        return_elements=[BOTTLENECK_TENSOR_NAME, JEPG_DATA_TENSOR_NAME])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义新的神经网络输入</span></span><br><span class="line">    bottleneck_input = tf.placeholder(</span><br><span class="line">        tf.float32, [<span class="literal">None</span>, BOTTLENECK_TENSOR_SIZE],</span><br><span class="line">        name=<span class="string">'BottleneckInputPlaceholder'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义新的标准答案输入</span></span><br><span class="line">    ground_truth_input = tf.placeholder(</span><br><span class="line">        tf.float32, [<span class="literal">None</span>, n_classes],</span><br><span class="line">        name=<span class="string">'GroundTruthInput'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后一层全链接解决新的图片分类问题</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'final_training_ops'</span>):</span><br><span class="line">        weights = tf.Variable(tf.truncated_normal([BOTTLENECK_TENSOR_SIZE, n_classes], stddev=<span class="number">0.001</span>))</span><br><span class="line">        biases = tf.Variable(tf.zeros([n_classes]))</span><br><span class="line">        logits = tf.matmul(bottleneck_input, weights) + biases</span><br><span class="line">        final_tensor = tf.nn.softmax(logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line">    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=ground_truth_input)</span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy_mean)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算正确率</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'evaluation'</span>):</span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(final_tensor, <span class="number">1</span>), tf.argmax(ground_truth_input, <span class="number">1</span>))</span><br><span class="line">        evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init = tf.initialize_all_variables()</span><br><span class="line">        sess.run(init)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练过程</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            train_bottlenecks, train_ground_truth = get_random_cached_bottlenecks(</span><br><span class="line">                sess, n_classes, image_lists, BATCH,</span><br><span class="line">                <span class="string">'training'</span>, jpeg_data_tensor, bottleneck_tensor)</span><br><span class="line"></span><br><span class="line">            sess.run(train_step, feed_dict=&#123;bottleneck_input: train_bottlenecks,</span><br><span class="line">                                            ground_truth_input: train_ground_truth&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 在验证数据上测试正确率</span></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">or</span> i + <span class="number">1</span> == STEPS:</span><br><span class="line">                validation_bottleneck, validation_ground_truth = get_random_cached_bottlenecks(</span><br><span class="line">                    sess, n_classes, image_lists, BATCH,</span><br><span class="line">                    <span class="string">'validation'</span>, jpeg_data_tensor, bottleneck_tensor)</span><br><span class="line"></span><br><span class="line">                validation_accuracy = sess.run(evaluation_step, feed_dict=&#123;</span><br><span class="line">                    bottleneck_input: validation_bottleneck,</span><br><span class="line">                    ground_truth_input: validation_ground_truth&#125;)</span><br><span class="line"></span><br><span class="line">                print(<span class="string">'Step %d : Validation accuracy on random sampled %d examples = %.1f%%'</span> % (i, BATCH, validation_accuracy * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在测试数据上测试正确率</span></span><br><span class="line">        test_bottlenecks, test_ground_truth = get_test_bottlenecks(</span><br><span class="line">            sess, image_lists, n_classes, jpeg_data_tensor, bottleneck_tensor)</span><br><span class="line"></span><br><span class="line">        test_accuracy = sess.run(evaluation_step, feed_dict=&#123;bottleneck_input: test_bottlenecks,</span><br><span class="line">                                                             ground_truth_input: test_ground_truth&#125;)</span><br><span class="line">        print(<span class="string">'Final test accuracy = %.1f%%'</span> % (test_accuracy * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeNet5实现MNIST模型</title>
      <link href="2019/05/17/tensorflow-cnn-lenet5/"/>
      <url>2019/05/17/tensorflow-cnn-lenet5/</url>
      
        <content type="html"><![CDATA[<h1 id="LeNet5介绍"><a href="#LeNet5介绍" class="headerlink" title="LeNet5介绍"></a>LeNet5介绍</h1><p>LeNet-5：是Yann LeCun在1998年设计的用于手写数字识别的卷积神经网络，当年美国大多数银行就是用它来识别支票上面的手写数字的，它是早期卷积神经网络中最有代表性的实验系统之一。<br>LenNet-5共有7层（不包括输入层），每层都包含不同数量的训练参数，如下图所示。<br><img src="/img/acticle/lenet5_model.png" alt=""><br>LeNet-5中主要有2个卷积层、2个下抽样层（池化层）、3个全连接层3种连接方式</p><h1 id="构建MNIST识别LeNet5模型"><a href="#构建MNIST识别LeNet5模型" class="headerlink" title="构建MNIST识别LeNet5模型"></a>构建MNIST识别LeNet5模型</h1><h2 id="输入数据为MNIST-DATA数据"><a href="#输入数据为MNIST-DATA数据" class="headerlink" title="输入数据为MNIST_DATA数据"></a>输入数据为MNIST_DATA数据</h2><p>input_tensor.shape = [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUMBER_CHANNELS]<br>[BATCH_SIZE, 28, 28, 1]</p><h2 id="卷积层-1"><a href="#卷积层-1" class="headerlink" title="卷积层-1"></a>卷积层-1</h2><p>weight = [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP] = [5, 5, 1, 32]<br>CONV1_SIZE代表卷积过滤器大小<br>NUM_CHANNELS代表通道数<br>CONV1_DEEP代表过滤器深度<br>conv1_biases=[CONV1_DEEP]<br>输出层结点矩阵为为[BATCH_SIZE, 24, 24, 32]</p><h2 id="池化层-1"><a href="#池化层-1" class="headerlink" title="池化层-1"></a>池化层-1</h2><p>ksize=[1, 2, 2, 1]<br>strides=[1, 2, 2, 1]<br>池化层过滤器大小为2，步长为2<br>则输出层结点矩阵为[BATCH_SIZE, 12, 12, 32]</p><h2 id="卷积层-2"><a href="#卷积层-2" class="headerlink" title="卷积层-2"></a>卷积层-2</h2><p>weights = [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP] = [5, 5, 1, 64]<br>输出层结点矩阵为[BATCH_SIZE, 8, 8, 64]</p><h2 id="池化层-2"><a href="#池化层-2" class="headerlink" title="池化层-2"></a>池化层-2</h2><p>ksize=[1, 2, 2, 1]<br>strides=[1, 2, 2, 1]<br>输出层结点矩阵为[BATCH_SIZE, 4, 4, 64]</p><h2 id="拉直操作"><a href="#拉直操作" class="headerlink" title="拉直操作"></a>拉直操作</h2><p>输出层为[BATCH_SIZE, 16 x 64]</p><h2 id="全链接-1"><a href="#全链接-1" class="headerlink" title="全链接-1"></a>全链接-1</h2><p>输入[BATCH_SIZE, 1024], weight = [1024, 512]</p><h2 id="全链接-2"><a href="#全链接-2" class="headerlink" title="全链接-2"></a>全链接-2</h2><p>输入[BATCH_SIZE, 512], weight = [1024, 10]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置参数</span></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line">IMAGE_SIZE = <span class="number">28</span></span><br><span class="line">NUM_CHANNELS = <span class="number">1</span></span><br><span class="line">NUM_LABELS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一层卷积层的尺寸和深度</span></span><br><span class="line">CONV1_DEEP = <span class="number">32</span></span><br><span class="line">CONV1_SIZE = <span class="number">5</span></span><br><span class="line"><span class="comment"># 第二层卷积层的尺寸和深度</span></span><br><span class="line">CONV2_DEEP = <span class="number">64</span></span><br><span class="line">CONV2_SIZE = <span class="number">5</span></span><br><span class="line"><span class="comment"># 全链接层的结点个数</span></span><br><span class="line">FC_SIZE = <span class="number">512</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, train, regularizer)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1-conv1'</span>):</span><br><span class="line">        conv1_weights = tf.get_variable(<span class="string">"weight"</span>, [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],</span><br><span class="line">                                        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        conv1_biases = tf.get_variable(<span class="string">"biases"</span>, [CONV1_DEEP])</span><br><span class="line"></span><br><span class="line">        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer2-pool1'</span>):</span><br><span class="line">        pool1 = tf.nn.max_pool(relu1, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer3-conv2'</span>):</span><br><span class="line">        conv2_weights = tf.get_variable(<span class="string">"weight"</span>, [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],</span><br><span class="line">                                        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        conv2_biases = tf.get_variable(<span class="string">"biases"</span>, [CONV2_DEEP])</span><br><span class="line"></span><br><span class="line">        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer4-pool2'</span>):</span><br><span class="line">        pool2 = tf.nn.max_pool(relu2, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将矩阵拉直成向量，pool_shape[0]为BATCH_SIZE的大小</span></span><br><span class="line">    pool_shape = pool2.get_shape().as_list()</span><br><span class="line">    nodes = pool_shape[<span class="number">1</span>] * pool_shape[<span class="number">2</span>] * pool_shape[<span class="number">3</span>]</span><br><span class="line">    reshaped = tf.reshape(pool2, [pool_shape[<span class="number">0</span>], nodes])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer5-fc1'</span>):</span><br><span class="line">        fc1_weights = tf.get_variable(<span class="string">"weight"</span>, [nodes, FC_SIZE],</span><br><span class="line">                                      initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        <span class="keyword">if</span> regularizer != <span class="literal">None</span>:</span><br><span class="line">            tf.add_to_collection(<span class="string">'losses'</span>, regularizer(fc1_weights))</span><br><span class="line">        fc1_biases = tf.get_variable(<span class="string">"biases"</span>, [FC_SIZE],</span><br><span class="line">                                     initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line">        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights), fc1_biases)</span><br><span class="line">        <span class="comment"># dropout可以避免过拟合，使得在测试数据上效果更好，一般用于全链接层</span></span><br><span class="line">        <span class="keyword">if</span> train:</span><br><span class="line">            fc1 = tf.nn.dropout(fc1, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer6-fc2'</span>):</span><br><span class="line">        fc2_weights = tf.get_variable(<span class="string">"weight"</span>, [FC_SIZE, NUM_LABELS],</span><br><span class="line">                                      initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        <span class="keyword">if</span> regularizer != <span class="literal">None</span>:</span><br><span class="line">            tf.add_to_collection(<span class="string">'losses'</span>, regularizer(fc2_weights))</span><br><span class="line">        fc2_biases = tf.get_variable(<span class="string">"biases"</span>, [NUM_LABELS],</span><br><span class="line">                                     initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line">        logit = tf.matmul(fc1, fc2_weights) + fc2_biases</span><br><span class="line">    <span class="keyword">return</span> logit</span><br></pre></td></tr></table></figure><h1 id="LaNet-5的局限性"><a href="#LaNet-5的局限性" class="headerlink" title="LaNet-5的局限性"></a>LaNet-5的局限性</h1><p>CNN能够得出原始图像的有效表征，这使得CNN能够直接从原始像素中，经过极少的预处理，识别视觉上面的规律。然而，由于当时缺乏大规模训练数据，计算机的计算能力也跟不上，LeNet-5 对于复杂问题的处理结果并不理想。</p><p>2006年起，人们设计了很多方法，想要克服难以训练深度CNN的困难。其中，最著名的是 Krizhevsky et al.提出了一个经典的CNN 结构，并在图像识别任务上取得了重大突破。其方法的整体框架叫做 AlexNet，与 LeNet-5 类似，但要更加深一些。 </p>]]></content>
      
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最佳实战MNIST识别</title>
      <link href="2019/05/17/tensorflow-best-mnist/"/>
      <url>2019/05/17/tensorflow-best-mnist/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络架构"><a href="#神经网络架构" class="headerlink" title="神经网络架构"></a>神经网络架构</h1><p>mnist_inference.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line">LAYER1_NODE = <span class="number">500</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到中间权重值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight_variable</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    weights = tf.get_variable(<span class="string">"weights"</span>, shape=shape, initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">    <span class="keyword">if</span> regularizer != <span class="literal">None</span>:</span><br><span class="line">        tf.add_to_collection(<span class="string">"losses"</span>, regularizer(weights))</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建两层网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, regularizer)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer1"</span>):</span><br><span class="line">        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)</span><br><span class="line">        biases = tf.get_variable(<span class="string">"biases"</span>, [LAYER1_NODE], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer2"</span>):</span><br><span class="line">        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)</span><br><span class="line">        biases = tf.get_variable(<span class="string">"biases"</span>, [OUTPUT_NODE], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        layer2 = tf.matmul(layer1, weights) + biases</span><br><span class="line">    <span class="keyword">return</span> layer2</span><br></pre></td></tr></table></figure></p><h1 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h1><ol><li>这里每训练一千次就将sess进行保存</li></ol><p>mnist_train.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> Google_Tensorflow.part_5.mnist_inference <span class="keyword">as</span> mnist_inference</span><br><span class="line"></span><br><span class="line"><span class="comment"># batch大小</span></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line"><span class="comment"># 基础学习率</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.8</span></span><br><span class="line"><span class="comment"># 学习率延迟</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span></span><br><span class="line"><span class="comment"># 正则化参数</span></span><br><span class="line">REGULARAZTION_RATE = <span class="number">0.0001</span></span><br><span class="line"><span class="comment"># 训练步数</span></span><br><span class="line">TRAINING_STEPS = <span class="number">30000</span></span><br><span class="line"><span class="comment"># 滑动平均延迟</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br><span class="line"><span class="comment"># 模型保存路径和文件名</span></span><br><span class="line">MODEL_SAVE_PATH = <span class="string">"./model1/"</span></span><br><span class="line">MODEL_NAME = <span class="string">"model.ckpt"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_inference.INPUT_NODE], name=<span class="string">'x-input'</span>)</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_inference.OUTPUT_NODE], name=<span class="string">'y-input'</span>)</span><br><span class="line">    <span class="comment"># 正则化参数</span></span><br><span class="line">    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)</span><br><span class="line">    y = mnist_inference.inference(x, regularizer)</span><br><span class="line"></span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 滑动平均值</span></span><br><span class="line">    variable_average = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    variable_average_op = variable_average.apply(tf.trainable_variables())</span><br><span class="line">    <span class="comment"># 交叉熵</span></span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line">    loss = cross_entropy_mean + tf.add_n(tf.get_collection(<span class="string">"losses"</span>))</span><br><span class="line"></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples/BATCH_SIZE,</span><br><span class="line">        LEARNING_RATE_DECAY</span><br><span class="line">    )</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, variable_average_op]):</span><br><span class="line">        train_op = tf.no_op(name=<span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        tf.initialize_all_variables().run()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict=&#123;x: xs, y_: ys&#125;)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"After %d training step(s), loss on training batch is %g."</span> % (step, loss_value))</span><br><span class="line">                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">'./MNIST_DATA'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure></p><h1 id="测试训练"><a href="#测试训练" class="headerlink" title="测试训练"></a>测试训练</h1><p>ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)<br>每次会读取最新保存的模型，并验证其正确率<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> Google_Tensorflow.part_5.mnist_inference <span class="keyword">as</span> mnist_inference</span><br><span class="line"><span class="keyword">import</span> Google_Tensorflow.part_5.mnist_train <span class="keyword">as</span> mnist_train</span><br><span class="line"></span><br><span class="line">EVAL_INTERVAL_SECS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</span><br><span class="line">        x = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_inference.INPUT_NODE], name=<span class="string">'x-input'</span>)</span><br><span class="line">        y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_inference.OUTPUT_NODE], name=<span class="string">'y-input'</span>)</span><br><span class="line">        validate_feed = &#123;x: mnist.validation.images,</span><br><span class="line">                         y_: mnist.validation.labels&#125;</span><br><span class="line">        <span class="comment"># 计算准确率</span></span><br><span class="line">        y = mnist_inference.inference(x, <span class="literal">None</span>)</span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 滑动平均值</span></span><br><span class="line">        variable_averages = tf.train.ExponentialMovingAverage(</span><br><span class="line">            mnist_train.MOVING_AVERAGE_DECAY)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        validates_to_restore = variable_averages.variables_to_restore()</span><br><span class="line">        saver = tf.train.Saver(validates_to_restore)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">                <span class="comment"># 检查点状态，checkpoint文件</span></span><br><span class="line">                ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)</span><br><span class="line">                <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">                    <span class="comment"># 从当前检查点载入sess，并运行得到准确率</span></span><br><span class="line">                    saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">                    global_step = ckpt.model_checkpoint_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'-'</span>)[<span class="number">-1</span>]</span><br><span class="line">                    accuracy_score = sess.run(accuracy, feed_dict=validate_feed)</span><br><span class="line">                    print(<span class="string">'After %s training step(s), validation accuracy = %g'</span> % (global_step, accuracy_score))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    print(<span class="string">'No checkpoint file found'</span>)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">                time.sleep(EVAL_INTERVAL_SECS)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">'./MNIST_DATA'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">    evaluate(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow模型持久化</title>
      <link href="2019/05/13/tensorflow-saver/"/>
      <url>2019/05/13/tensorflow-saver/</url>
      
        <content type="html"><![CDATA[<h1 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h1><ol><li>saver.save可以将tensorflow模型保存到路径下</li><li>tensorflow会将计算图的结构和图上参数取值分开保存</li><li>model.ckpt.meta保存了计算图的结构</li><li>model.ckpt保存了程序中每一个变量的取值</li><li>checkpoint保存了一个目录下所有的模型文件列表</li></ol><h2 id="保存模型code"><a href="#保存模型code" class="headerlink" title="保存模型code"></a>保存模型code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line"></span><br><span class="line">init_op = tf.initialize_all_variables()</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    saver.save(sess, <span class="string">"./model/model.ckpt"</span>)</span><br></pre></td></tr></table></figure><p><img src="/img/article/saver_test1" alt=""></p><h2 id="加载模型数据"><a href="#加载模型数据" class="headerlink" title="加载模型数据"></a>加载模型数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"./model/model.ckpt"</span>)</span><br><span class="line">    print(sess.run(result))</span><br></pre></td></tr></table></figure><h2 id="加载全部变量"><a href="#加载全部变量" class="headerlink" title="加载全部变量"></a>加载全部变量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">"./model/model.ckpt.meta"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"./model/model.ckpt"</span>)</span><br><span class="line">    print(sess.run(tf.get_default_graph().get_tensor_by_name(<span class="string">"add:0"</span>)))</span><br></pre></td></tr></table></figure><h1 id="保存加载部分变量"><a href="#保存加载部分变量" class="headerlink" title="保存加载部分变量"></a>保存加载部分变量</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">'other-v1'</span>)</span><br><span class="line">v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">'other-v2'</span>)</span><br><span class="line">saver = tf.train.Saver(&#123;<span class="string">"v1"</span>: v1, <span class="string">"v2"</span>: v2&#125;)</span><br></pre></td></tr></table></figure><h1 id="整个计算图放在一个文件中"><a href="#整个计算图放在一个文件中" class="headerlink" title="整个计算图放在一个文件中"></a>整个计算图放在一个文件中</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> graph_util</span><br><span class="line">v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">'v1'</span>)</span><br><span class="line">v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">'v2'</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line"></span><br><span class="line">init_op = tf.initialize_all_variables()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">sess.run(init_op)</span><br><span class="line">graph_def = tf.get_default_graph().as_graph_def()</span><br><span class="line">output_graph_def = graph_util.convert_variables_to_constants(sess, graph_def, [<span class="string">'add'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.gfile.GFile(<span class="string">"./model/combined_model.pb"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">f.write(output_graph_def.SerializeToString())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span>  gfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    model_filename = <span class="string">'./model/combined_model.pb'</span></span><br><span class="line">    <span class="comment"># 读取保存的模型文件，解析成对应的protobuffer</span></span><br><span class="line">    <span class="keyword">with</span> gfile.FastGFile(model_filename, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line"></span><br><span class="line">    result = tf.import_graph_def(graph_def, return_elements=[<span class="string">"add:0"</span>])</span><br><span class="line">    print(sess.run(result))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow变量管理</title>
      <link href="2019/05/13/tensorflow-variable-test/"/>
      <url>2019/05/13/tensorflow-variable-test/</url>
      
        <content type="html"><![CDATA[<h1 id="变量命名空间"><a href="#变量命名空间" class="headerlink" title="变量命名空间"></a>变量命名空间</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价定义</span></span><br><span class="line">a = tf.get_variable(<span class="string">"v"</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line">b = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>], name=<span class="string">'v'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在名字为foo的命令空间创建名字为v的变量</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</span><br><span class="line">    v = tf.get_variable(<span class="string">"v"</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为在命名空间foo中已经存在名字为v的变量，所有下面的代码将会报错</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</span><br><span class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">    v2 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line">    print(v == v2)  <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将参数设置为True时，tf.get_variable只能获取已经创建过的变量，</span></span><br><span class="line"><span class="comment"># 命名空间bar中没有创建变量v，所以下面会报错</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h1 id="两层网络变量处理"><a href="#两层网络变量处理" class="headerlink" title="两层网络变量处理"></a>两层网络变量处理</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, resue=False)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1'</span>, resue=resue):</span><br><span class="line">        weights = tf.get_variable(<span class="string">"weights"</span>, [INPUT_NODE, LAYER1_NODE],</span><br><span class="line">                                   initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        biases = tf.get_variable(<span class="string">"biases"</span>, [LAYER1_NODE],</span><br><span class="line">                                 initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer2'</span>, resue=resue):</span><br><span class="line">        weights = tf.get_variable(<span class="string">"weights"</span>, [INPUT_NODE, LAYER1_NODE],</span><br><span class="line">                                  initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        biases = tf.get_variable(<span class="string">"biases"</span>, [LAYER1_NODE],</span><br><span class="line">                                 initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        layer2 = tf.nn.relu(tf.matmul(layer1, weights) + biases)</span><br><span class="line">    <span class="keyword">return</span> layer2</span><br></pre></td></tr></table></figure><h1 id="命名空间嵌套"><a href="#命名空间嵌套" class="headerlink" title="命名空间嵌套"></a>命名空间嵌套</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"root"</span>):</span><br><span class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line">    print(v1.name)</span><br><span class="line">    print(tf.get_variable_scope().reuse)  <span class="comment"># 输出False</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</span><br><span class="line">        v2 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line">        print(v2.name)</span><br><span class="line">        print(tf.get_variable_scope().reuse)  <span class="comment"># 输出False</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">            print(tf.get_variable_scope().reuse)  <span class="comment"># 输出True</span></span><br><span class="line">    print(tf.get_variable_scope().reuse)  <span class="comment"># 输出False，退出</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MNIST手写数字识别</title>
      <link href="2019/05/11/tensorflow-mnist-train/"/>
      <url>2019/05/11/tensorflow-mnist-train/</url>
      
        <content type="html"><![CDATA[<h1 id="MNIST数字识别Code"><a href="#MNIST数字识别Code" class="headerlink" title="MNIST数字识别Code"></a>MNIST数字识别Code</h1><h2 id="tf-train-exponential-decay"><a href="#tf-train-exponential-decay" class="headerlink" title="tf.train.exponential_decay"></a>tf.train.exponential_decay</h2><p>学习率指数衰减，参数为：基础学习率,训练次数,batch数量,衰减度</p><h2 id="tf-train-ExponentialMovingAverage"><a href="#tf-train-ExponentialMovingAverage" class="headerlink" title="tf.train.ExponentialMovingAverage"></a>tf.train.ExponentialMovingAverage</h2><p>初始化滑动平均类，参数为：滑动平均衰减率，当前训练步数</p><h2 id="tf-control-dependencies"><a href="#tf-control-dependencies" class="headerlink" title="tf.control_dependencies"></a>tf.control_dependencies</h2><p>tf.control_dependencies是tensorflow中的一个flow顺序控制机制<br>作用有二：插入依赖（dependencies）和清空依赖（依赖是op或tensor）<br>此处也就是等同于调用tf.group(train_step, variable_average_op)<br>train_op = tf.no_op(name=’train’)什么也不做<br>后面调用train_op的时候，等同于调用train_step, variable_average_op这两个操作都会被调用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入结点</span></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line"><span class="comment"># 输出结点</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line"><span class="comment"># 隐藏层书</span></span><br><span class="line">LAYER1_NODE = <span class="number">500</span></span><br><span class="line"><span class="comment"># batch包大小</span></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.8</span></span><br><span class="line"><span class="comment"># 学习率的衰减率</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span></span><br><span class="line"><span class="comment"># 正则化系数</span></span><br><span class="line">REGULARIZATION_DATE = <span class="number">0.0001</span></span><br><span class="line"><span class="comment"># 训练轮数</span></span><br><span class="line">TRAINING_STEPS = <span class="number">30000</span></span><br><span class="line"><span class="comment"># 滑动平均衰减率</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, avg_class, weights1, biases1, weights2, biases2)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> avg_class == <span class="literal">None</span>:</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(layer1, weights2) + biases2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, INPUT_NODE], name=<span class="string">'x-input'</span>)</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, OUTPUT_NODE], name=<span class="string">'y-input'</span>)</span><br><span class="line"></span><br><span class="line">    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=<span class="number">0.1</span>))</span><br><span class="line">    biases1 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[LAYER1_NODE]))</span><br><span class="line">    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=<span class="number">0.1</span>))</span><br><span class="line">    biases2 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[OUTPUT_NODE]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不使用参数的滑动平均值</span></span><br><span class="line">    y = inference(x, <span class="literal">None</span>, weights1, biases1, weights2, biases2)</span><br><span class="line">    <span class="comment"># 不可训练参数</span></span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 初始化滑动平均类</span></span><br><span class="line">    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    <span class="comment"># GraphKeys.TRAINING_VARIABLES中的元素，就是可以训练变量的集合</span></span><br><span class="line">    variable_average_op = variable_averages.apply(tf.trainable_variables())</span><br><span class="line">    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># tf.argmax(y_, 1)是求y_行中最大值的索引，即为输入标签的数字，y是得出最后的预测实际数字</span></span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line"></span><br><span class="line">    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_DATE)</span><br><span class="line">    regularization = regularizer(weights1) + regularizer(weights2)</span><br><span class="line">    loss = cross_entropy_mean + regularization</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置指数衰减学习率</span></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples / BATCH_SIZE,</span><br><span class="line">        LEARNING_RATE_DECAY,</span><br><span class="line">        staircase=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, variable_average_op]):</span><br><span class="line">        train_op = tf.no_op(name=<span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算正确率</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(average_y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        <span class="comment"># 验证数据</span></span><br><span class="line">        validate_feed = &#123;x: mnist.validation.images, y_: mnist.validation.labels&#125;</span><br><span class="line">        <span class="comment"># 测试数据</span></span><br><span class="line">        test_feed = &#123;x: mnist.test.images, y_: mnist.test.labels&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 验证数据</span></span><br><span class="line">                <span class="comment"># 训练过程中验证集采用滑动平均模型得出的accuracy</span></span><br><span class="line">                validation_acc = sess.run(accuracy, feed_dict=validate_feed)</span><br><span class="line">                print(<span class="string">'After %d training step(s), validation accuracy using average model is %g'</span> % (i, validation_acc))</span><br><span class="line"></span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            sess.run(train_op, feed_dict=&#123;x: xs, y_: ys&#125;)</span><br><span class="line"></span><br><span class="line">        test_acc = sess.run(accuracy, feed_dict=test_feed)</span><br><span class="line">        print(<span class="string">'After %d training step(s), test accuracy using average model is %g'</span> % (TRAINING_STEPS, test_acc))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">'./MNIST_DATA'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之滑动平均模型</title>
      <link href="2019/05/10/tensorflow-moving-average-model/"/>
      <url>2019/05/10/tensorflow-moving-average-model/</url>
      
        <content type="html"><![CDATA[<h1 id="滑动平均模型"><a href="#滑动平均模型" class="headerlink" title="滑动平均模型"></a>滑动平均模型</h1><p>tf.train.ExponentialMovingAverage(decay, step)<br>decay = min{decay, 1+step/(10+step)}<br>shadow_variable = decay <em> shadow_variable + (1-decay) </em> v1<br>ema.average(v1) = shadow_variable<br>shadow_variable为影子变量，variable为待更新的变量，decay为衰减率<br>decay决定了模型的更新速度，decay越大，模型越稳定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 滑动平均模型</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">v1 = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line"><span class="comment"># 模拟轮数，控制衰减率</span></span><br><span class="line">step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个滑动平均的类class，初始化时给定了衰减率，和控制衰减率的变量step</span></span><br><span class="line">ema = tf.train.ExponentialMovingAverage(<span class="number">0.99</span>, step)</span><br><span class="line"><span class="comment"># 定义一个更新变量滑动平均的操作，这里需要给定一个列表，每次执行这个操作时</span></span><br><span class="line"><span class="comment"># 列表中的变量都会被更新</span></span><br><span class="line">maintain_averages_op = ema.apply([v1])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.initialize_all_variables()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="comment"># 初始值为(0, 0)</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新变量的值 为5</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 衰减率 = min&#123;0.99, (1+step)/(10+step)&#125; = 0.1</span></span><br><span class="line">    <span class="comment"># 数值为0.1*0 + 0.9*5 = 4.5</span></span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(tf.assign(step, <span class="number">10000</span>))</span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 衰减率 = min&#123;0.99, (1+step)/(10+step)&#125; = 0.99</span></span><br><span class="line">    <span class="comment"># 数值为0.99*4.5 + 0.01*10 = 4.555</span></span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 衰减率 = min&#123;0.99, (1+step)/(10+step)&#125; = 0.99</span></span><br><span class="line">    <span class="comment"># 数值为0.99*4.555 + 0.01*10 = 4.60945</span></span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>五层神经网络带L2正则的损失函数计算方法</title>
      <link href="2019/05/10/tensorflow-fivelayer-with-loss-L2/"/>
      <url>2019/05/10/tensorflow-fivelayer-with-loss-L2/</url>
      
        <content type="html"><![CDATA[<h1 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h1><h2 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h2><p>tf.add_to_collection(‘losses’, num)<br>将num收集到collection中，<br>tf.get_collection可以查看数据，返回是一个列表<br>tf.add_n(list)将所有数据加起来</p><h2 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h2><p>contrib.layers.l2_regularizer(_lambda)(var)<br>lambda为正则化参数，var为正则化对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib <span class="keyword">as</span> contrib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取一层神经网络的权重，并将权重写入losses集合, _lambda为正则化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, _lambda)</span>:</span></span><br><span class="line">    var = tf.Variable(tf.random_normal(tf.float32, shape))</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, contrib.layers.l2_regularizer(_lambda)(var))</span><br><span class="line">    <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"><span class="comment"># 定义每层NN的维度</span></span><br><span class="line">layer_dimension = [<span class="number">2</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">1</span>]</span><br><span class="line">n_layers = len(layer_dimension)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape = (None, 2)</span></span><br><span class="line">cur_layer = x</span><br><span class="line">in_dimension = layer_dimension[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每层网络</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">    <span class="comment"># 输出层维度</span></span><br><span class="line">    out_dimension = layer_dimension[<span class="number">1</span>]</span><br><span class="line">    weight = get_weight([in_dimension, out_dimension], <span class="number">0.001</span>)</span><br><span class="line">    bias = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[out_dimension]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用ReLU激活函数(None, 2) * (2, 10)</span></span><br><span class="line">    cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)</span><br><span class="line">    in_dimension = layer_dimension[i]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小均方误差</span></span><br><span class="line">mse_loss = tf.reduce_mean(tf.square(y_ - cur_layer))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入损失集合</span></span><br><span class="line">tf.add_to_collection(<span class="string">'losses'</span>, mse_loss)</span><br><span class="line"></span><br><span class="line">loss = tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 正则化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之实现神经网络常见层</title>
      <link href="2019/05/09/tensorflow-mutiply-layer/"/>
      <url>2019/05/09/tensorflow-mutiply-layer/</url>
      
        <content type="html"><![CDATA[<h1 id="卷积原理"><a href="#卷积原理" class="headerlink" title="卷积原理"></a>卷积原理</h1><ol><li>输入矩阵格式：四个维度，依次为：样本数、图像高度、图像宽度、图像通道数</li><li>输出矩阵格式：与输出矩阵的维度顺序和含义相同，但是后三个维度（图像高度、图像宽度、图像通道数）的尺寸发生变化。</li><li>权重矩阵（卷积核）格式：同样是四个维度，但维度的含义与上面两者都不同，为：卷积核高度、卷积核宽度、输入通道数、输出通道数（卷积核个数）</li><li>输入矩阵、权重矩阵、输出矩阵这三者之间的相互决定关系</li><li>卷积核的输入通道数（in depth）由输入矩阵的通道数所决定。（红色标注）</li><li>输出矩阵的通道数（out depth）由卷积核的输出通道数所决定。（绿色标注）</li><li>输出矩阵的高度和宽度（height, width）这两个维度的尺寸由输入矩阵、卷积核、扫描方式所共同决定。计算公式如下。（蓝色标注）<br><img src="/img/article/filter_func.png" alt=""></li></ol><h1 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h1><ol><li>tensorflow层是四维设计的,[batch_size, width, height, channels]，input4d.shape=(1, 1, 25, 1)，本例中，批量大小为1，宽度为1，高度为25，颜色通道为1</li><li>扩展维度expand_dims(),降维squeeze()，</li><li>卷积层结果维度公式：output_size = (W - F + 2P)/S + 1</li><li>W是输入数据维度，F是过滤层大小，P是padding大小，S是步长</li><li>filter的维度=(1, 5, 1, 1)，过滤器大小为1x5，输入通道为1， 输出通道（即卷积核个数）为1</li></ol><h2 id="卷积层-1"><a href="#卷积层-1" class="headerlink" title="卷积层-1"></a>卷积层-1</h2><p>输入[1, 1, 25, 1]，w=[1, 5, 1, 1]，输出为[1, 1, 21, 1]</p><h2 id="池化层-1"><a href="#池化层-1" class="headerlink" title="池化层-1"></a>池化层-1</h2><ol><li>池化层和卷积层类似，但是没有过滤层，只有形状，步长，和padding选项</li><li>输入[1, 1, 21, 1]，池化过滤器大小[1, 1, 5, 1]，输出为[1, 1, 17, 1]</li></ol><h2 id="全链接层"><a href="#全链接层" class="headerlink" title="全链接层"></a>全链接层</h2><ol><li>全链接weight_shape=[17, 5]，所以输入为[1, 1, 17, 1]，通过 tf.squeeze压缩到[1, 17]，最后得到shape=[1, 5]</li></ol><h2 id="expand-dims用法"><a href="#expand-dims用法" class="headerlink" title="expand_dims用法"></a>expand_dims用法</h2><p><img src="/img/article/expand_dims_test.png" alt=""></p><ol><li>tf.expand_dims(Matrix, axis) 即在第axis维度处添加一个维度</li><li>如上图, input1.shape = (5), tf.expand_dims(input1d, 0)即在第0个维度加一个即shape=(1,5)</li><li>在input3,shape=(1,1,5)情况下，调用tf.expand_dims(input3d, 3)即在第三个维度处添加一个即为shape=(1, 1, 5, 1)</li></ol><h2 id="squeeze用法"><a href="#squeeze用法" class="headerlink" title="squeeze用法"></a>squeeze用法</h2><p>squeeze(<br>    input,<br>    axis=None,<br>    name=None,<br>    squeeze_dims=None<br>)<br>类似expand_dims,他是删除维度为1的所有维度，或者指定维度(维度必须为1才能删除)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 初始化数据，长度为25</span></span><br><span class="line">data_size = <span class="number">25</span></span><br><span class="line">data_1d = np.random.normal(size=data_size)</span><br><span class="line">print(<span class="string">'Input data: '</span>)</span><br><span class="line">print(data_1d)</span><br><span class="line">x_input_id = tf.placeholder(dtype=tf.float32, shape=[data_size])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个卷积层函数，声明一个随机过滤层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_layer_1d</span><span class="params">(input_1d, my_filter)</span>:</span></span><br><span class="line">    <span class="comment"># 将输入扩展维度为4维，【batch_size, width, height, channels】</span></span><br><span class="line">    <span class="comment"># 输出维度为output_size = (W - F + 2P)/S + 1</span></span><br><span class="line">    <span class="comment"># W:输入数据维度</span></span><br><span class="line">    <span class="comment"># F:过滤层大小</span></span><br><span class="line">    <span class="comment"># P:padding大小</span></span><br><span class="line">    <span class="comment"># S:步长大小</span></span><br><span class="line">    input_2d = tf.expand_dims(input_1d, <span class="number">0</span>)</span><br><span class="line">    input_3d = tf.expand_dims(input_2d, <span class="number">0</span>)</span><br><span class="line">    input_4d = tf.expand_dims(input_3d, <span class="number">3</span>)</span><br><span class="line">    convolution_output = tf.nn.conv2d(input_4d,</span><br><span class="line">                                      filter=my_filter,</span><br><span class="line">                                      strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                      padding=<span class="string">'VALID'</span>)</span><br><span class="line">    conv_output_1d = tf.squeeze(convolution_output)</span><br><span class="line">    <span class="keyword">return</span>(conv_output_1d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成一个过滤层窗口大小</span></span><br><span class="line">my_filter = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"><span class="comment"># 卷积层输出结果</span></span><br><span class="line">my_convolution_output = conv_layer_1d(x_input_id, my_filter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明一个激活函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activation</span><span class="params">(input_1d)</span>:</span></span><br><span class="line">    <span class="keyword">return</span>(tf.nn.relu(input_1d))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积层输出后经过激活函数的结果</span></span><br><span class="line">my_activation_output = activation(my_convolution_output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明池化层函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool</span><span class="params">(input_1d, width)</span>:</span></span><br><span class="line">    input_2d = tf.expand_dims(input_1d, <span class="number">0</span>)</span><br><span class="line">    input_3d = tf.expand_dims(input_2d, <span class="number">0</span>)</span><br><span class="line">    input_4d = tf.expand_dims(input_3d, <span class="number">3</span>)</span><br><span class="line">    pool_out = tf.nn.max_pool(input_4d, ksize=[<span class="number">1</span>, <span class="number">1</span>, width, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line">    pool_output_1d = tf.squeeze(pool_out)</span><br><span class="line">    <span class="keyword">return</span>(pool_output_1d)</span><br><span class="line"></span><br><span class="line">my_maxpool_output = max_pool(my_activation_output, width=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后一层连接的是全链接层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fully_connected</span><span class="params">(input_layer, num_outputs)</span>:</span></span><br><span class="line">    weight_shape = tf.squeeze(tf.stack([tf.shape(input_layer), [num_outputs]]))</span><br><span class="line">    weight = tf.random_normal(weight_shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    bias = tf.random_normal(shape=[num_outputs])</span><br><span class="line"></span><br><span class="line">    input_layer_2d = tf.expand_dims(input_layer, <span class="number">0</span>)</span><br><span class="line">    full_output = tf.add(tf.matmul(input_layer_2d, weight), bias)</span><br><span class="line"></span><br><span class="line">    full_output_1d = tf.squeeze(full_output)</span><br><span class="line">    <span class="keyword">return</span> full_output_1d</span><br><span class="line"></span><br><span class="line">my_full_output = fully_connected(my_maxpool_output, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量,运行计算图打印每层输出结果</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line">feed_dict = &#123;x_input_id: data_1d&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积层输出</span></span><br><span class="line">print(<span class="string">'Input = array of length 25'</span>)</span><br><span class="line">print(<span class="string">'Convolution w/filter, length = 5, stride_size = 1, result in an array of length 21: '</span>)</span><br><span class="line">print(sess.run(my_convolution_output, feed_dict=feed_dict))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活函数输出</span></span><br><span class="line">print(<span class="string">'Input = the above array of length 21'</span>)</span><br><span class="line">print(<span class="string">'ReLU element wise returns the array of length 21: '</span>)</span><br><span class="line">print(sess.run(my_activation_output, feed_dict=feed_dict))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 池化层输出</span></span><br><span class="line">print(<span class="string">'Input = the above array of length 21'</span>)</span><br><span class="line">print(<span class="string">'MaxPool, window length = 5, stride size = 1, results in array of length 17: '</span>)</span><br><span class="line">print(sess.run(my_maxpool_output, feed_dict=feed_dict))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全链接层输出</span></span><br><span class="line">print(<span class="string">'Input = the above array of length 17'</span>)</span><br><span class="line">print(<span class="string">'Fully connected layer on all four rows with five outputs: '</span>)</span><br><span class="line">print(sess.run(my_full_output, feed_dict=feed_dict))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之单层神经网络</title>
      <link href="2019/05/09/tensorflow-singlelayer-network/"/>
      <url>2019/05/09/tensorflow-singlelayer-network/</url>
      
        <content type="html"><![CDATA[<h1 id="单层网络"><a href="#单层网络" class="headerlink" title="单层网络"></a>单层网络</h1><ol><li>x_vals输入的shape=(?, 3),?代表数据集样本个数</li><li>y_vals输入的shape=(1, ?),?代表数据集样本个数</li><li>归一化normalize_cols中axis可以理解为：将矩阵投射到0维度，下每一列选取最大最小值，然后进行归一化处理</li><li>x_data的shape=(?, 3),?代表一个batch_size大小</li><li>y_target的shape=(?, 1),?代表一个batch_size大小</li><li>隐藏层维度计算为[?, 3] * [3, layer_nodes] + [layer_nodes]</li><li>最后模型输出[None, layer_nodes] * [layer_nodes, 1] + [1]</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># shape = [None, 3]</span></span><br><span class="line">x_vals = np.array([x[<span class="number">0</span>:<span class="number">3</span>] <span class="keyword">for</span> x <span class="keyword">in</span> iris.data])</span><br><span class="line"><span class="comment"># shape = [1, None]</span></span><br><span class="line">y_vals = np.array([x[<span class="number">3</span>] <span class="keyword">for</span> x <span class="keyword">in</span> iris.data])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子</span></span><br><span class="line">seed = <span class="number">2</span></span><br><span class="line">tf.set_random_seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">train_indices = np.random.choice(len(x_vals), round(len(x_vals) * <span class="number">0.8</span>), replace=<span class="literal">False</span>)</span><br><span class="line">test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))</span><br><span class="line"></span><br><span class="line">x_vals_train = x_vals[train_indices]</span><br><span class="line">x_vals_test = x_vals[test_indices]</span><br><span class="line">y_vals_train = y_vals[train_indices]</span><br><span class="line">y_vals_test = y_vals[test_indices]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化处理, axis指的是从第0维度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_cols</span><span class="params">(m)</span>:</span></span><br><span class="line">    col_max = m.max(axis=<span class="number">0</span>)</span><br><span class="line">    col_min = m.min(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> (m-col_min)/(col_max - col_min)</span><br><span class="line"></span><br><span class="line">x_vals_train = np.nan_to_num(normalize_cols(x_vals_train))</span><br><span class="line">x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">3</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">hidden_layer_nodes = <span class="number">10</span></span><br><span class="line">A1 = tf.Variable(tf.random_normal(shape=[<span class="number">3</span>, hidden_layer_nodes]))</span><br><span class="line">b1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]))</span><br><span class="line">A2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes, <span class="number">1</span>]))</span><br><span class="line">b2 = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建隐藏层输出 [None, 3] * [3, layer_nodes] + [layer_nodes]</span></span><br><span class="line">hidden_output = tf.nn.relu(tf.add(tf.matmul(x_data, A1), b1))</span><br><span class="line"><span class="comment"># 创建训练模型的最后输出 [None, layer_nodes] * [layer_nodes, 1] + [1]</span></span><br><span class="line">final_output = tf.nn.relu(tf.add(tf.matmul(hidden_output, A2), b2))</span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y_target - final_output))</span><br><span class="line"></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.005</span>)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代训练模型</span></span><br><span class="line">loss_vec = []</span><br><span class="line">test_loss = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    rand_index = np.random.choice(len(x_vals_train), size=batch_size)</span><br><span class="line">    <span class="comment"># shape = (batch_size, 3)</span></span><br><span class="line">    rand_x = x_vals_train[rand_index]</span><br><span class="line">    <span class="comment"># shape = (batch_size, 1)</span></span><br><span class="line">    rand_y = np.transpose([y_vals_train[rand_index]])</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    loss_vec.append(np.sqrt(temp_loss))</span><br><span class="line"></span><br><span class="line">    test_temp_loss = sess.run(loss, feed_dict=&#123;x_data: x_vals_test, y_target: np.transpose([y_vals_test])&#125;)</span><br><span class="line">    test_loss.append(np.sqrt(test_temp_loss))</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Generation: '</span> + str(i+<span class="number">1</span>) + <span class="string">' Loss = '</span> + str(temp_loss))</span><br><span class="line"></span><br><span class="line">plt.plot(loss_vec, <span class="string">'k-'</span>, label=<span class="string">'Train Loss'</span>)</span><br><span class="line">plt.plot(test_loss, <span class="string">'r--'</span>, label=<span class="string">'Test Loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Loss (MSE) per Generation'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Generation'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/img/article/single_layer_nn.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之实现多分类SVM</title>
      <link href="2019/05/06/tensorflow-multy-classify-svm/"/>
      <url>2019/05/06/tensorflow-multy-classify-svm/</url>
      
        <content type="html"><![CDATA[<h1 id="多分类SVM"><a href="#多分类SVM" class="headerlink" title="多分类SVM"></a>多分类SVM</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>通过参数b增加一个维度计算三个模型<br>使用一对多，为每类创建一个分类器，最后预测类别是具有最大SVM间隔的分类</p><h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_vals = np.array([[x[<span class="number">0</span>], x[<span class="number">3</span>]] <span class="keyword">for</span> x <span class="keyword">in</span> iris.data])</span><br><span class="line">y_vals1 = np.array([<span class="number">1</span> <span class="keyword">if</span> y == <span class="number">0</span> <span class="keyword">else</span> <span class="number">-1</span> <span class="keyword">for</span> y <span class="keyword">in</span> iris.target])</span><br><span class="line">y_vals2 = np.array([<span class="number">1</span> <span class="keyword">if</span> y == <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span> <span class="keyword">for</span> y <span class="keyword">in</span> iris.target])</span><br><span class="line">y_vals3 = np.array([<span class="number">1</span> <span class="keyword">if</span> y == <span class="number">2</span> <span class="keyword">else</span> <span class="number">-1</span> <span class="keyword">for</span> y <span class="keyword">in</span> iris.target])</span><br><span class="line">y_vals = np.array([y_vals1, y_vals2, y_vals3])</span><br><span class="line"></span><br><span class="line">class1_x = [x[<span class="number">0</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> iris.target[i] == <span class="number">0</span>]</span><br><span class="line">class1_y = [x[<span class="number">1</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> iris.target[i] == <span class="number">0</span>]</span><br><span class="line">class2_x = [x[<span class="number">0</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> iris.target[i] == <span class="number">1</span>]</span><br><span class="line">class2_y = [x[<span class="number">1</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> iris.target[i] == <span class="number">1</span>]</span><br><span class="line">class3_x = [x[<span class="number">0</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> iris.target[i] == <span class="number">2</span>]</span><br><span class="line">class3_y = [x[<span class="number">1</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> iris.target[i] == <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集维度在变化，从单目标分类到三类目标分类。</span></span><br><span class="line"><span class="comment"># 我们将利用矩阵传播和reshape技术一次性计算所有的三类SVM</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">2</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="number">3</span>, <span class="literal">None</span>])</span><br><span class="line">prediction_grid = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">b = tf.Variable(tf.random_normal(shape=[<span class="number">3</span>, batch_size]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算高斯核函数</span></span><br><span class="line"><span class="comment"># 声明高斯核函数</span></span><br><span class="line">gamma = tf.constant(<span class="number">-10.0</span>)</span><br><span class="line">dist = tf.reduce_sum(tf.square(x_data), <span class="number">1</span>)</span><br><span class="line">dist = tf.reshape(dist, [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 实现了(xi-xj)的平方项</span></span><br><span class="line">sq_dists = tf.add(tf.subtract(dist, tf.multiply(<span class="number">2.</span>, tf.matmul(x_data, tf.transpose(x_data)))), tf.transpose(dist))</span><br><span class="line">my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reshape_matmul</span><span class="params">(mat)</span>:</span></span><br><span class="line">    v1 = tf.expand_dims(mat, <span class="number">1</span>)</span><br><span class="line">    v2 = tf.reshape(v1, [<span class="number">3</span>, batch_size, <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span>(tf.matmul(v2, v1))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算对偶问题</span></span><br><span class="line">model_output = tf.matmul(b, my_kernel)</span><br><span class="line"><span class="comment"># 损失函数对偶问题的第一项</span></span><br><span class="line">first_term = tf.reduce_sum(b)</span><br><span class="line">b_vec_cross = tf.matmul(tf.transpose(b), b)</span><br><span class="line">y_target_cross = reshape_matmul(y_target)</span><br><span class="line"><span class="comment"># 损失函数对偶问题的第二项</span></span><br><span class="line">second_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross, y_target_cross)), [<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 第一项加第二项的负数</span></span><br><span class="line">loss = tf.reduce_sum(tf.negative(tf.subtract(first_term, second_term)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建预测函数和准确度函数,先创建一个预测核函数</span></span><br><span class="line">rA = tf.reshape(tf.reduce_sum(tf.square(x_data), <span class="number">1</span>), [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), <span class="number">1</span>), [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># (x_data - prediction_grid)的平方项</span></span><br><span class="line">pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(<span class="number">2.</span>, tf.matmul(x_data, tf.transpose(prediction_grid)))), tf.transpose(rB))</span><br><span class="line">pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测输出</span></span><br><span class="line"><span class="comment"># 实现预测核函数后，我们创建预测函数。</span></span><br><span class="line"><span class="comment"># 与二类不同的是，不再对模型输出进行sign（）运算。</span></span><br><span class="line"><span class="comment"># 因为这里实现的是一对多方法，所以预测值是分类器有最大返回值的类别。</span></span><br><span class="line"><span class="comment"># 使用TensorFlow的内建函数argmax（）来实现该功能</span></span><br><span class="line">prediction_output = tf.matmul(tf.multiply(y_target, b), pred_kernel)</span><br><span class="line"><span class="comment"># 计算每一个预测的平均值</span></span><br><span class="line">prediction = tf.arg_max(prediction_output-tf.expand_dims(tf.reduce_mean(prediction_output, <span class="number">1</span>), <span class="number">1</span>), <span class="number">0</span>)</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(y_target, <span class="number">0</span>)), tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明优化器</span></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line">loss_vec = []</span><br><span class="line">batch_accuracy = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    rand_index = np.random.choice(len(x_vals), size=batch_size)</span><br><span class="line">    rand_x = x_vals[rand_index]</span><br><span class="line">    rand_y = y_vals[:, rand_index]</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    loss_vec.append(temp_loss)</span><br><span class="line"></span><br><span class="line">    acc_temp = sess.run(accuracy, feed_dict=&#123;x_data: rand_x, y_target: rand_y, prediction_grid: rand_x&#125;)</span><br><span class="line">    batch_accuracy.append(acc_temp)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">25</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step # '</span> + str(i+<span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(temp_loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到第一列里的最小值，和最大值</span></span><br><span class="line">x_min, x_max = x_vals[:, <span class="number">0</span>].min() - <span class="number">1</span>, x_vals[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line"><span class="comment"># 得到第二列里的最小值，和最大值</span></span><br><span class="line">y_min, y_max = x_vals[:, <span class="number">1</span>].min() - <span class="number">1</span>, x_vals[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 步长为0.02均分x_min-x_max形成一个向量</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"><span class="comment"># # 将两个xx，yy向量 拼成一个矩阵</span></span><br><span class="line">grid_points = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">grid_predictions = sess.run(prediction, feed_dict=&#123;x_data: rand_x, y_target: rand_y, prediction_grid: grid_points&#125;)</span><br><span class="line">grid_predictions = grid_predictions.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 绘图</span></span><br><span class="line">plt.contourf(xx, yy, grid_predictions, cmap=plt.cm.get_cmap(<span class="string">'Paired'</span>), alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.plot(class1_x, class1_y, <span class="string">'ro'</span>, label=<span class="string">'I setosa'</span>)</span><br><span class="line">plt.plot(class2_x, class2_y, <span class="string">'kx'</span>, label=<span class="string">'I versicolor'</span>)</span><br><span class="line">plt.plot(class3_x, class3_y, <span class="string">'gv'</span>, label=<span class="string">'I virginica'</span>)</span><br><span class="line">plt.title(<span class="string">'Gaussian SVM Result on Iris Data'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Pedal Length'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sepal Width'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.ylim([<span class="number">-0.5</span>, <span class="number">3</span>])</span><br><span class="line">plt.xlim([<span class="number">3.5</span>, <span class="number">8.5</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(batch_accuracy, <span class="string">'k-'</span>, label=<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.title(<span class="string">'Batch Accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Generation'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(loss_vec, <span class="string">'k-'</span>)</span><br><span class="line">plt.title(<span class="string">'Loss per Generation'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Generation'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/img/article/multy_classfy_svm01.png" alt=""><br><img src="/img/article/multy_classfy_svm02.png" alt=""><br><img src="/img/article/multy_classfy_svm03.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之核函数SVM</title>
      <link href="2019/05/06/tensorflow-svm-with-kernel/"/>
      <url>2019/05/06/tensorflow-svm-with-kernel/</url>
      
        <content type="html"><![CDATA[<h1 id="带有核函数的SVM"><a href="#带有核函数的SVM" class="headerlink" title="带有核函数的SVM"></a>带有核函数的SVM</h1><h2 id="带有核函数的对偶问题"><a href="#带有核函数的对偶问题" class="headerlink" title="带有核函数的对偶问题"></a>带有核函数的对偶问题</h2><script type="math/tex; mode=display">\max \sum_{i}^{m} a_{i}-\frac{1}{2} \sum_{j=1}^{m} \sum_{i=1}^{m} a_{i} a_{j} y_{i} y_{j} K(x_{i} x_{j})</script><script type="math/tex; mode=display">st :  \sum_{\text {i}}^{m} a_{i} y_{i}=0</script><script type="math/tex; mode=display">a_{i} \geqslant 0, i=1,2, \ldots, m</script><h2 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h2><p>线性核函数</p><script type="math/tex; mode=display">k\left(x_{i}, x_{j}\right)=x_{i}^{T} x_{j}</script><p>多项式核函数(d&gt;=1为多项式次数)</p><script type="math/tex; mode=display">k\left(x_{i}, x_{j}\right)=\left(x_{i}^{T} x_{j}\right)^{d}</script><p>高斯核函数(<script type="math/tex">\sigma>0</script>为高斯核的带宽)</p><script type="math/tex; mode=display">k\left(x_{i}, x_{j}\right)=\exp \left(-\frac{\left\|x_{i}-x_{j}\right\|^{2}}{2\sigma^{2}}\right)</script><p>拉普拉斯核函数(<script type="math/tex">\sigma>0</script>)</p><script type="math/tex; mode=display">k\left(x_{i}, x_{j}\right)=\exp \left(-\frac{\left\|x_{i}-x_{j}\right\|}{\sigma}\right)</script><p>Sigmoid核函数(tanh为双曲正切函数,<script type="math/tex">\beta>0, \theta<0</script>)</p><script type="math/tex; mode=display">k\left(x_{i}, x_{j}\right)=\tanh \left(\beta x_{i}^{\top} x_{j}+\theta\right)</script><h2 id="code-review"><a href="#code-review" class="headerlink" title="code review"></a>code review</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 生成环形数据</span></span><br><span class="line"><span class="comment"># n_samples：控制样本点总数</span></span><br><span class="line"><span class="comment"># noise：控制属于同一个圈的样本点附加的漂移程度</span></span><br><span class="line"><span class="comment"># factor：控制内外圈的接近程度，越大越接近，上限为1</span></span><br><span class="line">(x_vals, y_vals) = datasets.make_circles(n_samples=<span class="number">500</span>, factor=<span class="number">.5</span>, noise=<span class="number">.1</span>)</span><br><span class="line"></span><br><span class="line">y_vals = np.array([<span class="number">1</span> <span class="keyword">if</span> y == <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span> <span class="keyword">for</span> y <span class="keyword">in</span> y_vals])</span><br><span class="line">class1_x = [x[<span class="number">0</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> y_vals[i] == <span class="number">1</span>]</span><br><span class="line">class1_y = [x[<span class="number">1</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> y_vals[i] == <span class="number">1</span>]</span><br><span class="line">class2_x = [x[<span class="number">0</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> y_vals[i] == <span class="number">-1</span>]</span><br><span class="line">class2_y = [x[<span class="number">1</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> y_vals[i] == <span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明批量大小</span></span><br><span class="line">batch_size = <span class="number">250</span></span><br><span class="line"><span class="comment"># 样本点的数据x为一个二维数据</span></span><br><span class="line">x_data = tf.placeholder(shape=[<span class="literal">None</span>, <span class="number">2</span>], dtype=tf.float32)</span><br><span class="line"><span class="comment"># 样本点的数据y为一个1或者-1的数据</span></span><br><span class="line">y_target = tf.placeholder(shape=[<span class="literal">None</span>, <span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"><span class="comment"># 彩色网格可视化不同的区域</span></span><br><span class="line">prediction_grid = tf.placeholder(shape=[<span class="literal">None</span>, <span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">b = tf.Variable(tf.random.normal(shape=[<span class="number">1</span>, batch_size]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建高斯核函数</span></span><br><span class="line">gamma = tf.constant(<span class="number">-50.0</span>)</span><br><span class="line">dist = tf.reduce_sum(tf.square(x_data), <span class="number">1</span>)</span><br><span class="line">dist = tf.reshape(dist, [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 实现了(xi-xj)的平方项</span></span><br><span class="line">sq_dists = tf.add(tf.subtract(dist, tf.multiply(<span class="number">2.</span>, tf.matmul(x_data, tf.transpose(x_data)))), tf.transpose(dist))</span><br><span class="line">my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理对偶问题</span></span><br><span class="line">model_output = tf.matmul(b, my_kernel)</span><br><span class="line"><span class="comment"># 损失函数对偶问题的第一项</span></span><br><span class="line">first_term = tf.reduce_sum(b)</span><br><span class="line">b_vec_cross = tf.matmul(tf.transpose(b), b)</span><br><span class="line">y_target_cross = tf.matmul(y_target, tf.transpose(y_target))</span><br><span class="line"><span class="comment"># 损失函数对偶问题的第二项</span></span><br><span class="line">second_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross, y_target_cross)))</span><br><span class="line"><span class="comment"># 第一项加第二项的负数</span></span><br><span class="line">loss = tf.negative(tf.subtract(first_term, second_term))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建预测函数和准确度函数,先创建一个预测核函数</span></span><br><span class="line">rA = tf.reshape(tf.reduce_sum(tf.square(x_data), <span class="number">1</span>), [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), <span class="number">1</span>), [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># (x_data - prediction_grid)的平方项</span></span><br><span class="line">pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(<span class="number">2.</span>, tf.matmul(x_data, tf.transpose(prediction_grid)))), tf.transpose(rB))</span><br><span class="line">pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测输出</span></span><br><span class="line">prediction_output = tf.matmul(tf.multiply(tf.transpose(y_target), b), pred_kernel)</span><br><span class="line">prediction = tf.sign(prediction_output - tf.reduce_mean(prediction_output))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction), tf.squeeze(y_target)), tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建优化器函数</span></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始迭代训练</span></span><br><span class="line">loss_vec = []</span><br><span class="line">batch_accuracy = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    rand_index = np.random.choice(len(x_vals), size=batch_size)</span><br><span class="line">    rand_x = x_vals[rand_index]</span><br><span class="line">    rand_y = np.transpose([y_vals[rand_index]])</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    loss_vec.append(temp_loss)</span><br><span class="line"></span><br><span class="line">    acc_temp = sess.run(accuracy, feed_dict=&#123;x_data: rand_x, y_target: rand_y, prediction_grid: rand_x&#125;)</span><br><span class="line">    batch_accuracy.append(acc_temp)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step # '</span> + str(i+<span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(temp_loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到第一列里的最小值，和最大值</span></span><br><span class="line">x_min, x_max = x_vals[:, <span class="number">0</span>].min() - <span class="number">1</span>, x_vals[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line"><span class="comment"># 得到第二列里的最小值，和最大值</span></span><br><span class="line">y_min, y_max = x_vals[:, <span class="number">1</span>].min() - <span class="number">1</span>, x_vals[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 步长为0.02均分x_min-x_max形成一个向量</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.02</span>))</span><br><span class="line"><span class="comment"># # 将两个xx，yy向量 拼成一个矩阵</span></span><br><span class="line">grid_points = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line"></span><br><span class="line">[grid_predicttions] = sess.run(prediction, feed_dict=&#123;x_data: rand_x, y_target: rand_y, prediction_grid: grid_points&#125;)</span><br><span class="line">grid_predicttions = grid_predicttions.reshape(xx.shape)</span><br><span class="line">print(grid_predicttions)</span><br><span class="line"><span class="comment"># # 绘图</span></span><br><span class="line">plt.contourf(xx, yy, grid_predicttions, cmap=plt.cm.get_cmap(<span class="string">'Paired'</span>), alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.plot(class1_x, class1_y, <span class="string">'ro'</span>, label=<span class="string">'Class 1'</span>)</span><br><span class="line">plt.plot(class2_x, class2_y, <span class="string">'kx'</span>, label=<span class="string">'Class -1'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.ylim([<span class="number">-1.5</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.xlim([<span class="number">-1.5</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(batch_accuracy, <span class="string">'k-'</span>, label=<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.title(<span class="string">'Batch Accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Generation'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(loss_vec, <span class="string">'k-'</span>)</span><br><span class="line">plt.plot(<span class="string">'Loss per Generation'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Generation'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ol><li>dist = tf.reduce_sum(tf.square(x_data), 1)， dist = tf.reshape(dist, [-1, 1])是为了求向量每一个值的平方和</li><li>tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(x<em>data, tf.transpose(x_data)))), tf.transpose(dist))是为了求$$||(x</em>{i}-x_{j})||^2$$</li><li>将高斯核替换成其他的核即可实现其他的核SVM</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之支持向量回归</title>
      <link href="2019/05/05/tensorflow-svr/"/>
      <url>2019/05/05/tensorflow-svr/</url>
      
        <content type="html"><![CDATA[<h1 id="SVR"><a href="#SVR" class="headerlink" title="SVR"></a>SVR</h1><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><script type="math/tex; mode=display">\max \left(0,\left|y_{i}-\left(A x_{i}+b\right)\right|-\varepsilon\right)</script><h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_vals = np.array([x[<span class="number">3</span>] <span class="keyword">for</span> x <span class="keyword">in</span> iris.data])</span><br><span class="line">y_vals = np.array([y[<span class="number">0</span>] <span class="keyword">for</span> y <span class="keyword">in</span> iris.data])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line">train_indices = np.random.choice(len(x_vals), round(len(x_vals) * <span class="number">0.8</span>), replace=<span class="literal">False</span>)</span><br><span class="line">test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))</span><br><span class="line"></span><br><span class="line">x_vals_train = x_vals[train_indices]</span><br><span class="line">x_vals_test = x_vals[test_indices]</span><br><span class="line">y_vals_train = y_vals[train_indices]</span><br><span class="line">y_vals_test = y_vals[test_indices]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置批量大小</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置斜率A，和截距b的大小</span></span><br><span class="line">A = tf.Variable(tf.random.normal(shape=[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">b = tf.Variable(tf.random.normal(shape=[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">model_output = tf.add(tf.matmul(x_data, A), b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明损失函数、</span></span><br><span class="line">epsilon = tf.constant([<span class="number">0.5</span>])</span><br><span class="line">loss = tf.reduce_mean(tf.maximum(<span class="number">0.</span>, tf.subtract(tf.abs(tf.subtract(model_output, y_target)), epsilon)))</span><br><span class="line"></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.075</span>)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line">train_loss = []</span><br><span class="line">test_loss = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">    rand_index = np.random.choice(len(x_vals_train), size=batch_size)</span><br><span class="line">    rand_x = np.transpose([x_vals_train[rand_index]])</span><br><span class="line">    rand_y = np.transpose([y_vals_train[rand_index]])</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line"></span><br><span class="line">    temp_train_loss = sess.run(loss, feed_dict=&#123;x_data: np.transpose([x_vals_train]), y_target: np.transpose([y_vals_train])&#125;)</span><br><span class="line">    train_loss.append(temp_train_loss)</span><br><span class="line">    temp_test_loss = sess.run(loss, feed_dict=&#123;x_data: np.transpose([x_vals_test]), y_target: np.transpose([y_vals_test])&#125;)</span><br><span class="line">    test_loss.append(temp_test_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'------------'</span>)</span><br><span class="line">        print(<span class="string">'Generation : '</span> + str(i))</span><br><span class="line">        print(<span class="string">'A = '</span> + str(sess.run(A)) + <span class="string">' b = '</span> + str(sess.run(b)))</span><br><span class="line">        print(<span class="string">'Train Loss = '</span> + str(temp_train_loss))</span><br><span class="line">        print(<span class="string">'Test Loss = '</span> + str(temp_test_loss))</span><br><span class="line"></span><br><span class="line">[[slope]] = sess.run(A)</span><br><span class="line">[[y_intercept]] = sess.run(b)</span><br><span class="line">[width] = sess.run(epsilon)</span><br><span class="line"></span><br><span class="line">best_fit = []</span><br><span class="line">best_fit_upper = []</span><br><span class="line">best_fit_lower = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x_vals:</span><br><span class="line">    best_fit.append(slope * i + y_intercept)</span><br><span class="line">    best_fit_upper.append(slope * i + y_intercept + width)</span><br><span class="line">    best_fit_lower.append(slope * i + y_intercept - width)</span><br><span class="line"></span><br><span class="line">plt.plot(x_vals, y_vals, <span class="string">'o'</span>, label=<span class="string">'Data Points'</span>)</span><br><span class="line">plt.plot(x_vals, best_fit, <span class="string">'r-'</span>, label=<span class="string">'SVM Regression Line'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.plot(x_vals, best_fit_upper, <span class="string">'r--'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(x_vals, best_fit_lower, <span class="string">'r--'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">10</span>])</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.title(<span class="string">'Sepal Length vs Pedal Width'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Pedal Width'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sepal Length'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(train_loss, <span class="string">'k-'</span>, label=<span class="string">'Train Set Loss'</span>)</span><br><span class="line">plt.plot(test_loss, <span class="string">'r--'</span>, label=<span class="string">'Test Set Loss'</span>)</span><br><span class="line">plt.title(<span class="string">'L2 Loss per Generation'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Generation'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'L2 Loss'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="something-need-to-say"><a href="#something-need-to-say" class="headerlink" title="something need to say"></a>something need to say</h2><p>支持向量回归中，SVR需要我们定义一个常量ϵ&gt;0,对于某一个点(xi,yi)，如果|yi−w∙ϕ(xi)−b|≤ϵ，则完全没有损失;如果|yi−w∙ϕ(xi)−b|&gt;ϵ,则对应的损失为|yi−w∙ϕ(xi)−b|−ϵ<br>这个均方差损失函数不同，如果是均方差，那么只要yi−w∙ϕ(xi)−b≠0，那么就会有损失。</p><p><img src="/img/article/svr01.png" alt=""><br><img src="/img/article/svr02.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之软支持向量机</title>
      <link href="2019/05/05/tensorflow-svm01/"/>
      <url>2019/05/05/tensorflow-svm01/</url>
      
        <content type="html"><![CDATA[<h1 id="软支持向量机原理"><a href="#软支持向量机原理" class="headerlink" title="软支持向量机原理"></a>软支持向量机原理</h1><h2 id="soft-margin-损失函数"><a href="#soft-margin-损失函数" class="headerlink" title="soft margin 损失函数"></a>soft margin 损失函数</h2><script type="math/tex; mode=display">\frac{1}{n} \sum_{i=1}^{n} \max \left(0,1-y_{i}\left(A x_{i}-b\right)\right)+a\|A\|^{2}</script><p>即当样本点在间隔外的时候，损失函数为0，当样本点在间隔内的时候，损失函数为<script type="math/tex">1-y_{i}*output</script></p><script type="math/tex; mode=display">output=A x_{i}-b</script><h2 id="软支持向量机代码"><a href="#软支持向量机代码" class="headerlink" title="软支持向量机代码"></a>软支持向量机代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">x_vals = np.array([[x[<span class="number">0</span>], x[<span class="number">3</span>]] <span class="keyword">for</span> x <span class="keyword">in</span> iris.data])</span><br><span class="line">y_vals = np.array([<span class="number">1</span> <span class="keyword">if</span> y == <span class="number">0</span> <span class="keyword">else</span> <span class="number">-1</span> <span class="keyword">for</span> y <span class="keyword">in</span> iris.target])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line">train_indices = np.random.choice(len(x_vals), round(len(x_vals) * <span class="number">0.8</span>), replace=<span class="literal">False</span>)</span><br><span class="line">test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))</span><br><span class="line"></span><br><span class="line">x_vals_train = x_vals[train_indices]</span><br><span class="line">x_vals_test = x_vals[test_indices]</span><br><span class="line">y_vals_train = y_vals[train_indices]</span><br><span class="line">y_vals_test = y_vals[test_indices]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置批量大小</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">2</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置斜率A，和截距b的大小</span></span><br><span class="line">A = tf.Variable(tf.random.normal(shape=[<span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">b = tf.Variable(tf.random.normal(shape=[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">model_output = tf.subtract(tf.matmul(x_data, A), b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明最大间隔损失函数</span></span><br><span class="line">l2_norm = tf.reduce_mean(tf.square(A))</span><br><span class="line"><span class="comment"># 设置一个aplha值</span></span><br><span class="line">alpha = tf.constant([<span class="number">0.1</span>])</span><br><span class="line"><span class="comment"># 软支持向量机分类误差，即在间隔内部有误差，为1-（输出值*实际值），在间隔外部为0，</span></span><br><span class="line">classification_term = tf.reduce_mean(tf.maximum(<span class="number">0.</span>, tf.subtract(<span class="number">1.</span>, tf.multiply(model_output, y_target))))</span><br><span class="line">loss = tf.add(classification_term, tf.multiply(alpha, l2_norm))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明预测函数和准确度函数</span></span><br><span class="line">prediction = tf.sign(model_output)</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target), tf.float32))</span><br><span class="line"></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始遍历迭代训练模型</span></span><br><span class="line">loss_vec = []</span><br><span class="line">train_accuracy = []</span><br><span class="line">test_accuracy = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    rand_index = np.random.choice(len(x_vals_train), size=batch_size)</span><br><span class="line">    rand_x = x_vals_train[rand_index]</span><br><span class="line">    rand_y = np.transpose([y_vals_train[rand_index]])</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    loss_vec.append(temp_loss)</span><br><span class="line"></span><br><span class="line">    train_acc_temp = sess.run(accuracy, feed_dict=&#123;x_data: x_vals_train, y_target: np.transpose([y_vals_train])&#125;)</span><br><span class="line">    train_accuracy.append(train_acc_temp)</span><br><span class="line"></span><br><span class="line">    test_acc_temp = sess.run(accuracy, feed_dict=&#123;x_data: x_vals_test, y_target: np.transpose([y_vals_test])&#125;)</span><br><span class="line">    test_accuracy.append(test_acc_temp)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step # '</span> + str(i+<span class="number">1</span>) + <span class="string">' A = '</span> + str(sess.run(A)) + <span class="string">' b = '</span> + str(sess.run(b)))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(temp_loss))</span><br><span class="line"></span><br><span class="line">[[a1], [a2]] = sess.run(A)</span><br><span class="line">[[b]] = sess.run(b)</span><br><span class="line">slope = -a2/a1</span><br><span class="line">y_intercept = b/a1</span><br><span class="line">x1_vals = [d[<span class="number">1</span>] <span class="keyword">for</span> d <span class="keyword">in</span> x_vals]</span><br><span class="line">best_fit = []</span><br><span class="line"><span class="comment"># 按照最优A，b得出直线的点所在的位置,best_fit为第二维的数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x1_vals:</span><br><span class="line">    best_fit.append(slope * i + y_intercept)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求出间隔左右两边的点的集合</span></span><br><span class="line">setosa_x = [d[<span class="number">1</span>] <span class="keyword">for</span> i, d <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> y_vals[i] == <span class="number">1</span>]</span><br><span class="line">setosa_y = [d[<span class="number">0</span>] <span class="keyword">for</span> i, d <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> y_vals[i] == <span class="number">1</span>]</span><br><span class="line">not_setosa_x = [d[<span class="number">1</span>] <span class="keyword">for</span> i, d <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> y_vals[i] == <span class="number">-1</span>]</span><br><span class="line">not_setosa_y = [d[<span class="number">0</span>] <span class="keyword">for</span> i, d <span class="keyword">in</span> enumerate(x_vals) <span class="keyword">if</span> y_vals[i] == <span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(setosa_x, setosa_y, <span class="string">'o'</span>, label=<span class="string">'I. setosa'</span>)</span><br><span class="line">plt.plot(not_setosa_x, not_setosa_y, <span class="string">'x'</span>, label=<span class="string">'Non-setosa'</span>)</span><br><span class="line">plt.plot(x1_vals, best_fit, <span class="string">'r-'</span>, label=<span class="string">'Linear Separator'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">10</span>])</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.title(<span class="string">'Sepal Length vs Pedal Width'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Pedal Width'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sepal Length'</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(train_accuracy)</span><br><span class="line">print(test_accuracy)</span><br><span class="line">plt.plot(train_accuracy, <span class="string">'k-'</span>, label=<span class="string">'Training Accuracy'</span>)</span><br><span class="line">plt.plot(test_accuracy, <span class="string">'r--'</span>, label=<span class="string">'Test Accuracy'</span>)</span><br><span class="line">plt.title(<span class="string">'Train and Test Accuracies'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Generation'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(loss_vec, <span class="string">'k-'</span>)</span><br><span class="line">plt.title(<span class="string">'Loss per Generation'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Generation'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ol><li>tf.equal(prediction, y_target) 是得到一个true，false的列表</li><li>tf.cast()是将true，false转换成1或者0</li><li>我们每次得到的结果都不尽相同，原因是因为初始化A，b的结果不同，而且每次训练集和测试集的随机分割，每次批量大小不同</li></ol><p><img src="/img/article/soft-margin-svm01.png" alt=""><br><img src="/img/article/soft-margin-svm02.png" alt=""><br><img src="/img/article/soft-margin-svm03.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之lasso回归和岭回归</title>
      <link href="2019/04/30/tensorflow-lasso-and-lin/"/>
      <url>2019/04/30/tensorflow-lasso-and-lin/</url>
      
        <content type="html"><![CDATA[<h1 id="lasso回归和岭回归"><a href="#lasso回归和岭回归" class="headerlink" title="lasso回归和岭回归"></a>lasso回归和岭回归</h1><p>岭回归与Lasso回归的出现是为了解决线性回归出现的过拟合以及在通过正规方程方法求解θ的过程中出现的x转置乘以x不可逆这两类问题的，这两种回归均通过在损失函数中引入正则化项来达到目的，具体三者的损失函数对比见下图：<br>线性回归损失函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p>岭回归损失函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}</script><p>lasso回归损失函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n}\left|\theta_{j}\right|</script><p>其中λ称为正则化参数，如果λ选取过大，会把所有参数θ均最小化，造成欠拟合，<br>如果λ选取过小，会导致对过拟合问题解决不当，因此λ的选取是一个技术活。<br>岭回归与Lasso回归最大的区别在于岭回归引入的是L2范数惩罚项，Lasso回归引入的是L1范数惩罚项，Lasso回归能够使得损失函数中的许多θ均变成0，这点要优于岭回归，因为岭回归是要所有的θ均存在的，这样计算量Lasso回归将远远小于岭回归。 </p><h2 id="lasso"><a href="#lasso" class="headerlink" title="lasso"></a>lasso</h2><p>增加损失函数，其为改良过的连续阶跃函数，lasso回归的截止点设为0.9。<br>Loss = L1_Loss + heavyside_step</p><script type="math/tex; mode=display">heavyside_step = \frac{1}{1+e^{-100(A-0.9)}}</script><p>通过这个函数来限制A的值<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">x_vals = np.array([x[<span class="number">3</span>] <span class="keyword">for</span> x <span class="keyword">in</span> iris.data])</span><br><span class="line">y_vals = np.array([y[<span class="number">0</span>] <span class="keyword">for</span> y <span class="keyword">in</span> iris.data])</span><br><span class="line"><span class="comment"># 声明学习率，批量大小，占位符，和模型变量</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">A = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">b = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"><span class="comment"># 函数输出值</span></span><br><span class="line">model_output = tf.add(tf.matmul(x_data, A), b)</span><br><span class="line"></span><br><span class="line"><span class="comment">##加入L1正则化的损失函数</span></span><br><span class="line">lasso_param = tf.constant(<span class="number">0.9</span>)</span><br><span class="line">heavyside_step = tf.truediv(<span class="number">1.</span>, tf.add(<span class="number">1.</span>, tf.exp(tf.multiply(<span class="number">-100.</span>, tf.subtract(A, lasso_param)))))</span><br><span class="line">regularization_param = tf.multiply(heavyside_step, <span class="number">99.</span>)</span><br><span class="line">loss = tf.add(tf.reduce_mean(tf.square(y_target - model_output)), regularization_param)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br><span class="line"></span><br><span class="line">loss_vec = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1500</span>):</span><br><span class="line">    rand_index = np.random.choice(len(x_vals), size=batch_size)</span><br><span class="line">    rand_x = np.transpose([x_vals[rand_index]])</span><br><span class="line">    rand_y = np.transpose([y_vals[rand_index]])</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    loss_vec.append(temp_loss[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">300</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step # '</span> + str(i + <span class="number">1</span>) + <span class="string">' A = '</span> + str(sess.run(A)) +</span><br><span class="line">              <span class="string">' b = '</span> + str(sess.run(b)))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(temp_loss))</span><br></pre></td></tr></table></figure></p><h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><p>对于岭回归在上述代码中修改loss项即可<br>tf.expand_dims是在后面的式子中加了一个0维度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 岭回归</span></span><br><span class="line">ridge_param = tf.constant(<span class="number">1.</span>)</span><br><span class="line">ridge_loss = tf.reduce_mean(tf.square(A))</span><br><span class="line">loss = tf.expand_dims(tf.add(tf.reduce_mean(tf.square(y_target - model_output)), tf.multiply(ridge_param, ridge_loss)), <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之实现线性回归</title>
      <link href="2019/04/30/tensorflow-Linear-R2/"/>
      <url>2019/04/30/tensorflow-Linear-R2/</url>
      
        <content type="html"><![CDATA[<h1 id="实现线性回归"><a href="#实现线性回归" class="headerlink" title="实现线性回归"></a>实现线性回归</h1><h2 id="LinearRegression原理"><a href="#LinearRegression原理" class="headerlink" title="LinearRegression原理"></a>LinearRegression原理</h2><p>采用最小二乘法求所有样本点的的<script type="math/tex">y_{i}</script>到Y的总距离作为损失函数，然后求其最小值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">x_vals = np.array([x[<span class="number">3</span>] <span class="keyword">for</span> x <span class="keyword">in</span> iris.data])</span><br><span class="line">y_vals = np.array([y[<span class="number">0</span>] <span class="keyword">for</span> y <span class="keyword">in</span> iris.data])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明学习率，批量大小，占位符，和模型变量</span></span><br><span class="line">learning_rate = <span class="number">0.05</span></span><br><span class="line">batch_size = <span class="number">25</span></span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">A = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">b = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数输出值</span></span><br><span class="line">model_output = tf.add(tf.matmul(x_data, A), b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明L2损失函数, 定义优化器，然后进行优化</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y_target - model_output))</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历迭代计算</span></span><br><span class="line">loss_vec = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    rand_index = np.random.choice(len(x_vals), size=batch_size)</span><br><span class="line">    rand_x = np.transpose([x_vals[rand_index]])</span><br><span class="line">    rand_y = np.transpose([y_vals[rand_index]])</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    loss_vec.append(temp_loss)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">25</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step # '</span> + str(i+<span class="number">1</span>) + <span class="string">' A = '</span> + str(sess.run(A)) +</span><br><span class="line">              <span class="string">' b = '</span> + str(sess.run(b)))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(temp_loss))</span><br><span class="line"></span><br><span class="line">[slope] = sess.run(A)</span><br><span class="line">[y_intecept] = sess.run(b)</span><br><span class="line">best_fit = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x_vals:</span><br><span class="line">    best_fit.append(slope * i + y_intecept)</span><br><span class="line"></span><br><span class="line">plt.plot(x_vals, y_vals, <span class="string">'o'</span>, label=<span class="string">'Data Points'</span>)</span><br><span class="line">plt.plot(x_vals, best_fit, <span class="string">'r-'</span>, label=<span class="string">'Best fit line'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.title(<span class="string">'Sepal Length vs Pedal Width'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Pedal Width'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sepal Length'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(loss_vec, <span class="string">'k-'</span>)</span><br><span class="line">plt.title(<span class="string">'L2 Loss per Generation'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Generation'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'L2 Loss'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/img/article/Linear_03.png" alt=""><br><img src="/img/article/Linear_04.png" alt=""></p><h1 id="实现戴明回归"><a href="#实现戴明回归" class="headerlink" title="实现戴明回归"></a>实现戴明回归</h1><h2 id="DemingRegression原理"><a href="#DemingRegression原理" class="headerlink" title="DemingRegression原理"></a>DemingRegression原理</h2><p>给定直线为<script type="math/tex">y=mx+b</script>, 点<script type="math/tex">(x_{0},y_{0})</script>,求两者间的距离公式为</p><script type="math/tex; mode=display">d=\frac{\left|y_{0}-\left(m x_{0}+b\right)\right|}{\sqrt{m^{2}+1}}</script><p>所有的d总距离作为损失函数，并求其最小值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 函数输出值</span></span><br><span class="line">model_output = tf.add(tf.matmul(x_data, A), b)</span><br><span class="line"><span class="comment"># 计算点到直线的距离总和为损失函数, 定义优化器，然后进行优化</span></span><br><span class="line"><span class="comment"># 计算|y-(m*x + b)|</span></span><br><span class="line">deming_numerator = tf.abs(tf.subtract(y_target, tf.add(tf.matmul(x_data, A), b)))</span><br><span class="line"><span class="comment"># 计算(m^2 + 1)^ 1/2，+1是为了防止斜率m为0的情况</span></span><br><span class="line">deming_denominator = tf.sqrt(tf.add(tf.square(A), <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 上式两者相除即为点到直线的距离</span></span><br><span class="line">loss = tf.reduce_mean(tf.truediv(deming_numerator, deming_denominator))</span><br></pre></td></tr></table></figure><p><img src="/img/article/Linear_05.png" alt=""><br><img src="/img/article/Linear_06.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之求逆矩阵以及矩阵分解实现线性回归</title>
      <link href="2019/04/29/tensorflow-Linear-R/"/>
      <url>2019/04/29/tensorflow-Linear-R/</url>
      
        <content type="html"><![CDATA[<h1 id="直接用公式来求解"><a href="#直接用公式来求解" class="headerlink" title="直接用公式来求解"></a>直接用公式来求解</h1><h2 id="公式原理"><a href="#公式原理" class="headerlink" title="公式原理"></a>公式原理</h2><p>线性回归<script type="math/tex">Y = w^{T}x + b</script><br>直接用公式可以解出<br>A = [X, 1],转换为Y = W<em>A = [w, b]</em>[X, 1]</p><script type="math/tex; mode=display">w=\left(A^{T} A\right)^{-1} A^{T}y</script><p>这里求出的w就是[w;b]形式<br>然后就可以得出直线的斜率和截距</p><h2 id="求逆矩阵代码"><a href="#求逆矩阵代码" class="headerlink" title="求逆矩阵代码"></a>求逆矩阵代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集合</span></span><br><span class="line">x_vals = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y_vals = x_vals + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># X列向量</span></span><br><span class="line">x_vals_column = np.transpose(np.matrix(x_vals))</span><br><span class="line"><span class="comment"># 列向量，值全为1</span></span><br><span class="line">ones_column = np.transpose(np.matrix(np.repeat(<span class="number">1</span>, <span class="number">100</span>)))</span><br><span class="line"><span class="comment"># A为[X:1], 扩展向量</span></span><br><span class="line">A = np.column_stack((x_vals_column, ones_column))</span><br><span class="line">b = np.transpose(np.matrix(y_vals))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将A向量和b转换为张量</span></span><br><span class="line">A_tensor = tf.constant(A)</span><br><span class="line">b_tensor = tf.constant(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tA_A为 A的转置乘以A</span></span><br><span class="line">tA_A = tf.matmul(tf.transpose(A_tensor), A_tensor)</span><br><span class="line"><span class="comment"># 求逆运算</span></span><br><span class="line">tA_A_inv = tf.matrix_inverse(tA_A)</span><br><span class="line"><span class="comment"># 乘以转置</span></span><br><span class="line">product = tf.matmul(tA_A_inv, tf.transpose(A_tensor))</span><br><span class="line"><span class="comment"># 再乘以b</span></span><br><span class="line">solution = tf.matmul(product, b_tensor)</span><br><span class="line">solution_eval = sess.run(solution)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从解中抽取系数,斜率和y截距y-intercept, 即为w和b的值</span></span><br><span class="line">slope = solution_eval[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">y_intercept = solution_eval[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">print(<span class="string">'slope: '</span> + str(slope))</span><br><span class="line">print(<span class="string">'y_intercept'</span> + str(y_intercept))</span><br><span class="line"></span><br><span class="line">best_fit = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x_vals:</span><br><span class="line">    best_fit.append(slope * i + y_intercept)</span><br><span class="line">plt.plot(x_vals, y_vals, <span class="string">'o'</span>, label=<span class="string">'Data'</span>)</span><br><span class="line">plt.plot(x_vals, best_fit, <span class="string">'r--'</span>, label=<span class="string">'Best fit line'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/img/article/linearRegess.png" alt=""></p><h1 id="利用tensorflow实现矩阵分解"><a href="#利用tensorflow实现矩阵分解" class="headerlink" title="利用tensorflow实现矩阵分解"></a>利用tensorflow实现矩阵分解</h1><h2 id="矩阵分解原理"><a href="#矩阵分解原理" class="headerlink" title="矩阵分解原理"></a>矩阵分解原理</h2><ul><li>这里将用TensorFlow实现矩阵分解，对于一般的线性回归算法，求解Ax=b,则<script type="math/tex">x=(A^{T}A)^{-1}A^{T}b</script>,然而该方法在大部分情况下是低效率的</li><li>特别是当矩阵非常大时效率更低。另一种方式则是通过矩阵分解来提高效率。这里采用TensorFlow内建的Cholesky矩阵分解法实现线性回归</li></ul><ol><li>Cholesky可以将一个矩阵分解为上三角矩阵和下三角矩阵，即<script type="math/tex">A = LL^{T}</script></li><li>接下来通过tf.cholesky函数计算L,由于该函数返回的是矩阵分解的下三角矩阵，上三角矩阵为该矩阵的转置，L = tf.cholesky(tA_A)</li><li><script type="math/tex; mode=display">Ax = b -> A^{T}Ax = A^{T}b -> LL^{T}x = A^{T}b</script></li><li>tf.matrix_solve()函数返回Ax=b的解。<script type="math/tex">Ly = A^{T}b</script> ，求出y</li><li><script type="math/tex">L^{T}x = y</script>，求出x</li></ol><h2 id="矩阵分解代码"><a href="#矩阵分解代码" class="headerlink" title="矩阵分解代码"></a>矩阵分解代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.Session()</span><br><span class="line">x_vals = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y_vals = x_vals + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">x_vals_column = np.transpose(np.matrix(x_vals))</span><br><span class="line">ones_column = np.transpose(np.matrix(np.repeat(<span class="number">1</span>, <span class="number">100</span>)))</span><br><span class="line">A = np.column_stack((x_vals_column, ones_column))</span><br><span class="line">b = np.transpose(np.matrix(y_vals))</span><br><span class="line"><span class="comment"># 将A向量和b转换为张量</span></span><br><span class="line">A_tensor = tf.constant(A)</span><br><span class="line">b_tensor = tf.constant(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵分解</span></span><br><span class="line"><span class="comment"># 计算A&#123;T&#125;*A</span></span><br><span class="line">tA_A = tf.matmul(tf.transpose(A_tensor), A_tensor)</span><br><span class="line">L = tf.cholesky(tA_A)</span><br><span class="line">tA_b = tf.matmul(tf.transpose(A_tensor), b)</span><br><span class="line">sol1 = tf.matrix_solve(L, tA_b)</span><br><span class="line">sol2 = tf.matrix_solve(tf.transpose(L), sol1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抽取系数</span></span><br><span class="line">solution_eval = sess.run(sol2)</span><br><span class="line">slope = solution_eval[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">y_intercept = solution_eval[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">print(<span class="string">'slpoe : '</span> + str(slope))</span><br><span class="line">print(<span class="string">'y_intercept : '</span> + str(y_intercept))</span><br><span class="line"></span><br><span class="line">best_fit = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x_vals:</span><br><span class="line">    best_fit.append(slope * i + y_intercept)</span><br><span class="line">plt.plot(x_vals, y_vals, <span class="string">'0'</span>, label=<span class="string">'Data'</span>)</span><br><span class="line">plt.plot(x_vals, best_fit, <span class="string">'r-'</span>, label=<span class="string">'Best fit line'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper, left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/img/article/LinearR2.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之模型评估</title>
      <link href="2019/04/29/tensorflow-model-estimate/"/>
      <url>2019/04/29/tensorflow-model-estimate/</url>
      
        <content type="html"><![CDATA[<h1 id="回归算法模型评估"><a href="#回归算法模型评估" class="headerlink" title="回归算法模型评估"></a>回归算法模型评估</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据</span></span><br><span class="line">x_vals = np.random.normal(<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">100</span>)</span><br><span class="line">y_vals = np.repeat(<span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">batch_size = <span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集合下标--&gt;随机选取【0-len（x_vals)】中0.8倍数量的下标</span></span><br><span class="line">train_indices = np.random.choice(len(x_vals), round(len(x_vals) * <span class="number">0.8</span>), replace=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 测试集合下标</span></span><br><span class="line">test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))</span><br><span class="line"></span><br><span class="line">x_vals_train = x_vals[train_indices]</span><br><span class="line">x_vals_test = x_vals[test_indices]</span><br><span class="line">y_vals_train = y_vals[train_indices]</span><br><span class="line">y_vals_test = y_vals[test_indices]</span><br><span class="line"><span class="comment"># 随机选取变量A，正太分布中的一个值</span></span><br><span class="line">A = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明算法模型,损失函数和优化算法器</span></span><br><span class="line">my_output = tf.matmul(x_data, A)</span><br><span class="line"><span class="comment"># tf.reduce_mean计算batch_size中的平均值</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(my_output - y_target))</span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess.run(init)</span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.02</span>)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    rand_index = np.random.choice(len(x_vals_train), size=batch_size)</span><br><span class="line">    <span class="comment"># transpose可以将接收的向量进行转置，需要的是一个列向量[None, 1]，传入的是一个行向量</span></span><br><span class="line">    rand_x = np.transpose([x_vals_train[rand_index]])</span><br><span class="line">    rand_y = np.transpose([y_vals_train[rand_index]])</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">25</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step # '</span> + str(i+<span class="number">1</span>) + <span class="string">' A = '</span> + str(sess.run(A)))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 对模型进行评估</span></span><br><span class="line">mse_test = sess.run(loss, feed_dict=&#123;x_data: np.transpose([x_vals_test]), y_target: np.transpose([y_vals_test])&#125;)</span><br><span class="line">mse_train = sess.run(loss, feed_dict=&#123;x_data: np.transpose([x_vals_train]), y_target: np.transpose([y_vals_train])&#125;)</span><br><span class="line">print(<span class="string">'MSE on test : '</span> + str(np.round(mse_test, <span class="number">2</span>)))</span><br><span class="line">print(<span class="string">'MSE on train : '</span> + str(np.round(mse_train, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure><p>将原数据集一分为2，百分之八十为训练集，百分之二十为测试集<br>训练集中训练好之后的模型放在测试集进行测试<br>算出最小均方误差的平均值 tf.reduce_mean(tf.square(my_output - y_target))</p><h1 id="分类算法模型评估"><a href="#分类算法模型评估" class="headerlink" title="分类算法模型评估"></a>分类算法模型评估</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">batch_size = <span class="number">25</span></span><br><span class="line">x_vals = np.concatenate((np.random.normal(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">50</span>), np.random.normal(<span class="number">2</span>, <span class="number">1</span>, <span class="number">50</span>)))</span><br><span class="line">y_vals = np.concatenate((np.repeat(<span class="number">0.</span>, <span class="number">50</span>), np.repeat(<span class="number">1.</span>, <span class="number">50</span>)))</span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="number">1</span>, <span class="literal">None</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="number">1</span>, <span class="literal">None</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集测试集</span></span><br><span class="line">train_indices = np.random.choice(len(x_vals), round(len(x_vals) * <span class="number">0.8</span>), replace=<span class="literal">False</span>)</span><br><span class="line">test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))</span><br><span class="line"></span><br><span class="line">x_vals_train = x_vals[train_indices]</span><br><span class="line">y_vals_train = y_vals[train_indices]</span><br><span class="line">x_vals_test = x_vals[test_indices]</span><br><span class="line">y_vals_test = y_vals[test_indices]</span><br><span class="line"></span><br><span class="line">A = tf.Variable(tf.random_normal(mean=<span class="number">10</span>, shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化变量,增加模型和损失函数 以及 优化器</span></span><br><span class="line">my_output = tf.add(x_data, A)</span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess.run(init)</span><br><span class="line"><span class="comment"># tf.reduce_mean计算batch_size中的平均值</span></span><br><span class="line">xentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=my_output, labels=y_target))</span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.05</span>)</span><br><span class="line">train_step = my_opt.minimize(xentropy)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1800</span>):</span><br><span class="line">    rand_index = np.random.choice(len(x_vals_train), size=batch_size)</span><br><span class="line">    rand_x = [x_vals_train[rand_index]]</span><br><span class="line">    rand_y = [y_vals_train[rand_index]]</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step # '</span> + str(i+<span class="number">1</span>) + <span class="string">' A = '</span> + str(sess.run(A)))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(sess.run(xentropy, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 在测试集和训练集中评估训练模型</span></span><br><span class="line"><span class="comment"># tf.squeeze()可以删除维度为1的维度，用squeeze（）函数封装预测操作，使得预测值和目标值有相同的维度。</span></span><br><span class="line">y_prediction = tf.squeeze(tf.round(tf.nn.sigmoid(tf.add(x_data, A))))</span><br><span class="line"><span class="comment"># 返回真值</span></span><br><span class="line">correct_prediction = tf.equal(y_prediction, y_target)</span><br><span class="line"><span class="comment"># 将真假值转换为数字0和1，再计算平均</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">acc_value_test = sess.run(accuracy, feed_dict=&#123;x_data: [x_vals_test], y_target: [y_vals_test]&#125;)</span><br><span class="line">acc_value_train = sess.run(accuracy, feed_dict=&#123;x_data: [x_vals_train], y_target: [y_vals_train]&#125;)</span><br><span class="line">print(<span class="string">'Accuracy on train set '</span> + str(acc_value_train))</span><br><span class="line">print(<span class="string">'Accuracy on test set '</span> + str(acc_value_test))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">A_result = sess.run(A)</span><br><span class="line">bins = np.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">50</span>)</span><br><span class="line">plt.hist(x_vals[<span class="number">0</span>:<span class="number">50</span>], bins, alpha=<span class="number">0.5</span>, label=<span class="string">'N(-1,1)'</span>, color=<span class="string">'blue'</span>)</span><br><span class="line">plt.hist(x_vals[<span class="number">50</span>:<span class="number">100</span>], bins[<span class="number">0</span>:<span class="number">50</span>], alpha=<span class="number">0.5</span>, label=<span class="string">'N(2,1)'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">plt.plot((A_result, A_result), (<span class="number">0</span>, <span class="number">8</span>), <span class="string">'k--'</span>, linewidth=<span class="number">3</span>, label=<span class="string">'A = '</span> + str(np.round(A_result, <span class="number">2</span>)))</span><br><span class="line">plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.title(<span class="string">'Binary Classifier, Accuracy='</span> + str(np.round(acc_value_test, <span class="number">2</span>)))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>将原数据集一分为2，百分之八十为训练集，百分之二十为测试集<br>训练集中训练好之后的模型放在测试集进行测试<br>分类算法中是计算分类的正确率<br>损失函数为sigmoid的交叉熵</p><p><img src="/img/article/guji.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之随机训练和批量训练</title>
      <link href="2019/04/27/tensorflow-simple-and-batch-train/"/>
      <url>2019/04/27/tensorflow-simple-and-batch-train/</url>
      
        <content type="html"><![CDATA[<h1 id="随机训练与批量训练"><a href="#随机训练与批量训练" class="headerlink" title="随机训练与批量训练"></a>随机训练与批量训练</h1><p><img src="/img/article/suijixunlian.png" alt=""></p><p>随机训练损失更不规则，批量训练更平滑</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置批量大小</span></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">x_vals = np.random.normal(<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">100</span>)</span><br><span class="line">y_vals = np.repeat(<span class="number">10.</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">A = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标输出函数</span></span><br><span class="line">my_output = tf.matmul(x_data, A)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化A</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每个数据点的L2损失的平均值</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(my_output - y_target))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义梯度下降优化器</span></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.02</span>)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量训练</span></span><br><span class="line">loss_batch = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 定义的批大小为20个，即生成一个20个x_val的行向量</span></span><br><span class="line">    <span class="comment"># 生成20个y_val的行向量</span></span><br><span class="line">    rand_index = np.random.choice(<span class="number">100</span>, size=batch_size)</span><br><span class="line">    rand_x = np.transpose([x_vals[rand_index]])</span><br><span class="line">    rand_y = np.transpose([y_vals[rand_index]])</span><br><span class="line"></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step1 #'</span> + str(i+<span class="number">1</span>) + <span class="string">'A = '</span> + str(sess.run(A)))</span><br><span class="line">        temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">        print(<span class="string">'Loss1 = '</span> + str(temp_loss))</span><br><span class="line">        loss_batch.append(temp_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机训练</span></span><br><span class="line"><span class="comment"># 重新选定A值</span></span><br><span class="line">A = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line">loss_stochastic = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    rand_index = np.random.choice(<span class="number">100</span>)</span><br><span class="line">    rand_x = [[x_vals[rand_index]]]</span><br><span class="line">    rand_y = [[y_vals[rand_index]]]</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step2 #'</span> + str(i + <span class="number">1</span>) + <span class="string">'A = '</span> + str(sess.run(A)))</span><br><span class="line">        temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">        print(<span class="string">'Loss2 = '</span> + str(temp_loss))</span><br><span class="line">        loss_stochastic.append(temp_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.plot(range(<span class="number">0</span>, <span class="number">100</span>, <span class="number">5</span>), loss_stochastic, <span class="string">'b-'</span>, label=<span class="string">'StochasticLoss'</span>)</span><br><span class="line">plt.plot(range(<span class="number">0</span>, <span class="number">100</span>, <span class="number">5</span>), loss_batch, <span class="string">'r--'</span>, label=<span class="string">'BatchLoss size=20'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper right'</span>, prop=&#123;<span class="string">'size'</span>: <span class="number">11</span>&#125;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之实现反向传播</title>
      <link href="2019/04/26/tensorflow-bp-method/"/>
      <url>2019/04/26/tensorflow-bp-method/</url>
      
        <content type="html"><![CDATA[<h1 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h1><ol><li>生成数据</li><li>初始化占位符和变量</li><li>创建损失函数</li><li>定义一个优化器Optimizer</li><li>通过随机样本进行迭代，更新变量</li></ol><h1 id="回归算法实例"><a href="#回归算法实例" class="headerlink" title="回归算法实例"></a>回归算法实例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据 x为正太分布，y为输出值</span></span><br><span class="line">x_vals = np.random.normal(<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">100</span>)</span><br><span class="line">y_vals = np.repeat(<span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="number">1</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="number">1</span>])</span><br><span class="line">A = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加乘法操作</span></span><br><span class="line">my_output = tf.multiply(x_data, A)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加L2正则损失函数</span></span><br><span class="line">loss = tf.square(my_output - y_target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行之前初始化变量</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明优化器</span></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.02</span>)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练算法 迭代100步</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    rand_index = np.random.choice(<span class="number">100</span>)</span><br><span class="line">    rand_x = [x_vals[rand_index]]</span><br><span class="line">    rand_y = [y_vals[rand_index]]</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">25</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step #'</span> + str(i+<span class="number">1</span>) + <span class="string">'A = '</span> + str(sess.run(A)))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))</span><br></pre></td></tr></table></figure><ol><li>生成输入数据x_vals，为N(1, 0.1)的正太分布，100个数据量</li><li>y_vals = 全部为10</li><li>随机生成正太分布变量A</li><li>目标函数为 A*X</li><li>采用L2正则损失函数，最小化目标函数的损失函数</li><li>使用梯度下降，学习率为0.02</li><li>迭代100次，随机在x_vals，y_vals中选取x,进行优化</li><li>输出A的值以及loss</li></ol><h1 id="二值分类算法"><a href="#二值分类算法" class="headerlink" title="二值分类算法"></a>二值分类算法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">ops.reset_default_graph()</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从正太分布(N(-1,1), N(3,1))生成数据，同时也生成目标标签</span></span><br><span class="line">x_vals = np.concatenate((np.random.normal(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">50</span>), np.random.normal(<span class="number">3</span>, <span class="number">1</span>, <span class="number">50</span>)))</span><br><span class="line">y_vals = np.concatenate((np.repeat(<span class="number">0.</span>, <span class="number">50</span>), np.repeat(<span class="number">1.</span>, <span class="number">50</span>)))</span><br><span class="line">x_data = tf.placeholder(tf.float32, shape=[<span class="number">1</span>])</span><br><span class="line">y_target = tf.placeholder(tf.float32, shape=[<span class="number">1</span>])</span><br><span class="line">A = tf.Variable(tf.random_normal(mean=<span class="number">10</span>, shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加转换操作</span></span><br><span class="line">my_output = tf.add(x_data, A)</span><br><span class="line"><span class="comment"># 增加一个维度</span></span><br><span class="line">my_output_expanded = tf.expand_dims(my_output, <span class="number">0</span>)</span><br><span class="line">y_target_expanded = tf.expand_dims(y_target, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化A</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明损失函数</span></span><br><span class="line">xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_target_expanded, logits=my_output_expanded)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加优化器</span></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.05</span>)</span><br><span class="line">train_step = my_opt.minimize(xentropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机选取数据迭代几百次，相应的更新变量A</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1400</span>):</span><br><span class="line">    rand_index = np.random.choice(<span class="number">100</span>)</span><br><span class="line">    rand_x = [x_vals[rand_index]]</span><br><span class="line">    rand_y = [y_vals[rand_index]]</span><br><span class="line"></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step #'</span> + str(i+<span class="number">1</span>) + <span class="string">'A = '</span> + str(sess.run(A)))</span><br><span class="line">        print(<span class="string">'Loss #'</span> + str(sess.run(xentropy, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))</span><br></pre></td></tr></table></figure><ol><li>生成x_vals100个样本，其中一半是N(-1,1)分布，一半是N(3,1)分布</li><li>生成y_vals100个样本，一半是0，一半是1</li><li>随机生成N(10, _)的变量</li><li>目标函数为sigmoid(A+X)</li><li>目标A的值为1，因为-1的分类为0，3的分类为1，中间值为1，则1+A=0，A=-1</li><li>损失函数为sigmoid交叉熵损失函数</li><li>梯度下降优化，步长为0.05</li><li>迭代1400次随机选取x,y，输出A和损失函数</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow之激励函数和损失函数</title>
      <link href="2019/04/26/tensorflow-activate-lost/"/>
      <url>2019/04/26/tensorflow-activate-lost/</url>
      
        <content type="html"><![CDATA[<h1 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h1><h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>函数为max(0, x)<br>是神经网络最常用的非线性函数<br>线形整流单元</p><h2 id="ReLU6"><a href="#ReLU6" class="headerlink" title="ReLU6"></a>ReLU6</h2><p>函数为min(max(0, x), 6)<br>为了抵消ReLU的线形增长部分</p><h2 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h2><p>函数为1/(1+exp(-x))<br>训练过程中反向传播项趋近于0，因此不怎么使用</p><h2 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h2><p>双曲正切函数<br>与sigmoid类似，不同的是tanh的取值范围是0到1，sigmoid的取值凡事是-1到1<br>函数是((exp(x)-exp(-x))/(exp(x)+exp(-x)))</p><h2 id="softsign"><a href="#softsign" class="headerlink" title="softsign"></a>softsign</h2><p>函数是x/(abs(x)+1), 符号连续估计</p><h2 id="softplus"><a href="#softplus" class="headerlink" title="softplus"></a>softplus</h2><p>是ReLU激励函数的平滑版<br>函数为log(exp(x)+1)</p><h2 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2><p>与softplus类似<br>不同点在于当输入无限小时，ELU激励函数趋近于-1，而softplus趋近于0<br>表达式为(exp(x)+1) if x&lt;0 else x</p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p><img src="/img/article/funclost.png" alt=""><br><img src="/img/article/funclost2.png" alt=""></p><h2 id="L1损失函数"><a href="#L1损失函数" class="headerlink" title="L1损失函数"></a>L1损失函数</h2><p>即绝对损失函数</p><h2 id="L2损失函数"><a href="#L2损失函数" class="headerlink" title="L2损失函数"></a>L2损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.l2_loss()</span><br></pre></td></tr></table></figure><h2 id="Pseudo-Huber损失函数"><a href="#Pseudo-Huber损失函数" class="headerlink" title="Pseudo-Huber损失函数"></a>Pseudo-Huber损失函数</h2><pre><code class="lang-python">delta1 = tf.constant(0.25)phuber1_y_vals = tf.multiply(tf.square(delta1), tf.sqrt(1. + tf.square((target - x_vals)/delta1) - 1.))phuber1_y_out = sess.run(phuber1_y_vals)</code></pre><h2 id="Hinge损失函数"><a href="#Hinge损失函数" class="headerlink" title="Hinge损失函数"></a>Hinge损失函数</h2><p>主要用来估计支持向量机算法，但有时也用来估计神经网络算法。</p><h2 id="两类交叉损失函数（cross-entropy）"><a href="#两类交叉损失函数（cross-entropy）" class="headerlink" title="两类交叉损失函数（cross-entropy）"></a>两类交叉损失函数（cross-entropy）</h2><p>作为逻辑损失函数，预测两类目标0或者1时，希望度量预测值到真实分类值（0或1）的距离</p><pre><code class="lang-python">xentropy_y_vals = -tf.multiply(target, tf.log(x_vals)) - tf.multiply((1. - target), tf.log(1. - x_vals))xentropy_y_out = sess.run(xentropy_y_vals)</code></pre><h2 id="sigmoid交叉熵损失函数（sigmoid-cross-entropy）"><a href="#sigmoid交叉熵损失函数（sigmoid-cross-entropy）" class="headerlink" title="sigmoid交叉熵损失函数（sigmoid cross entropy）"></a>sigmoid交叉熵损失函数（sigmoid cross entropy）</h2><p>先把x_vals值通过sigmoid函数转换，再计算交叉熵</p><pre><code class="lang-python">xentropy_sigmoid_y_vals = tf.nn.sigmoid_cross_entropy_with_logits(x_vals, targets)xentropy_sigmoid_y_out = sess.run(xentropy_sigmoid_y_vals)</code></pre><h2 id="加权交叉熵损失函数（Weighted-cross-entropy）"><a href="#加权交叉熵损失函数（Weighted-cross-entropy）" class="headerlink" title="加权交叉熵损失函数（Weighted cross-entropy）"></a>加权交叉熵损失函数（Weighted cross-entropy）</h2><pre><code class="lang-python">weight = tf.constant(0.5)xentropy_weighted_y_vals = tf.nn.weighted_cross_entropy_with_logits(x_vals, targets, weight)</code></pre><h2 id="softmax交叉熵损失函数（softmax-cross-entropy）"><a href="#softmax交叉熵损失函数（softmax-cross-entropy）" class="headerlink" title="softmax交叉熵损失函数（softmax cross-entropy）"></a>softmax交叉熵损失函数（softmax cross-entropy）</h2><p>作用于非归一化的输出。通过softmax函数将输出结果转化成概率分布，然后计算真值概率分布损失</p><pre><code class="lang-python">unscaled_logits = tf.constant([[1., -3., 10.]])target_dist = tf.constant([[0.1, 0.02, 0.88]])softmax_xentropy = tf.nn.softmax_cross_entropy_with_logits(unscaled_logits, target_dist)</code></pre><h2 id="稀疏softmax交叉熵损失函数，"><a href="#稀疏softmax交叉熵损失函数，" class="headerlink" title="稀疏softmax交叉熵损失函数，"></a>稀疏softmax交叉熵损失函数，</h2><p>和上一个类似，它把目标分类为true转化成index，而softmax交叉熵将目标转成概率分布</p><pre><code class="lang-python">unscaled_logits = tf.constant([[1., -3., 10.]])sparse_target_dist = tf.constant([2])sparse_xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=sparse_target_dist, logits=unscaled_logits)# print(sess.run(sparse_xentropy))</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Thread.State</title>
      <link href="2019/04/25/thread-state/"/>
      <url>2019/04/25/thread-state/</url>
      
        <content type="html"><![CDATA[<h1 id="线程直接的状态主要有如下几种"><a href="#线程直接的状态主要有如下几种" class="headerlink" title="线程直接的状态主要有如下几种"></a>线程直接的状态主要有如下几种</h1><ul><li>NEW 就绪</li><li>RUNNABLE 线程运行中或I/O等待</li><li>BLOCKED 线程在等待monitor锁(synchronized关键字)</li><li>WAITING 线程在无限等待唤醒</li><li>TIME_WAITING 线程在等待唤醒，但设置了时限</li></ul><h1 id="几种线程关键字的解释"><a href="#几种线程关键字的解释" class="headerlink" title="几种线程关键字的解释"></a>几种线程关键字的解释</h1><p>sleep: 线程进入等待状态，获得到了cpu进行等待时间，即等待状态<br>yield: 线程释放cpu，重新进入就绪状态，与其他线程一同抢占cpu<br>join: thread.join()，即thread插入当前线程之前，在运行完thread之后，当前线程继续执行<br>wait: 是Object下的方法，wait()的作用是让当前线程进入等待状态，同时，wait()也会让当前线程释放它所持有的锁<br>notify, notifyAll: notify()和notifyAll()的作用，则是唤醒当前对象上的等待线程；notify()是唤醒单个线程，而notifyAll()是唤醒所有的线程。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 多线程 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
